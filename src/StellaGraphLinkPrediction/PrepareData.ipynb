{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrepareData.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky1tivBXVsRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install google-api-python-client\n",
        "\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tweepy\n",
        "import json\n",
        "import json\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import urllib.parse\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niCrQix8Yg52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_title(dct):\n",
        "        if \"title\" in dct:\n",
        "            return dct[\"title\"]     \n",
        "        return dct\n",
        "def decode_date(dct):\n",
        "        if \"publishedAt\" in dct:\n",
        "            return dct[\"publishedAt\"]    \n",
        "        return dct\n",
        "def decode_source(dct):\n",
        "        if \"source\" in dct:\n",
        "            return dct[\"source\"]    \n",
        "        return dct\n",
        "def decode_url(dct):\n",
        "        if \"url\" in dct:\n",
        "            return dct[\"url\"]   \n",
        "        return dct\n",
        "def decode_description(dct):\n",
        "        if \"description\" in dct:\n",
        "            return p.clean(dct[\"description\"])      \n",
        "        return dct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hf-WhT5jCGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "ner = spacy.load(\"en_core_web_sm\")\n",
        "def getArticle(url):\n",
        "    result = requests.get(url)\n",
        "    c = result.content\n",
        "    soup = BeautifulSoup(c)\n",
        "\n",
        "    article_text = ''\n",
        "    article = soup.findAll('p')\n",
        "    for element in article:\n",
        "        article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    return article_text\n",
        "\n",
        "def getEntity(article_cor):\n",
        "  ent = ner(article_cor)\n",
        "  entities = []\n",
        "  for entity in ent.ents:\n",
        "    entities.append(entity.text)\n",
        "  return entities\n",
        "\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
        "                   vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                       reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def getTopics(url):\n",
        "    topic = \"\"\n",
        "    article_text = getArticle(url)\n",
        "    articleEntity = getEntity(article_text)\n",
        "    topics = get_top_n_words(articleEntity,10)\n",
        "    for item in topics:\n",
        "      topic = topic + item[0] + \" \"\n",
        "    return topic.strip()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVHJE1z3oql-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "date_format='%m/%d/%Y %H:%M:%S %Z'\n",
        "date = datetime.now(tz=pytz.utc)\n",
        "date = date.astimezone(timezone('US/Pacific'))\n",
        "print(date)\n",
        "time = date.strftime(\"%m/%d/%Y-%H-%M\")\n",
        "timestamp = time.replace(\"/\",\"-\")\n",
        "print(\"time:\", timestamp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQWTZQVKVTOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# publishedAt name(source) url description\n",
        "\n",
        "def extract_google_news_data(query):\n",
        "    df = pd.DataFrame()\n",
        "    keywords = urllib.parse.quote(query)\n",
        "    url = \"https://gnews.io/api/v3/search?q=\"+keywords+\"&token=a6419076f909fc74c42016c6bebf0755\"\n",
        "    response = requests.get(url)\n",
        "    gNewsApi_json = json.dumps(response.json(), indent = 4,sort_keys=True)\n",
        "    parsed_title = json.loads(gNewsApi_json,object_hook=decode_title)\n",
        "    parsed_date = json.loads(gNewsApi_json,object_hook=decode_date)\n",
        "    parsed_source = json.loads(gNewsApi_json,object_hook=decode_source)\n",
        "    parsed_url = json.loads(gNewsApi_json,object_hook=decode_url)\n",
        "    parsed_description = json.loads(gNewsApi_json,object_hook=decode_description)\n",
        "    date = parsed_date[\"articles\"]\n",
        "    date = [i[:10] for i in date]\n",
        "    headline = parsed_title[\"articles\"]\n",
        "    source = [i[\"name\"] for i in parsed_source[\"articles\"]]\n",
        "    url = parsed_url[\"articles\"]\n",
        "    description = parsed_description[\"articles\"]\n",
        "    # topics = [getTopics(i) for i in url]\n",
        "    df[\"DATE\"] = date\n",
        "    df[\"HEADLINE\"] = headline\n",
        "    df[\"SOURCE\"] = source\n",
        "    df[\"URL\"] = url\n",
        "    df[\"DESCRIPTION\"] = description\n",
        "    df[\"API\"] = \"GOOGLE NEWS API\"\n",
        "    # df[\"TOPICS\"] = topics\n",
        "    filename = \"gnews-\"+timestamp+\".csv\"\n",
        "    print(df)\n",
        "    df.to_csv(filename)\n",
        "    return df\n",
        "\n",
        "\n",
        "# title = parsed_title[\"articles\"]\n",
        "# keywords = urllib.parse.quote(query)\n",
        "# url = \"https://newsapi.org/v2/everything?q=\"+keywords+\"&from=&apiKey=2ed8bf16847a458d9af4ed36ce4f979b\"\n",
        "# response = requests.get(url)\n",
        "# newsApi_json = json.dumps(response.json(),indent = 4,sort_keys=True)\n",
        "# print(newsApi_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bduApTrB4Z53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def extract_news_api_data(query):\n",
        "      df = pd.DataFrame()\n",
        "      keywords = urllib.parse.quote(query)\n",
        "      url = \"https://newsapi.org/v2/everything?q=\"+keywords+\"&from=&apiKey=2ed8bf16847a458d9af4ed36ce4f979b\"\n",
        "      response = requests.get(url)\n",
        "      newsApi_json = json.dumps(response.json(),indent=4,sort_keys=True)\n",
        "      # print(newsApi_json)\n",
        "      parsed_title = json.loads(newsApi_json,object_hook=decode_title)\n",
        "      parsed_date = json.loads(newsApi_json,object_hook=decode_date)\n",
        "      parsed_source = json.loads(newsApi_json,object_hook=decode_source)\n",
        "      parsed_url = json.loads(newsApi_json,object_hook=decode_url)\n",
        "      parsed_description = json.loads(newsApi_json,object_hook=decode_description)\n",
        "      date = parsed_date[\"articles\"]\n",
        "      date = [i[:10] for i in date]\n",
        "      headline = parsed_title[\"articles\"]\n",
        "      source = [i[\"name\"] for i in parsed_source[\"articles\"]]\n",
        "      url = parsed_url[\"articles\"]\n",
        "      description = parsed_description[\"articles\"]\n",
        "      df[\"DATE\"] = date\n",
        "      df[\"HEADLINE\"] = headline\n",
        "      df[\"SOURCE\"] = source\n",
        "      df[\"URL\"] = url\n",
        "      df[\"DESCRIPTION\"] = description\n",
        "      df[\"API\"] = \"NEWS API\"\n",
        "      # df[\"TOPICS\"] = topics\n",
        "      filename = \"newsapi-\"+timestamp+\".csv\"\n",
        "      print(df)\n",
        "      df.to_csv(filename)\n",
        "      return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP-DLAHM-W-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"coronavirus\"\n",
        "df_gnews = extract_google_news_data(query)\n",
        "df_newsapi = extract_news_api_data(query)\n",
        "\n",
        "result = df_gnews.append(df_newsapi,ignore_index=True, sort=False)\n",
        "filename = \"news-\"+timestamp+\".csv\"\n",
        "result.to_csv(filename,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqgytvuJg-RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}