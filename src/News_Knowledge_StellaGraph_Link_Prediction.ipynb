{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News-Knowledge-StellaGraph-Link-Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q84-UPkrHKAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install google-api-python-client\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tweepy\n",
        "import json\n",
        "import json\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import urllib.parse\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/news/news.csv\", sep=',')\n",
        "df.sort_values(by=['DATE'], inplace=True,ignore_index=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kktFTYqzUC_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hf-WhT5jCGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "ner = spacy.load(\"en_core_web_sm\")\n",
        "def getArticle(url):\n",
        "    result = requests.get(url)\n",
        "    c = result.content\n",
        "    soup = BeautifulSoup(c)\n",
        "\n",
        "    article_text = ''\n",
        "    article = soup.findAll('p')\n",
        "    for element in article:\n",
        "        article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    return article_text\n",
        "\n",
        "def getEntity(article_cor):\n",
        "  ent = ner(article_cor)\n",
        "  entities = []\n",
        "  for entity in ent.ents:\n",
        "    entities.append(entity.text)\n",
        "  return entities\n",
        "\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
        "                   vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                       reverse=True)\n",
        "    return words_freq[:n]\n",
        "def getTopicWords(corpus):\n",
        "    topic = \"\"\n",
        "    topics = get_top_n_words(corpus,5)\n",
        "    for item in topics:\n",
        "      topic = topic + item[0] + \" \"\n",
        "    return topic.strip()\n",
        "\n",
        "def getTopics(url):\n",
        "    topic = \"\"\n",
        "    article_text = getArticle(url)\n",
        "    articleEntity = getEntity(article_text)\n",
        "    topics = get_top_n_words(articleEntity,5)\n",
        "    for item in topics:\n",
        "      topic = topic + item[0] + \" \"\n",
        "    return topic.strip()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXv0tzsiHtSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# le = preprocessing.LabelEncoder()\n",
        "# df['DATE_TRANS'] = le.fit_transform(df['DATE'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EYQ56PiiCSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"ARTICLES\"] = df[\"URL\"].apply(lambda x: getArticle(x))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVz4u1wSPRds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop([5,20,21,25,26])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTw2QpJ3TzIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"TOPICS\"] = df[\"ARTICLES\"].apply(lambda x: getTopicWords([x]))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0yqyDK7mywd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJeNy0ACX8JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date = []\n",
        "topic = []\n",
        "for index, row in df.iterrows():\n",
        "  tempdate = [row[\"DATE\"]]\n",
        "  date.extend(tempdate*5)\n",
        "  topic.extend(row[\"TOPICS\"].split(\" \"))\n",
        "print(len(date))\n",
        "print(len(topic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "867ani6GZwXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "df_news = pd.DataFrame() \n",
        "df_news[\"source\"] = le.fit_transform(date)\n",
        "df_news[\"target\"] = le.fit_transform(topic)\n",
        "df_news.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzrEZAtdaryr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_news.to_csv(\"embeddings.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKs-x-PFlnNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "df_news['label'] = 'topic'\n",
        "Gnx = nx.from_pandas_edgelist(df_news, edge_attr='label')\n",
        "nx.set_node_attributes(Gnx, 'news', 'label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqMCgt4dYR1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "headline = df[\"HEADLINE\"]\n",
        "headline = [i.replace(\"‘\",\"\") for i in headline]\n",
        "headline = [i.replace(\"’\",\"\") for i in headline]\n",
        "headline = [i.replace(\"'\",\"\") for i in headline]\n",
        "headline = [i.replace(\"?\",\"\") for i in headline]\n",
        "headline = [i.replace(\",\",\"\") for i in headline]\n",
        "headline = [i.replace(\"(\",\"\") for i in headline]\n",
        "headline = [i.replace(\")\",\"\") for i in headline]\n",
        "headline = [i.replace(\"-\",\"\") for i in headline]\n",
        "headline = [i.split(\" \") for i in headline]\n",
        "# headline = np.array(headline) \n",
        "print(headline)\n",
        "# headline_arr = np.array(headline)\n",
        "\n",
        "# for item in headline_arr:\n",
        "#   item = np.array(item)\n",
        "# print(headline_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBmOm3QlY-YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_encoder = pd.DataFrame() \n",
        "# label_encoder =LabelEncoder()\n",
        "# headline_int = label_encoder.fit_transform(headline).reshape(*headline.shape)\n",
        "# one_hot_encoder = OneHotEncoder.fit_transform(headline_int).toarray()\n",
        "# df_encoder = one_hot_encoder\n",
        "# df_encoder.to_csv(\"news.content\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGjFrYSuKvT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = [\"w_{}\".format(ii) for ii in range(1433)]\n",
        "column_names =  feature_names + ['subject']\n",
        "node_data = pd.read_csv(\"/content/drive/My Drive/cora/cora.content\", sep='\\t', header=None, names=column_names)\n",
        "node_data.head()\n",
        "node_features = node_data[feature_names]\n",
        "node_features = node_features[:91]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbgGgwGdLwDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "node_features.reset_index(drop=True, inplace=True)\n",
        "node_features.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEDlZswYjDE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(node_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPEFo1pdQOzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  %pip install -q stellargraph[demos]==1.0.0rc1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQlBFn9PRR_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import stellargraph as sg\n",
        "try:\n",
        "    sg.utils.validate_notebook_version(\"1.0.0rc1\")\n",
        "except AttributeError:\n",
        "    raise ValueError(\n",
        "        f\"This notebook requires StellarGraph version 1.0.0rc1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
        "    ) from None\n",
        "from stellargraph.mapper import GraphSAGELinkGenerator, GraphSAGENodeGenerator\n",
        "from stellargraph.layer import GraphSAGE, link_classification\n",
        "from stellargraph.data import UnsupervisedSampler\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUuPaqKPT6d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# G = sg.StellarGraph(Gnx)\n",
        "G = sg.StellarGraph(Gnx, node_features=node_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpm9j7dZUJGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(G.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfH38_xUTKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stellargraph as sg\n",
        "from stellargraph.data import EdgeSplitter\n",
        "from stellargraph.mapper import FullBatchLinkGenerator\n",
        "from stellargraph.layer import GCN, LinkEmbedding\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing, feature_extraction, model_selection\n",
        "\n",
        "from stellargraph import globalvar\n",
        "from stellargraph import datasets\n",
        "from IPython.display import display, HTML\n",
        "%matplotlib inline\n",
        "# Define an edge splitter on the original graph G:\n",
        "edge_splitter_test = EdgeSplitter(G)\n",
        "\n",
        "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G, and obtain the\n",
        "# reduced graph G_test with the sampled links removed:\n",
        "G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
        "    p=0.1, method=\"global\", keep_connected=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2euzmknZUzhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define an edge splitter on the reduced graph G_test:\n",
        "edge_splitter_train = EdgeSplitter(G_test)\n",
        "\n",
        "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
        "# reduced graph G_train with the sampled links removed:\n",
        "G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
        "    p=0.1, method=\"global\", keep_connected=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cb0vGhGUk--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "train_gen = FullBatchLinkGenerator(G_train, method=\"gcn\")\n",
        "train_flow = train_gen.flow(edge_ids_train, edge_labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsOurvF5xo1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyleD2CqNVVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_gen = FullBatchLinkGenerator(G_test, method=\"gcn\")\n",
        "test_flow = train_gen.flow(edge_ids_test, edge_labels_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7N5fDhCMx9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gcn = GCN(\n",
        "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=train_gen, dropout=0.3\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0H5YsJ7M2XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_inp, x_out = gcn.in_out_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfs77OV2M6zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = LinkEmbedding(activation=\"relu\", method=\"ip\")(x_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tPgNVrZNAA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = keras.layers.Reshape((-1,))(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaohBMXRND_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(lr=0.01),\n",
        "    loss=keras.losses.binary_crossentropy,\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3i-BrAjNI1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_train_metrics = model.evaluate(train_flow)\n",
        "init_test_metrics = model.evaluate(test_flow)\n",
        "\n",
        "print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
        "for name, val in zip(model.metrics_names, init_train_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
        "\n",
        "print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
        "for name, val in zip(model.metrics_names, init_test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKmUnnjYNfW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    train_flow, epochs=epochs, validation_data=test_flow, verbose=2, shuffle=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh3keZCQNxS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sg.utils.plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StC7H17jN1Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_metrics = model.evaluate(train_flow)\n",
        "test_metrics = model.evaluate(test_flow)\n",
        "\n",
        "print(\"\\nTrain Set Metrics of the trained model:\")\n",
        "for name, val in zip(model.metrics_names, train_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
        "\n",
        "print(\"\\nTest Set Metrics of the trained model:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OMNqhK0wLHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predcit1 = train_gen.flow([[12,48]],[1])\n",
        "# prediction1 = model.predict(predcit1)\n",
        "# prediction1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S07LFXAjrck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = \"\"\n",
        "lastTopic = \"\"\n",
        "for index, row in df.iterrows():\n",
        "  data = data + row[\"TOPICS\"] + \"\\n\"\n",
        "  lastTopic = row[\"TOPICS\"]\n",
        "print(data.strip())\n",
        "topiclist = lastTopic.split(\" \")\n",
        "print(topiclist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGK_panFmUKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# generate a sequence from a model\n",
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pre-pad sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\treturn in_text\n",
        "\n",
        "# source text\n",
        "# prepare the tokenizer on the source text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "# determine the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# create line-based sequences\n",
        "sequences = list()\n",
        "for line in data.split('\\n'):\n",
        "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(encoded)):\n",
        "\t\tsequence = encoded[:i+1]\n",
        "\t\tsequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "# pad input sequences\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
        "print('Max Sequence Length: %d' % max_length)\n",
        "# split into input and output elements\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1],sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=500, verbose=2)\n",
        "# evaluate model\n",
        "predcitTopics = []\n",
        "for word in topiclist:\n",
        "   predcitTopics.append(generate_seq(model, tokenizer, max_length-1, word, 4))\n",
        "\n",
        "print(predcitTopics)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}