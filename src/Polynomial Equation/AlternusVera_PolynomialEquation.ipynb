{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "AlternusVera_PolynomialEquation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junten/context-graph/blob/master/src/Polynomial%20Equation/AlternusVera_PolynomialEquation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EUkES4k4rxC",
        "colab_type": "text"
      },
      "source": [
        "### Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjvnsAG_4rxE",
        "colab_type": "code",
        "outputId": "bb126380-7dfb-4189-a07d-726a4b868fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from string import punctuation\n",
        "from scipy import sparse\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol2v0jis6hYF",
        "colab_type": "code",
        "outputId": "ac7df448-4ba1-45bb-a970-363a1c2ffa5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mItmJBWk4rxh",
        "colab_type": "text"
      },
      "source": [
        "### [Google News corpus word2vec](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)\n",
        "\n",
        "### Spell Check \n",
        "\n",
        "-  You can download the pre-trained model [**here**](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)\n",
        "\n",
        "- Or clone it from GitHub [**GoogleNews-vectors-negative300**](https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n",
        "\n",
        "> It’s 1.5GB! It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\n",
        "\n",
        "**3 million words * 300 features * 4bytes/feature = ~3.35GB**\n",
        "\n",
        "> This file consist of the word2vec -  pre-trained Google News corpus (3 billion running words) to word vector model (3 million 300-dimension English word vectors).\n",
        "\n",
        "> Look at the [**vocabulory list**](https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary) used to train this model. Each text file contains 100,000 entries from the model. \n",
        "\n",
        "\n",
        ">  There are few things that this dataset contains and not. It has stop words like  “the”, “also”, “should” and does not have stop words like “a”, “and”, “of”. As I have removed the stop words the complexity is reduced as there is no need to check the spelling for stop words. \n",
        "\n",
        "> It does have numbers but in the form of entried wiht #. e.g., you won’t find “100”. But it does include entries like “###MHz_DDR2_SDRAM”. \n",
        "\n",
        "The model used [**WinPython-64bit-2.7.10.3**](https://winpython.github.io/) for efficient python distribution on Windows system. Helps to run the scripts in batches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSWUzgh64rxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "word2vec_pickle_file = \"/content/drive/My Drive/junteng_dev/data_set/words2vec.pkl\"\n",
        "\n",
        "WORDS = None\n",
        "if path.exists(word2vec_pickle_file):\n",
        "    with open(word2vec_pickle_file, 'rb') as f:\n",
        "        WORDS = pickle.load(f)\n",
        "else:\n",
        "    google_word2vec_file = '/content/drive/My Drive/context_graph/GoogleNews-vectors-negative300.bin.gz'\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(google_word2vec_file, binary=True)\n",
        "    words = model.index2word\n",
        "    w_rank = {}\n",
        "    for i,word in enumerate(words):\n",
        "        w_rank[word] = i\n",
        "\n",
        "    WORDS = w_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMvrMTsp4rxV",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning and Text Preprocessing \n",
        "\n",
        "*Steps included in the preprocessing:*\n",
        "- Remove Special Characters and Punctuations\n",
        "- Lower case the news\n",
        "- Tokenization\n",
        "- Remove Stop Words\n",
        "- Lemmatization\n",
        "- Stemming \n",
        "- Spell Check "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0_GKk3J4rxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def cleaning(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)\n",
        "\n",
        "def words(text): \n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return - WORDS.get(word, 0)\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def spell_checker(text):\n",
        "    all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n",
        "    spell_checked_text  = []\n",
        "    for i in range(len(all_words)):\n",
        "        spell_checked_text.append(correction(all_words[i]))\n",
        "    return ' '.join(spell_checked_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYg2k3934rxX",
        "colab_type": "text"
      },
      "source": [
        "###  Putting It All Together \n",
        "\n",
        "To make the code reusable, we need to create a function that can be called many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xplNzpEFT_bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "\n",
        "train_processed_file = dataset_path + \"/train_processed.csv\"\n",
        "test_processed_file = dataset_path + \"/test_processed.csv\"\n",
        "valid_processed_file = dataset_path + \"/valid_processed.csv\"\n",
        "colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']\n",
        "\n",
        "if path.exists(train_processed_file):\n",
        "    train_news = pd.read_csv(train_processed_file, error_bad_lines=False)\n",
        "else:\n",
        "    train_filename =  dataset_path + \"/train.tsv\"\n",
        "    train_news = pd.read_csv(train_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
        "    train_news['clean'] = train_news[\"headline_text\"].apply(cleaning) \n",
        "    train_news['clean'] = train_news['clean'].apply(spell_checker)\n",
        "    train_news.to_csv(train_processed_file, sep=',')\n",
        "\n",
        "if path.exists(test_processed_file):\n",
        "    test_news = pd.read_csv(test_processed_file, error_bad_lines=False)\n",
        "else:\n",
        "    test_filename =  dataset_path + \"/test.tsv\"\n",
        "    test_news = pd.read_csv(test_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
        "    test_news['clean'] = test_news[\"headline_text\"].apply(cleaning)\n",
        "    test_news['clean'] = test_news['clean'].apply(spell_checker)\n",
        "    test_news.to_csv(test_processed_file, sep=',')\n",
        "\n",
        "if path.exists(valid_processed_file):\n",
        "    valid_news = pd.read_csv(valid_processed_file, error_bad_lines=False)\n",
        "else:\n",
        "    valid_filename = dataset_path + \"/valid.tsv\"\n",
        "    valid_news = pd.read_csv(valid_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
        "    valid_news['clean'] = valid_news[\"headline_text\"].apply(cleaning)\n",
        "    valid_news['clean'] = valid_news['clean'].apply(spell_checker)\n",
        "    valid_news.to_csv(test_processed_file, sep=',')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lIy2dQ4ryZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#1: Sentiment analysis \n",
        "\n",
        "#### Using Vader Sentiment Analyser\n",
        "\n",
        "##### [Sentiment Intensity Analyzer](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html)\n",
        "\n",
        "\n",
        "> VADER, or the **Valence Aware Dictionary and sEntiment Reasoner** has created a package that performes sentiment analysis using the polarity-based, where pieces of texts are classified as either positive or negative, or valence-based, where the intensity of the sentiment is taken into account. For example, the words ‘good’ and ‘excellent’ would be treated the same in a polarity-based approach, whereas ‘excellent’ would be treated as more positive than ‘good’ in a valence-based approach\n",
        "\n",
        "- It is based on lexicons of sentiment-related word.\n",
        "- The first three, positive, neutral and negative, represent the proportion of the text that falls into those categories.\n",
        "- The final metric, the compound score, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ7gcg3o_a46",
        "colab_type": "code",
        "outputId": "c6c98f9f-7fb4-4b77-9548-5c4782fda3cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxfShYZV4ryb",
        "colab_type": "code",
        "outputId": "3568a9e6-12c9-4fde-e11b-a3546e80331c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import warnings\n",
        "import nltk.sentiment\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
        "\n",
        "def print_sentiment_scores(sentence):\n",
        "    snt = senti.polarity_scores(sentence)\n",
        "    print(\"{:-<40} \\n{}\".format(sentence, str(snt)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMqgTL9H4ryg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vader_polarity(snt):\n",
        "    if not snt:\n",
        "        return None\n",
        "    elif snt['neg'] > snt['pos'] and snt['neg'] > snt['neu']:\n",
        "        return -1\n",
        "    elif snt['pos'] > snt['neg'] and snt['pos'] > snt['neu']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kVhNPx74ryn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to determine if a text is negative(-1) or postive (1) or neutral (0)\n",
        "def get_polarity_type(sentence):\n",
        "    sentimentVector = []\n",
        "    snt = senti.polarity_scores(sentence)\n",
        "    sentimentVector.append(get_vader_polarity(snt))\n",
        "    sentimentVector.append(snt['neg'])\n",
        "    sentimentVector.append(snt['neu'])\n",
        "    sentimentVector.append(snt['pos'])\n",
        "    sentimentVector.append(snt['compound'])\n",
        "    \n",
        "    print(sentimentVector)\n",
        "    return sentimentVector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfzFOgqP4ryu",
        "colab_type": "text"
      },
      "source": [
        "- senti.polarity_scores is a dictionary\n",
        "- pos and neg indicates - positive and negative emotions in sentence\n",
        "- we should be interested in compound score which calculates the final effect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL5IRuo04rzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "train_sentiment_file = dataset_path + \"/train_sentiment.csv\"\n",
        "test_sentiment_file = dataset_path + \"/test_sentiment.csv\"\n",
        "valid_sentiment_file = dataset_path + \"/valid_sentiment.csv\"\n",
        "\n",
        "# sentiment for train data\n",
        "if path.exists(train_sentiment_file):\n",
        "    train_news = pd.read_csv(train_sentiment_file, error_bad_lines=False)\n",
        "else:\n",
        "    sentiment = []\n",
        "    vader_pol = []\n",
        "    cmp_score = []\n",
        "    for row in train_news['clean']:\n",
        "        get_pols = get_polarity_type(row)\n",
        "        sentiment.append(get_pols[1:])\n",
        "        vader_pol.append(get_pols[0])\n",
        "        cmp_score.append(get_pols[1:][-1]) #last element \n",
        "    train_news['sentiment_vector'] = sentiment\n",
        "    train_news['vader_polarity'] = vader_pol\n",
        "    train_news['sentiment_score'] = cmp_score\n",
        "    train_news.to_csv(valid_sentiment_file, sep=',')\n",
        "\n",
        "# sentiment for test data\n",
        "if path.exists(test_sentiment_file):\n",
        "    test_news = pd.read_csv(test_sentiment_file, error_bad_lines=False)\n",
        "else:\n",
        "    sentiment = []\n",
        "    vader_pol = []\n",
        "    cmp_score = []\n",
        "    for row in test_news['clean']:\n",
        "        get_pols = get_polarity_type(row)\n",
        "        sentiment.append(get_pols[1:])\n",
        "        vader_pol.append(get_pols[0])\n",
        "        cmp_score.append(get_pols[1:][-1]) #last element \n",
        "    test_news['sentiment_vector'] = sentiment\n",
        "    test_news['vader_polarity'] = vader_pol\n",
        "    test_news['sentiment_score'] = cmp_score\n",
        "    test_news.to_csv(test_sentiment_file, sep=',')\n",
        "\n",
        "# sentiment for valid data\n",
        "if path.exists(valid_sentiment_file):\n",
        "    valid_news = pd.read_csv(valid_sentiment_file, error_bad_lines=False)\n",
        "else:\n",
        "    sentiment = []\n",
        "    vader_pol = []\n",
        "    cmp_score = []\n",
        "    for row in valid_news['clean']:\n",
        "        get_pols = get_polarity_type(row)\n",
        "        sentiment.append(get_pols[1:])\n",
        "        vader_pol.append(get_pols[0])\n",
        "        cmp_score.append(get_pols[1:][-1]) #last element \n",
        "    valid_news['sentiment_vector'] = sentiment\n",
        "    valid_news['vader_polarity'] = vader_pol\n",
        "    valid_news['sentiment_score'] = cmp_score\n",
        "    valid_news.to_csv(valid_sentiment_file, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4FGqhJYTU91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SentimentAnalysis():\n",
        "    def __init__(self):        \n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        train_sentiment_file = dataset_path + \"/train_sentiment.csv\"\n",
        "        test_sentiment_file = dataset_path + \"/test_sentiment.csv\"\n",
        "        valid_sentiment_file = dataset_path + \"/valid_sentiment.csv\"\n",
        "        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\"]\n",
        "        dataTrain = pd.read_csv(train_sentiment_file, sep=',', header=None, names = columnNames)\n",
        "        dataTest = pd.read_csv(test_sentiment_file, sep=',', header=None, names = columnNames)\n",
        "\n",
        "        #dropping columns\n",
        "        columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector']\n",
        "        dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
        "        dataTest = dataTest.drop(columns=columnsToRemove)\n",
        "        dataTrain = dataTrain.loc[1:] \n",
        "        dataTest = dataTest.loc[1:]\n",
        "    \n",
        "    \n",
        "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "                ('LogRCV', tfidfV),\n",
        "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
        "                ])\n",
        "\n",
        "        self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['vader_polarity'])\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
        "        score = metrics.accuracy_score(dataTest['vader_polarity'], predicted_LogR)\n",
        "        print(\"Sentiment Analysis Model Trained - accuracy:   %0.6f\" % score)\n",
        "        \n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return float(predicedProb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFPqejzyT6CK",
        "colab_type": "code",
        "outputId": "9104f2e9-a118-44f0-aa16-d5ce262ba57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getSentimentAnalysisScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/sentiment_analysis.pkl', 'rb') as f:\n",
        "        sa = pickle.load(f)\n",
        "    probValue = sa.predict(text)\n",
        "    return probValue\n",
        "\n",
        "DATAMINERS_getSentimentAnalysisScore(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9448046694778502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgRkVeeJ4rzQ",
        "colab_type": "text"
      },
      "source": [
        "#2:  LDA Topic Modelling \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaPrX2Aj4rzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_news['index'] = train_news.index\n",
        "data = train_news\n",
        "train_lda = data[['clean', 'index']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbavb_CB4rzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_news['index'] = test_news.index\n",
        "data = test_news\n",
        "test_lda = data[['clean', 'index']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzpp9-2F4rzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_news['index'] = valid_news.index\n",
        "data = valid_news\n",
        "valid_lda = data[['clean', 'index']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyIVYnPP4rzd",
        "colab_type": "text"
      },
      "source": [
        "#### Split the clean news into list of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSGIypMv4rze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_docs = train_lda['clean'].map(lambda doc: doc.split(\" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ZgmGPV4rzi",
        "colab_type": "text"
      },
      "source": [
        "### Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "> It is an example of a probabilistic topic model. Topic models are a great way to automatically explore and structure a large set of documents: they group or cluster documents based on the words that occur in them. As documents on similar topics tend to use a similar sub-vocabulary, the resulting clusters of documents can be interpreted as discussing different 'topics'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9lZg0zx4rzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tokens(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if len(token) > 3:\n",
        "            result.append(token)\n",
        "    return result\n",
        "tokenized_docs_local = train_news['clean'].map(get_word_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cxR-6EX4rzo",
        "colab_type": "text"
      },
      "source": [
        "### Create a function to build the dictionary and tokenized docs for given feature\n",
        "\n",
        "Below function does the following\n",
        "* #### Dictionary\n",
        "Returns Dictionary given, dataframe and column name\n",
        "* #### Tokenizeddocs\n",
        "Returns Tokenizeddocs, of the all the words in a text in that column can be used for bow_corpus\n",
        "* #### Dictionary is filtered using Gensim filter_extremes\n",
        "    Filter out tokens that appear in less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number). after the above two steps, keep only the first 100000 most frequent tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MEfi8DP4rzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dictionary_print_words(dataframe,colname):\n",
        "    dictionary_gensim = gensim.corpora.Dictionary(processed_docs)\n",
        "    count = 0\n",
        "    print('######## DICTIONARY Words and occurences ########')\n",
        "    for k, v in dictionary_gensim.iteritems():\n",
        "        print(k, v)\n",
        "        count += 1\n",
        "        if count > 10:\n",
        "            break\n",
        "    dictionary_gensim.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "    return dictionary_gensim, tokenized_docs_local"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykyp2IqL4rzs",
        "colab_type": "text"
      },
      "source": [
        "#### Gensim filter_extremes\n",
        "\n",
        "> Filter out tokens that appear less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number). after the above two steps, keep only the first 100000 most frequent tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMxC5xaC4rzt",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function to build bow_corpus from dictionary and tokenized_docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik7DM7Ic4rzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bow_corpus_print_sample(dataframe,colname):\n",
        "    dictionary_gensim, tokenized_docs_local = get_dictionary_print_words(dataframe, colname)\n",
        "    bow_corpus_local = [dictionary_gensim.doc2bow(doc) for doc in tokenized_docs_local]\n",
        "    bow_doc_local_0 = bow_corpus_local[0]\n",
        "    print('\\n ######## BOW VECTOR FIRST ITEM ########')\n",
        "    print(bow_doc_local_0)\n",
        "    print('\\n ######## PREVIEW BOW ########')\n",
        "    for i in range(len(bow_doc_local_0)):\n",
        "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_local_0[i][0], \n",
        "                                               dictionary_gensim[bow_doc_local_0[i][0]], bow_doc_local_0[i][1]))\n",
        "    return bow_corpus_local, dictionary_gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcfJsrzd4rzw",
        "colab_type": "text"
      },
      "source": [
        "**Gensim doc2bow**\n",
        "\n",
        "For each document we create a dictionary reporting how many words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYlXt6Jt4rzx",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function to build tfidf_corpus from bow_corpus\n",
        "\n",
        "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMBb97DV4rzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tfidf_corpus_print_sample(bow_corpus_local):\n",
        "    from gensim import corpora, models\n",
        "    tfidf = models.TfidfModel(bow_corpus_local)\n",
        "    tfidf_corpus_local = tfidf[bow_corpus_local]\n",
        "    print('\\n ######## TFIDF VECTOR FIRST ITEM ########')\n",
        "    \n",
        "    from pprint import pprint\n",
        "    for doc in tfidf_corpus_local:\n",
        "        pprint(doc)\n",
        "        break\n",
        "    return tfidf_corpus_local"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufR8OT4T4rz2",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function to run ldamodel and print top 10 topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeDpdCyK4rz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lda_model_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2)\n",
        "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n",
        "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
        "\n",
        "    #Below Code Prints Topics and Words\n",
        "    for topic,words in lda_topics_words:\n",
        "        print(str(topic)+ \"::\"+ str(words))\n",
        "    return lda_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxNfha7j4rz-",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function to run ldamodel and print top 10 topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFx0C_TG4rz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lda_model_topics_topwords_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2, random_state=1)\n",
        "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n",
        "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
        "\n",
        "    #Below Code Prints Topics and Words\n",
        "    for topic,words in lda_topics_words:\n",
        "        print(str(topic)+ \"::\"+ str(words))\n",
        "    return lda_model,lda_topics_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W1Q-FGQ4r0E",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function to enrich data with lda topics, lda topics score, top words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8BLvczv4r0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identify_topic_number_score_label_topwords(text,dictionary_local,lda_model_local,lda_topics_top_words_local):\n",
        "    bow_vector_local = dictionary_local.doc2bow(get_word_tokens(text))\n",
        "    topic_number_local, topic_score_local = sorted(\n",
        "        lda_model_local[bow_vector_local], key=lambda tup: -1*tup[1])[0]\n",
        "    #print (topic_number_local, topic_score_local)\n",
        "    return pd.Series([topic_number_local, topic_score_local,\" \".join(lda_topics_top_words_local[int(topic_number_local)][1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basZ6WSa4r0J",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function that can enrich topic data to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MMPQYNj4r0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_lda_results_to_dataset(dataframe,topiccolnames,coltoapplylda,colnamedictionary,colnameldamodel, colnameldatopwords):\n",
        "    dataframe[topiccolnames] = dataframe.apply(\n",
        "    lambda row: identify_topic_number_score_label_topwords(\n",
        "        row[coltoapplylda],colnamedictionary,colnameldamodel,\n",
        "        colnameldatopwords), axis=1)\n",
        "    return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jy1Z5kH4r0P",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words\n",
        "\n",
        "#### Create a dictionary and tokens\n",
        "\n",
        "> Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRiLdEuv4r0Q",
        "colab_type": "text"
      },
      "source": [
        "#### Create a function to convert text to word tokens from cleaned dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldUWiUTK4r0R",
        "colab_type": "code",
        "outputId": "c468eb7a-6774-498b-ec78-234f529ca84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "bow_corpus_headline, dictionary_headline = get_bow_corpus_print_sample(train_news,\n",
        "                                                                      'clean')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######## DICTIONARY Words and occurences ########\n",
            "0 abort\n",
            "1 anni\n",
            "2 demand\n",
            "3 group\n",
            "4 list\n",
            "5 polit\n",
            "6 say\n",
            "7 support\n",
            "8 third\n",
            "9 trimmest\n",
            "10 administr\n",
            "\n",
            " ######## BOW VECTOR FIRST ITEM ########\n",
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (6, 1), (7, 1)]\n",
            "\n",
            " ######## PREVIEW BOW ########\n",
            "Word 0 (\"abort\") appears 1 time.\n",
            "Word 1 (\"demand\") appears 1 time.\n",
            "Word 2 (\"group\") appears 1 time.\n",
            "Word 3 (\"list\") appears 1 time.\n",
            "Word 4 (\"polit\") appears 1 time.\n",
            "Word 6 (\"support\") appears 1 time.\n",
            "Word 7 (\"third\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbyJHZd34r0V",
        "colab_type": "text"
      },
      "source": [
        "### Running LDA using Bag of Words\n",
        "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf6s2MO14r0W",
        "colab_type": "code",
        "outputId": "04b34f94-52d0-41a4-cc38-5416846f27d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "lda_model_headline, lda_headline_topic_words = get_lda_model_topics_topwords_print_top_topics(\n",
        "    bow_corpus_headline, 10 ,dictionary_headline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0::['vote', 'sent', 'republican', 'support', 'state', 'democrat', 'obama', 'romney', 'percent', 'would']\n",
            "1::['percent', 'state', 'time', 'averag', 'american', 'school', 'year', 'hous', 'billion', 'rais']\n",
            "2::['presid', 'obama', 'country', 'bill', 'barack', 'state', 'bush', 'year', 'billion', 'florida']\n",
            "3::['year', 'percent', 'school', 'would', 'budget', 'public', 'every', 'state', 'obama', 'spend']\n",
            "4::['obama', 'state', 'nation', 'said', 'million', 'peopl', 'work', 'presid', 'clinton', 'text']\n",
            "5::['percent', 'year', 'million', 'state', 'obama', 'time', 'rate', 'peopl', 'presid', 'wisconsin']\n",
            "6::['percent', 'increas', 'american', 'peopl', 'state', 'year', 'health', 'sinc', 'care', 'vote']\n",
            "7::['year', 'last', 'state', 'million', 'health', 'vote', 'creat', 'number', 'nation', 'would']\n",
            "8::['health', 'care', 'state', 'year', 'cost', 'would', 'percent', 'plan', 'insur', 'budget']\n",
            "9::['state', 'trump', 'unit', 'donald', 'georgia', 'want', 'clinton', 'percent', 'feder', 'iller']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyZDFTOP4r0Y",
        "colab_type": "text"
      },
      "source": [
        "#### Generate TF-IDF bow_corpus\n",
        "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88PMc6BZ4r0Z",
        "colab_type": "code",
        "outputId": "164e1bab-483e-48d6-b6a2-8aeddb65309c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "tfidf_corpus_headline = get_tfidf_corpus_print_sample(bow_corpus_headline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ######## TFIDF VECTOR FIRST ITEM ########\n",
            "[(0, 0.3213684467163471),\n",
            " (1, 0.45885933393208955),\n",
            " (2, 0.3867148870952622),\n",
            " (3, 0.4388676762258462),\n",
            " (4, 0.3854712129906647),\n",
            " (6, 0.25709294587835624),\n",
            " (7, 0.3596280760961804)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRJgx-fZ4r0g",
        "colab_type": "text"
      },
      "source": [
        "### Running LDA model using Bag of Words\n",
        "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’\n",
        "\n",
        "**GOAL**: To get top ten topics with top words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ifUxB3MW4r0h",
        "colab_type": "code",
        "outputId": "f1a2d637-7a59-4b0f-928d-b3f0d569412b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "lda_tfidf_model_headline  = get_lda_model_print_top_topics(tfidf_corpus_headline,10,dictionary_headline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0::['care', 'health', 'american', 'percent', 'year', 'nation', 'rate', 'support', 'hous', 'million']\n",
            "1::['year', 'percent', 'state', 'busi', 'feder', 'million', 'near', 'scott', 'budget', 'bush']\n",
            "2::['percent', 'obama', 'rate', 'year', 'sinc', 'presid', 'abort', 'unemploy', 'state', 'first']\n",
            "3::['vote', 'court', 'state', 'social', 'secur', 'said', 'time', 'trump', 'peopl', 'year']\n",
            "4::['state', 'percent', 'nation', 'year', 'highest', 'country', 'today', 'vote', 'cost', 'billion']\n",
            "5::['immigra', 'iller', 'year', 'percent', 'state', 'obamacare', 'american', 'school', 'vote', 'million']\n",
            "6::['would', 'obama', 'presid', 'mccain', 'trump', 'plan', 'iraq', 'barack', 'year', 'money']\n",
            "7::['state', 'billion', 'presid', 'obama', 'iran', 'person', 'medical', 'year', 'bill', 'democrat']\n",
            "8::['state', 'rais', 'child', 'obama', 'year', 'vote', 'bill', 'every', 'first', 'wisconsin']\n",
            "9::['state', 'year', 'vote', 'percent', 'peopl', 'make', 'clinton', 'hillary', 'time', 'campaign']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuQz_8QI4r0o",
        "colab_type": "text"
      },
      "source": [
        "#### Explanation for LDA \n",
        "![[Explanation of LDA](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1508239587/n4ZpIXl_egq7mq.png)](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1508239587/n4ZpIXl_egq7mq.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_GiC6p64r0p",
        "colab_type": "text"
      },
      "source": [
        "### Semisupervised Labeling\n",
        "Based on train,test and valid data explored the topic scores for sample data and identified below topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YHJh4HL4r0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "semisupervised_topic_labels = ['topic0','topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfNtcVCK4r0u",
        "colab_type": "text"
      },
      "source": [
        "####  Function to add topicnumber, topicscore, topiclabel, topwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-gfGpbl4r0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlinetopiccolnames = ['topic_number','lda_score','topic_top_words']\n",
        "train_news = update_lda_results_to_dataset(\n",
        "    train_news, headlinetopiccolnames,'clean', dictionary_headline, lda_model_headline, lda_headline_topic_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nsXFQXM4r0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_news = update_lda_results_to_dataset(\n",
        "    test_news,headlinetopiccolnames,'clean',\n",
        "  dictionary_headline,lda_model_headline,lda_headline_topic_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFCiXYyZ4r04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_news = update_lda_results_to_dataset(\n",
        "    valid_news,headlinetopiccolnames,'clean',\n",
        "  dictionary_headline,lda_model_headline,lda_headline_topic_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eXvCNfc4r06",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the LDA Distribution of news against Top 10 Topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKVS7TZD4r08",
        "colab_type": "text"
      },
      "source": [
        "**GOAL 1:** *Each of the N documents will be represented in the LDA model by a vector of length M*\n",
        "**GOAL 2:** *Each of the M topics is represented by a vector of length V*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8fCNhkl4r09",
        "colab_type": "code",
        "outputId": "92e12a7f-006e-472a-841f-bd8b3d3226b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "import seaborn as sb\n",
        "def create_distribution(dataFile):\n",
        "    g = sb.countplot(x='topic_number', data=dataFile, palette='hls')\n",
        "    g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
        "\n",
        "    return g\n",
        "\n",
        "create_distribution(train_news) # TRAIN Document Vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4352ab5b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xQy0Jslq4r0-",
        "colab_type": "code",
        "outputId": "e2501c00-8389-44b8-dc55-1cb968da6183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "create_distribution(test_news)# TEST Document Vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4352c44d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEECAYAAADHzyg1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAV6klEQVR4nO3de7hldX3f8fcHRongBXCOZGCYDFHQYluVHpGISRCsoqFAfRQhUUfFTqfiraZFUJ/S5HlIQdNYja08U7mmBgJEZZJSFVEhidwO93scUWAmXEbxFk0gI9/+sdYsNsM5cDhz9tqH2e/X88xz9v6ttffvM7fzOWvtdUlVIUkSwDajDiBJWjgsBUlSx1KQJHUsBUlSx1KQJHUsBUlSZ9GoA2yJxYsX1/Lly0cdQ5KeUq6++urvV9XEdMue0qWwfPlypqamRh1Dkp5Sktw50zJ3H0mSOpaCJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOpaCJKnzlD55bXNT71/VyzyTnz6ll3kkqW9uKUiSOpaCJKkztFJIclqS+5PctNn4+5LcluTmJB8fGD8+ydoktyd53bBySZJmNszPFM4APgOctWkgyauBw4CXVNWDSZ7Xju8NHAm8GNgV+FqSvarqF0PMJ0nazNC2FKrqUuCBzYb/A3BSVT3YrnN/O34YcE5VPVhV3wXWAvsOK5skaXp9f6awF/DrSa5IckmSl7fjuwF3D6y3rh2TJPWo70NSFwE7A/sBLwfOTfKrT+YNkqwEVgIsW7Zs3gNK0jjre0thHfCFalwJPAwsBtYDuw+st7Qde4yqWl1Vk1U1OTEx7Y2DJElz1HcpfAl4NUCSvYCnA98H1gBHJtkuyR7AnsCVPWeTpLE3tN1HSc4GDgAWJ1kHnACcBpzWHqb6ELCiqgq4Ocm5wC3ARuAYjzySpP4NrRSq6qgZFr11hvVPBE4cVh5J0hPzjGZJUsdSkCR1LAVJUsdSkCR1LAVJUsdSkCR1LAVJUsdSkCR1LAVJUsdSkCR1LAVJUqfv+ylIY2nVt6Z6m+uUV072Npe2Pm4pSJI6loIkqWMpSJI6loIkqTO0UkhyWpL727usbb7sd5NUksXt8yT5dJK1SW5Iss+wckmSZjbMLYUzgIM3H0yyO/Ba4K6B4dfT3Jd5T2Al8Nkh5pIkzWBopVBVlwIPTLPok8CxQA2MHQacVY3LgR2TLBlWNknS9Ho9TyHJYcD6qro+yeCi3YC7B56va8fu6TGe5tmxU6t6m+vjk6f0Npee+r6+qr/zRg485al13khvpZBke+AjNLuOtuR9VtLsYmLZsmXzkEyStEmfRx89H9gDuD7J94ClwDVJfhlYD+w+sO7Sduwxqmp1VU1W1eTExMSQI0vSeOmtFKrqxqp6XlUtr6rlNLuI9qmqe4E1wNvbo5D2A35cVe46kqSeDfOQ1LOBy4AXJlmX5OjHWf1C4A5gLfC/gfcMK5ckaWZD+0yhqo56guXLBx4XcMywskiSZsczmiVJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJnV5vsqP+rOnpBjeHenMbaaviloIkqWMpSJI6loIkqWMpSJI6w7zz2mlJ7k9y08DYJ5LcluSGJF9MsuPAsuOTrE1ye5LXDSuXJGlmwzz66AzgM8BZA2MXAcdX1cYkJwPHAx9OsjdwJPBiYFfga0n2qqpfDDGfJI3U1KrTeptr8pR3zWq9oW0pVNWlwAObjX21qja2Ty8HlraPDwPOqaoHq+q7NPdq3ndY2SRJ0xvlZwrvAv5f+3g34O6BZevaMUlSj0Zy8lqSjwIbgc/P4bUrgZUAy5Ytm+dk2tqsmvpfvc11yuR7eptLGpbetxSSvAM4BPidqqp2eD2w+8BqS9uxx6iq1VU1WVWTExMTQ80qSeOm11JIcjBwLHBoVf18YNEa4Mgk2yXZA9gTuLLPbJKkIe4+SnI2cACwOMk64ASao422Ay5KAnB5Va2qqpuTnAvcQrNb6RiPPJKk/g2tFKrqqGmGT32c9U8EThxWHknSE/OMZklSx1KQJHUsBUlSx1KQJHUsBUlSx1KQJHW8R/M8mzq3n3sjA0we4f2RJc0vtxQkSR1LQZLUcfeRNCZWfXyqt7lOOXayt7k0v9xSkCR1LAVJUsdSkCR1LAVJUsdSkCR1LAVJUmdopZDktCT3J7lpYGznJBcl+Xb7dad2PEk+nWRtkhuS7DOsXJKkmQ1zS+EM4ODNxo4DLq6qPYGL2+cAr6e5L/OewErgs0PMJUmawdBKoaouBR7YbPgw4Mz28ZnA4QPjZ1XjcmDHJEuGlU2SNL2+P1PYparuaR/fC+zSPt4NuHtgvXXtmCSpRyP7oLmqCqgn+7okK5NMJZnasGHDEJJJ0vjquxTu27RbqP16fzu+Hth9YL2l7dhjVNXqqpqsqsmJiYmhhpWkcdN3KawBVrSPVwAXDIy/vT0KaT/gxwO7mSRJPRnaVVKTnA0cACxOsg44ATgJODfJ0cCdwBHt6hcCbwDWAj8H3jmsXJKkmQ2tFKrqqBkWHTTNugUcM6wskqTZmdXuoyQXz2ZMkvTU9rhbCkl+CdieZhfQTkDaRc/GQ0YlaavzRLuP/j3wQWBX4GoeKYWfAJ8ZYi5J0gg8bilU1aeATyV5X1X9cU+ZJEkjMqsPmqvqj5O8Elg++JqqOmtIuSRJIzCrUkjyJ8DzgeuAX7TDBVgKkrQVme0hqZPA3u2ho5KkrdRsz2i+CfjlYQaRJI3ebLcUFgO3JLkSeHDTYFUdOpRUkqSRmG0p/NdhhpAkLQyzPfrokmEHkSSN3myPPvopj9z74OnA04CfVdWzhxVMktS/2W4pPGvT4yShuX3mfsMKJUkajSd9P4X2PspfAl43hDySpBGa7e6jNw483YbmvIV/HEoiSdLIzPboo38z8Hgj8D2aXUiS9KT8/qqp3ub6L6dM9jbX1mK2nynM653QkvxH4N00H17fSHOntSXAOcBzaa7I+raqemg+55UkPb7Z3mRnaZIvJrm//fXnSZbOZcIkuwHvByar6p8D2wJHAicDn6yqFwA/BI6ey/tLkuZuth80nw6sobmvwq7AX7Rjc7UIeEaSRTQ38bkHOBA4v11+JnD4Fry/JGkOZlsKE1V1elVtbH+dAUzMZcKqWg/8IXAXTRn8mGZ30Y+qamO72jq8s5sk9W62pfCDJG9Nsm37663AD+YyYXtbz8OAPWi2OnYADn4Sr1+ZZCrJ1IYNG+YSQZI0g9mWwruAI4B7aX66fxPwjjnO+Rrgu1W1oar+CfgCsD+wY7s7CWApsH66F1fV6qqarKrJiYk5baxIkmYw21L4fWBFVU1U1fNoSuL35jjnXcB+SbZvz44+CLgF+AZN2QCsAC6Y4/tLkuZotqXwL6vqh5ueVNUDwMvmMmFVXUHzgfI1NIejbgOsBj4MfCjJWprDUk+dy/tLkuZutievbZNkp03FkGTnJ/Hax6iqE4ATNhu+A9h3ru8pSdpys/3G/t+By5Kc1z5/M3DicCJJkkZltmc0n5VkiuZcAoA3VtUtw4slSRqFWe8CakvAIpCkrdiTvnS2JGnrZSlIkjqWgiSpYylIkjqWgiSpYylIkjqWgiSpYylIkjqWgiSpYylIkjqWgiSpYylIkjqWgiSpM5JSSLJjkvOT3Jbk1iS/lmTnJBcl+Xb7dadRZJOkcTaqLYVPAV+uqhcBLwFuBY4DLq6qPYGL2+eSpB71XgpJngP8Bu09mKvqoar6EXAYcGa72pnA4X1nk6RxN4othT2ADcDpSa5N8rkkOwC7VNU97Tr3AruMIJskjbVRlMIiYB/gs1X1MuBnbLarqKoKqOlenGRlkqkkUxs2bBh6WEkaJ6MohXXAuqq6on1+Pk1J3JdkCUD79f7pXlxVq6tqsqomJyYmegksSeOi91KoqnuBu5O8sB06iObez2uAFe3YCuCCvrNJ0rhbNKJ53wd8PsnTgTuAd9IU1LlJjgbuBI4YUTZJGlsjKYWqug6YnGbRQX1nkSQ9wjOaJUkdS0GS1LEUJEkdS0GS1LEUJEkdS0GS1LEUJEkdS0GS1LEUJEkdS0GS1LEUJEkdS0GS1LEUJEkdS0GS1LEUJEkdS0GS1BlZKSTZNsm1Sf6yfb5HkiuSrE3yZ+1d2SRJPRrllsIHgFsHnp8MfLKqXgD8EDh6JKkkaYyNpBSSLAV+C/hc+zzAgcD57SpnAoePIpskjbNRbSn8D+BY4OH2+XOBH1XVxvb5OmC3UQSTpHHWeykkOQS4v6qunuPrVyaZSjK1YcOGeU4nSeNtFFsK+wOHJvkecA7NbqNPATsmWdSusxRYP92Lq2p1VU1W1eTExEQfeSVpbPReClV1fFUtrarlwJHA16vqd4BvAG9qV1sBXNB3NkkadwvpPIUPAx9KspbmM4ZTR5xHksbOoideZXiq6pvAN9vHdwD7jjKPJI27hbSlIEkaMUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJHUtBktSxFCRJnd5LIcnuSb6R5JYkNyf5QDu+c5KLkny7/bpT39kkadyNYkthI/C7VbU3sB9wTJK9geOAi6tqT+Di9rkkqUe9l0JV3VNV17SPfwrcCuwGHAac2a52JnB439kkadyN9DOFJMuBlwFXALtU1T3tonuBXWZ4zcokU0mmNmzY0EtOSRoXIyuFJM8E/hz4YFX9ZHBZVRVQ072uqlZX1WRVTU5MTPSQVJLGx0hKIcnTaArh81X1hXb4viRL2uVLgPtHkU2Sxtkojj4KcCpwa1X90cCiNcCK9vEK4IK+s0nSuFs0gjn3B94G3JjkunbsI8BJwLlJjgbuBI4YQTZJGmu9l0JV/TWQGRYf1GcWSdKjeUazJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOpaCJKljKUiSOguuFJIcnOT2JGuTHDfqPJI0ThZUKSTZFvifwOuBvYGjkuw92lSSND4WVCkA+wJrq+qOqnoIOAc4bMSZJGlsLLRS2A24e+D5unZMktSDVNWoM3SSvAk4uKre3T5/G/CKqnrvwDorgZXt0xcCt2/htIuB72/he2yphZABFkaOhZABFkaOhZABFkaOhZABFkaO+cjwK1U1Md2CRVv4xvNtPbD7wPOl7VinqlYDq+drwiRTVTU5X+/3VM2wUHIshAwLJcdCyLBQciyEDAslx7AzLLTdR1cBeybZI8nTgSOBNSPOJEljY0FtKVTVxiTvBb4CbAucVlU3jziWJI2NBVUKAFV1IXBhj1PO266oLbAQMsDCyLEQMsDCyLEQMsDCyLEQMsDCyDHUDAvqg2ZJ0mgttM8UJEkjZClIkjoL7jOFYUvyIpqzpDedFLceWFNVt44u1Wi0fxa7AVdU1d8PjB9cVV/uKcO+QFXVVe0lTQ4Gbms/WxqJJGdV1dtHNX+b4VU0Z/jfVFVf7XHeVwC3VtVPkjwDOA7YB7gF+IOq+nEPGd4PfLGq7n7ClYeXYdPRj39XVV9L8tvAK4FbgdVV9U89ZvlV4I00h+v/Avhb4E+r6idDmW+cPlNI8mHgKJrLZ6xrh5fS/OWfU1UnjSrbJkneWVWn9zDP+4FjaP6RvxT4QFVd0C67pqr26SHDCTTXuVoEXAS8AvgG8K+Br1TViT1k2PyQ5wCvBr4OUFWHDjtDm+PKqtq3ffzvaP5uvgi8FviLvv5tJrkZeEl7JOBq4OfA+cBB7fgbe8jwY+BnwHeAs4HzqmrDsOfdLMPnaf5dbg/8CHgm8AWaP4dU1YqecrwfOAS4FHgDcG2b598C76mqb877pFU1Nr9oGvZp04w/Hfj2qPO1We7qaZ4bgWe2j5cDUzTFAHBtjxm2pfmP9xPg2e34M4AbespwDfB/gAOA32y/3tM+/s0e/96vHXh8FTDRPt4BuLHHHLcO/tlstuy6vv4saHZtvxY4FdgAfBlYATyrpww3tF8XAfcB27bP09e/zXa+Gwfm3h74Zvt42bD+n47b7qOHgV2BOzcbX9Iu60WSG2ZaBOzSU4xtqt1lVFXfS3IAcH6SX2lz9GFjVf0C+HmS71S7OVxV/5Ckr7+PSeADwEeB/1xV1yX5h6q6pKf5N9kmyU403wxT7U/GVfWzJBt7zHHTwNbq9Ukmq2oqyV5AX7tMqqoeBr4KfDXJ02i2KI8C/hCY9vIM82ybdhfSDjTfjJ8DPABsBzyth/kHLaLZbbQdzRYLVXVX++cylMnGyQeBi5N8m0cuvLcMeAHw3hlfNf92AV4H/HCz8QDf6inDfUleWlXXAVTV3yc5BDgN+Bc9ZXgoyfZV9XPgX20aTPIceirp9pvPJ5Oc1369j9H8v3gOcDXNv4FKsqSq7knyTPoraYB3A59K8jGa6+tcluRumv8v7+4pw6N+v9Xsv18DrEmyfU8ZTgVuo9mS/ShwXpI7gP1odj/35XPAVUmuAH4dOBkgyQRNSc27sfpMASDJNjQf4A1+0HxV+xNrXxlOBU6vqr+eZtmfVtVv95BhKc1P6vdOs2z/qvqbHjJsV1UPTjO+GFhSVTcOO8M0c/8WsH9VfaTvuafTfhPcpaq+2/O8zwb2oCnIdVV1X49z71VVf9vXfI+TY1eAqvq7JDsCr6HZvXtlzzleDPwzmoMObhv6fONWCpKkmXmegiSpYylIkjqWgiSpYyloq5NkxyTv2YLXX9h+sDhySQ5I8pejzqHxYSloa7QjMOdSqKo3VNWP5jHPyCTZdtQZ9NRiKWhrdBLw/CTXJflE++umJDcmeQt0P4FfmuT/Jrk9ySnt4cok+V57WCxJ3p7khiTXJ/mTmSZMckaSTyf5VpI72vuNP+Yn/SSfSfKOgXn+W5tzKsk+Sb6S5DtJVg28/bNnyPnaJJcluSbJee05DZve9+Qk1wBvns8/WG39LAVtjY4DvlNVLwUup7m200tojjP/RJIl7Xr7Au8D9gaeT3PRsU57fPjHgAOr6iU0Zz4/niXAq2iuVTPbaxXd1eb8K+AM4E00J0j93sA6j8nZltbHgNdUc52qKeBDA6/5QVXtU1V9nmilrcC4ndGs8fMq4Oz25MT7klwCvJzmWktXVtUdAEnObtc9f+C1B9JcjO37AFX1RGeQfqk9Q/qWJLO9XMmmC/JtuhbVT4GfJnlw4HON6XL+I01J/E0SaK7fddnA+/7ZLOeXHsVS0Djb/MzNLT2Tc/Ds7E2XatjIo7fIf2mG1zy82esf5pH/n9PlDHBRVR01Q5afzSawtDl3H2lr9FPgWe3jvwLekmTb9noxvwFsukzBvkn2aPfRvwXY/LIjXwfenOS5AEl2nkOWO4G9k2zX/uR/0BzeY7qclwP7J3lBm22H9qJ10haxFLTVqaof0OxWuQn4NeAG4Hqab/LHDlzv6SrgMzT3lPguzf0LBt/nZuBE4JIk1wN/NIcsdwPnAje1X6+dw2/pMTnbq6i+Azi7veruZcCL5vDe0qN47SONpfZS4f+pqg4ZdRZpIXFLQZLUcUtBehKSfJTHHvt/XvVw61CpD5aCJKnj7iNJUsdSkCR1LAVJUsdSkCR1LAVJUuf/A6PxChUYY0VXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jjdfABZX4r1A",
        "colab_type": "code",
        "outputId": "7a6132f1-ae4c-4958-f825-06b06254c3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "create_distribution(valid_news)# VALID Document Vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4352e3ccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXbFFeOz4r1E",
        "colab_type": "text"
      },
      "source": [
        "##### Saved the latest dataset into a seperate CSV file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SOUcaMv4r1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "train_lda_file = dataset_path + \"/train_lda.csv\"\n",
        "test_lda_file = dataset_path + \"/test_lda.csv\"\n",
        "valid_lda_file = dataset_path + \"/valid_lda.csv\"\n",
        "\n",
        "if not path.exists(train_lda_file):\n",
        "    train_news.to_csv(train_lda_file, sep=',')\n",
        "if not path.exists(test_lda_file):\n",
        "    test_news.to_csv(test_lda_file, sep=',')\n",
        "if not path.exists(valid_lda_file):\n",
        "    valid_news.to_csv(valid_lda_file, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6qRlYuRUIDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class LDATopicModelling():\n",
        "\n",
        "    def __init__(self):\n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        train_lda_file = dataset_path + \"/train_lda.csv\"\n",
        "        test_lda_file = dataset_path + \"/test_lda.csv\"\n",
        "        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\", \"index\", \"topic_number\", \"lda_score\", \"topic_top_words\"]\n",
        "        dataTrain = pd.read_csv(train_lda_file, sep=',', header=None, names = columnNames)\n",
        "        dataTest = pd.read_csv(test_lda_file, sep=',', header=None, names = columnNames)\n",
        "\n",
        "        #dropping columns\n",
        "        columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector', 'sentiment_vector','vader_polarity', 'sentiment_score', 'index']\n",
        "        dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
        "        dataTest = dataTest.drop(columns=columnsToRemove)\n",
        "        dataTrain = dataTrain.loc[1:] \n",
        "        dataTest = dataTest.loc[1:]\n",
        "    \n",
        "    \n",
        "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "                ('LogRCV', tfidfV),\n",
        "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
        "                ])\n",
        "\n",
        "        self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['topic_number'])\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
        "        score = metrics.accuracy_score(dataTest['topic_number'], predicted_LogR)\n",
        "        # print(\"LDA Topic Model Trained - accuracy:   %0.6f\" % score)\n",
        "        \n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return bool(predicted), float(predicedProb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM6g_rllUde1",
        "colab_type": "code",
        "outputId": "4fb86b89-2449-477d-ebc3-30c383a14ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getLDATopicModellingScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/lda_topic_modelling.pkl','rb') as f:\n",
        "        ldaTopicModelling = pickle.load(f)\n",
        "    binaryValue, probValue = ldaTopicModelling.predict(text)\n",
        "    return float(probValue)\n",
        "\n",
        "DATAMINERS_getLDATopicModellingScore(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08185803576455541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKS_0vi24r1J",
        "colab_type": "text"
      },
      "source": [
        "#3:  Sensational Feature Analysis\n",
        "\n",
        "#### [Sensational Words Corpus](https://www.thepersuasionrevolution.com/380-high-emotion-persuasive-words/)\n",
        "\n",
        ">  Words aren’t just strings of alphabets sewn together with ink. Words are cues. Words are triggers. Words when used correctly can transform an “eh whatever” into “wow that’s it!”. Words can make you go from literally ROFL to fuming with fury to an uncontrollable-urge-to-take-action-NOW-or-the-earth-may-stop-swinging -on-its-axis.\n",
        "\n",
        "> Highly emotional words are capable capable of transforming an absolute no into almost yes and a “perhaps” into “for sure”!\n",
        "\n",
        "Words that are used:\n",
        "- When you are trying to sell people a solution\n",
        "- When you are trying to get them to take an action (like, share, subscribe, buy)\n",
        "- When you are trying to get people to click and read your article\n",
        "- When you are trying to get someone to agree with you\n",
        "\n",
        "**There are 1400+ words that are both positive and negative emotions that will help to predict the sensational score for an article**\n",
        "\n",
        "> I have used these words to perform cosin similarity and predict the sensational similarity score for each news in the give dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkShZJx5BWnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\", \"index\", \"topic_number\", \"lda_score\", \"topic_top_words\"]\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "dataTrain = pd.read_csv(dataset_path  + '/train_lda.csv', sep=',', header=None, names = columnNames)\n",
        "dataTest = pd.read_csv(dataset_path  + '/test_lda.csv', sep=',', header=None, names = columnNames)\n",
        "\n",
        "#dropping columns\n",
        "columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector', 'sentiment_vector','vader_polarity', 'sentiment_score', 'index', 'topic_number', 'lda_score', 'topic_top_words']\n",
        "train_news = dataTrain.drop(columns=columnsToRemove)\n",
        "test_news = dataTest.drop(columns=columnsToRemove)\n",
        "train_news = train_news.loc[1:] \n",
        "test_news = test_news.loc[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FP7qje54r1K",
        "colab_type": "code",
        "outputId": "2aeb44e0-9e1e-404a-93de-8566e2effdb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus = []\n",
        "for i in train_news['clean']:\n",
        "    corpus.append(i)\n",
        "# corpus\n",
        "\n",
        "sensational_corpus=[]\n",
        "sensational_words = pd.read_csv(\"/content/drive/My Drive/junteng_dev/data_set/sensational_words_dict.csv\", sep=\"\\t+\", header=None, usecols=[0] )\n",
        "print(len(sensational_words))\n",
        "sensational_dictionary = ' '.join(sensational_words[0].astype(str))\n",
        "sensational_corpus.append(sensational_dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7si0WIM4r1R",
        "colab_type": "text"
      },
      "source": [
        "## SenticNet:   \n",
        "\n",
        "#### Data Enrichment \n",
        "\n",
        "> It provides polarity associated with 50,000 natural language concepts. A polarity is a floating number between -1 and +1. Minus one is extreme negativity, and plus one is extreme positivity. The knowledge base is free. It can be downloaded as XML file. \n",
        "SenticNet 5 reaches 100,000 commonsense concepts by employing recurrent neural networks to infer primitives by lexical substitution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1oktgaNv4r1S",
        "colab_type": "code",
        "outputId": "3d5a7765-e48f-4d93-8470-267ed5ae37da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sentic_net = pd.read_csv(\"/content/drive/My Drive/junteng_dev/data_set/senticnet5.txt\", sep=\"\\t+\", header=None, usecols=[0,1,2], names = [\"Token\", \"Polarity\", \"Intensity\"])\n",
        "sentic_net = sentic_net[~sentic_net['Token'].str.contains('|'.join('_'),na=False)]\n",
        "sentic_net = sentic_net.reset_index(drop=True)\n",
        "print(\"Senticnet Vocab Size: \",len(sentic_net))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Senticnet Vocab Size:  39891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbBIB9Tb4r1U",
        "colab_type": "code",
        "outputId": "2c9b5101-b130-4824-c7af-61c94e35a853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# sentic_net['Token'] provides list of words from the SenticNet DICTIONARY\n",
        "senti_pos = sentic_net.loc[sentic_net.Polarity == \"positive\"]\n",
        "senti_pos = senti_pos.loc[senti_pos.Intensity > 0.90]\n",
        "dictionary = ' '.join(senti_pos.Token.astype(str))\n",
        "sensational_corpus.append(dictionary)\n",
        "print(len(senti_pos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WykaFuoe4r1e",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF and Cosine Similarity\n",
        "\n",
        "#### TF-IDF\n",
        "\n",
        "> TF-IDF (Term Frequency - Inverse Document Frequency) can be represented tf(d,t) X idf(t). TF-IDF uses the method diminishing the weight (importance) of words appeared in many documents in common, considered them incapable of discerning the documents, rather than simply counting the frequency of words as CountVectorizer does. The outcome matrix consists of each document (row) and each word (column) and the importance (weight) computed by tf * idf (values of the matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tggvUhI74r1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfVec = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "tfidf_corpus = tfidfVec.fit_transform(corpus)\n",
        "tf_idf_senti = tfidfVec.fit_transform(sensational_corpus)\n",
        "words = tfidfVec.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq77eo4c4r1i",
        "colab_type": "code",
        "outputId": "f9e229ab-0245-43e3-fc55-4f07cd0f1da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "tfidf_corpus.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8b6o5Jp4r1p",
        "colab_type": "code",
        "outputId": "e25505c5-2f85-49ed-b044-7a931ab7a3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "tf_idf_senti.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02934579, 0.05869158, 0.05869158, ..., 0.        , 0.        ,\n",
              "        0.08803737],\n",
              "       [0.        , 0.        , 0.        , ..., 0.19269152, 0.06423051,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpLIE_Rs4r1t",
        "colab_type": "code",
        "outputId": "52a70fac-b009-4744-c4d1-685d26a1ff46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tfidfVec.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'absolutely': 1,\n",
              " 'advantage': 3,\n",
              " 'qaeda': 742,\n",
              " 'amazing': 4,\n",
              " 'antagonistic': 6,\n",
              " 'attack': 14,\n",
              " 'authentic': 15,\n",
              " 'authority': 16,\n",
              " 'banned': 21,\n",
              " 'behind': 24,\n",
              " 'the': 929,\n",
              " 'scenes': 881,\n",
              " 'best': 26,\n",
              " 'bill': 28,\n",
              " 'bitterness': 29,\n",
              " 'black': 31,\n",
              " 'market': 187,\n",
              " 'blacklisted': 32,\n",
              " 'blissful': 34,\n",
              " 'up': 990,\n",
              " 'breathtaking': 37,\n",
              " 'campaign': 40,\n",
              " 'for': 95,\n",
              " 'censored': 42,\n",
              " 'cheer': 44,\n",
              " 'cheerful': 45,\n",
              " 'church': 47,\n",
              " 'class': 48,\n",
              " 'concealed': 52,\n",
              " 'confessions': 53,\n",
              " 'confidential': 55,\n",
              " 'control': 59,\n",
              " 'controversial': 61,\n",
              " 'cost': 62,\n",
              " 'cover': 63,\n",
              " 'crime': 64,\n",
              " 'critical': 65,\n",
              " 'infrastructure': 126,\n",
              " 'cyber': 67,\n",
              " 'security': 886,\n",
              " 'terror': 928,\n",
              " 'daring': 68,\n",
              " 'of': 388,\n",
              " 'desire': 71,\n",
              " 'disastrous': 72,\n",
              " 'drug': 76,\n",
              " 'ecstatic': 79,\n",
              " 'on': 409,\n",
              " 'emergency': 81,\n",
              " 'response': 859,\n",
              " 'enforcement': 83,\n",
              " 'eye': 85,\n",
              " 'opening': 415,\n",
              " 'faith': 86,\n",
              " 'faithfulness': 87,\n",
              " 'fbi': 89,\n",
              " 'first': 93,\n",
              " 'responder': 858,\n",
              " 'high': 118,\n",
              " 'forbidden': 96,\n",
              " 'force': 97,\n",
              " 'fire': 92,\n",
              " 'foul': 99,\n",
              " 'freedom': 101,\n",
              " 'gas': 104,\n",
              " 'gleeful': 107,\n",
              " 'grateful': 112,\n",
              " 'grid': 113,\n",
              " 'gun': 114,\n",
              " 'heart': 116,\n",
              " 'hidden': 117,\n",
              " 'horrific': 120,\n",
              " 'immigration': 122,\n",
              " 'inquiring': 127,\n",
              " 'mind': 253,\n",
              " 'insider': 128,\n",
              " 'investment': 131,\n",
              " 'inviolability': 132,\n",
              " 'jubilant': 133,\n",
              " 'keen': 134,\n",
              " 'law': 136,\n",
              " 'light': 140,\n",
              " 'lunatic': 147,\n",
              " 'lurking': 151,\n",
              " 'lying': 154,\n",
              " 'mad': 160,\n",
              " 'magic': 161,\n",
              " 'malware': 174,\n",
              " 'markets': 188,\n",
              " 'marriage': 193,\n",
              " 'maul': 210,\n",
              " 'meddlesome': 217,\n",
              " 'media': 218,\n",
              " 'merry': 234,\n",
              " 'mexico': 244,\n",
              " 'military': 249,\n",
              " 'militia': 250,\n",
              " 'blowing': 35,\n",
              " 'miracle': 258,\n",
              " 'mistake': 265,\n",
              " 'money': 276,\n",
              " 'motivation': 288,\n",
              " 'murder': 300,\n",
              " 'muse': 302,\n",
              " 'naked': 310,\n",
              " 'narcotics': 313,\n",
              " 'nation': 319,\n",
              " 'national': 320,\n",
              " 'preparedness': 652,\n",
              " 'nations': 321,\n",
              " 'naughty': 325,\n",
              " 'need': 327,\n",
              " 'news': 339,\n",
              " 'newsmonger': 342,\n",
              " 'nightmare': 346,\n",
              " 'no': 351,\n",
              " 'obligation': 375,\n",
              " 'questions': 751,\n",
              " 'strings': 914,\n",
              " 'nosiness': 365,\n",
              " 'notion': 367,\n",
              " 'nuclear': 370,\n",
              " 'threat': 931,\n",
              " 'numbers': 371,\n",
              " 'obama': 372,\n",
              " 'obnoxious': 376,\n",
              " 'offensive': 391,\n",
              " 'official': 394,\n",
              " 'officials': 395,\n",
              " 'officiousness': 397,\n",
              " 'off': 389,\n",
              " 'oil': 399,\n",
              " 'oklahoma': 403,\n",
              " 'nine': 348,\n",
              " 'organized': 427,\n",
              " 'outage': 434,\n",
              " 'outbreak': 435,\n",
              " 'outlawed': 440,\n",
              " 'outrage': 447,\n",
              " 'overjoyed': 465,\n",
              " 'painful': 478,\n",
              " 'parents': 501,\n",
              " 'parties': 506,\n",
              " 'passion': 508,\n",
              " 'patients': 516,\n",
              " 'payback': 520,\n",
              " 'peace': 521,\n",
              " 'peaceful': 522,\n",
              " 'peeping': 527,\n",
              " 'peril': 545,\n",
              " 'perky': 548,\n",
              " 'perpetuation': 550,\n",
              " 'phishing': 561,\n",
              " 'phobia': 562,\n",
              " 'played': 580,\n",
              " 'playful': 581,\n",
              " 'plummet': 588,\n",
              " 'poison': 596,\n",
              " 'police': 598,\n",
              " 'policies': 600,\n",
              " 'policy': 601,\n",
              " 'politics': 603,\n",
              " 'poll': 604,\n",
              " 'population': 617,\n",
              " 'pound': 636,\n",
              " 'power': 638,\n",
              " 'praise': 641,\n",
              " 'preposterous': 654,\n",
              " 'preserve': 659,\n",
              " 'president': 660,\n",
              " 'price': 664,\n",
              " 'prices': 665,\n",
              " 'primary': 666,\n",
              " 'prison': 670,\n",
              " 'privacy': 672,\n",
              " 'private': 673,\n",
              " 'progress': 688,\n",
              " 'projects': 689,\n",
              " 'promise': 693,\n",
              " 'promote': 694,\n",
              " 'prop': 697,\n",
              " 'propose': 701,\n",
              " 'protected': 707,\n",
              " 'proven': 712,\n",
              " 'provocative': 715,\n",
              " 'prurience': 717,\n",
              " 'prying': 719,\n",
              " 'psych': 721,\n",
              " 'punish': 733,\n",
              " 'questioning': 749,\n",
              " 'race': 760,\n",
              " 'rage': 766,\n",
              " 'raise': 768,\n",
              " 'rally': 770,\n",
              " 'rankle': 772,\n",
              " 'rate': 775,\n",
              " 'reason': 779,\n",
              " 'reassure': 781,\n",
              " 'recession': 784,\n",
              " 'proof': 696,\n",
              " 'recommend': 788,\n",
              " 'recovery': 792,\n",
              " 'reform': 810,\n",
              " 'refresh': 811,\n",
              " 'refund': 813,\n",
              " 'regard': 814,\n",
              " 'reliability': 826,\n",
              " 'republican': 841,\n",
              " 'republicans': 842,\n",
              " 'research': 846,\n",
              " 'resentment': 850,\n",
              " 'restrictions': 864,\n",
              " 'results': 867,\n",
              " 'revelation': 875,\n",
              " 'scandalous': 880,\n",
              " 'self': 887,\n",
              " 'indulgence': 125,\n",
              " 'sensational': 888,\n",
              " 'sensual': 891,\n",
              " 'sex': 894,\n",
              " 'and': 5,\n",
              " 'tired': 935,\n",
              " 'silly': 898,\n",
              " 'spectacular': 905,\n",
              " 'state': 909,\n",
              " 'stir': 911,\n",
              " 'stock': 912,\n",
              " 'stunning': 916,\n",
              " 'suicide': 918,\n",
              " 'sunny': 919,\n",
              " 'suspicious': 923,\n",
              " 'tantalizing': 925,\n",
              " 'tawdry': 926,\n",
              " 'thirst': 930,\n",
              " 'knowledge': 135,\n",
              " 'thrilling': 933,\n",
              " 'tickled': 934,\n",
              " 'tornado': 948,\n",
              " 'toxic': 958,\n",
              " 'trafficking': 959,\n",
              " 'transparency': 967,\n",
              " 'transportation': 968,\n",
              " 'trap': 969,\n",
              " 'tremor': 976,\n",
              " 'trepidation': 979,\n",
              " 'trial': 981,\n",
              " 'trustworthiness': 985,\n",
              " 'you': 999,\n",
              " 'unconditional': 988,\n",
              " 'upbeat': 991,\n",
              " 'urge': 993,\n",
              " 'wonderful': 995,\n",
              " 'world': 996,\n",
              " 'secret': 885,\n",
              " 'underground': 989,\n",
              " 'one': 410,\n",
              " 'plot': 586,\n",
              " 'record': 791,\n",
              " 'unbelievable': 986,\n",
              " 'magical': 162,\n",
              " 'instantly': 129,\n",
              " 'missing': 264,\n",
              " 'out': 433,\n",
              " 'magnificent': 164,\n",
              " 'most': 285,\n",
              " 'profitable': 686,\n",
              " 'quick': 752,\n",
              " 'remarkable': 833,\n",
              " 'startling': 908,\n",
              " 'strongly': 915,\n",
              " 'superb': 920,\n",
              " 'tremendous': 975,\n",
              " 'bargain': 22,\n",
              " 'discount': 73,\n",
              " 'reduced': 802,\n",
              " 'skyrocket': 902,\n",
              " 'perplexed': 551,\n",
              " 'misgiving': 261,\n",
              " 'manipulative': 180,\n",
              " 'paralyzed': 499,\n",
              " 'pathetic': 514,\n",
              " 'overwhelmed': 469,\n",
              " 'trapped': 971,\n",
              " 'panicked': 489,\n",
              " 'ordeal': 425,\n",
              " 'outrageousness': 449,\n",
              " 'provoke': 716,\n",
              " 'repulsive': 844,\n",
              " 'shocking': 897,\n",
              " 'tragic': 960,\n",
              " 'dreadful': 75,\n",
              " 'controlling': 60,\n",
              " 'resentful': 849,\n",
              " 'malicious': 170,\n",
              " 'repulsed': 843,\n",
              " 'quarrelsome': 747,\n",
              " 'rebellious': 782,\n",
              " 'poisonous': 597,\n",
              " 'retaliating': 868,\n",
              " 'reprimanding': 840,\n",
              " 'powerless': 640,\n",
              " 'pessimistic': 556,\n",
              " 'cut': 66,\n",
              " 'certain': 43,\n",
              " 'confident': 54,\n",
              " 'delighted': 70,\n",
              " 'effective': 80,\n",
              " 'conscientious': 56,\n",
              " 'approving': 8,\n",
              " 'honored': 119,\n",
              " 'privileged': 674,\n",
              " 'adaptable': 2,\n",
              " 'relaxed': 824,\n",
              " 'astonishing': 12,\n",
              " 'assured': 11,\n",
              " 'fulfilled': 102,\n",
              " 'genuine': 106,\n",
              " 'sufficient': 917,\n",
              " 'reliable': 827,\n",
              " 'sure': 922,\n",
              " 'excellent': 84,\n",
              " 'responsible': 861,\n",
              " 'trusting': 984,\n",
              " 'supported': 921,\n",
              " 'humility': 121,\n",
              " 'glorious': 108,\n",
              " 'like': 141,\n",
              " 'top': 946,\n",
              " 'optimistic': 420,\n",
              " 'spirited': 906,\n",
              " 'funny': 103,\n",
              " 'intelligent': 130,\n",
              " 'resourceful': 854,\n",
              " 'at': 13,\n",
              " 'ease': 78,\n",
              " 'comfortable': 50,\n",
              " 'pleased': 583,\n",
              " 'content': 58,\n",
              " 'serene': 892,\n",
              " 'bright': 38,\n",
              " 'blessed': 33,\n",
              " 'glowing': 109,\n",
              " 'motivated': 287,\n",
              " 'earnest': 77,\n",
              " 'brave': 36,\n",
              " 'clear': 49,\n",
              " 'balanced': 20,\n",
              " 'okay': 400,\n",
              " 'carefree': 41,\n",
              " 'forgiving': 98,\n",
              " 'sincere': 899,\n",
              " 'uplifted': 992,\n",
              " 'unburdened': 987,\n",
              " 'productive': 680,\n",
              " 'in': 124,\n",
              " 'responsive': 862,\n",
              " 'calm': 39,\n",
              " 'quiet': 754,\n",
              " 'radiant': 763,\n",
              " 'reflective': 808,\n",
              " 'open': 414,\n",
              " 'minded': 254,\n",
              " 'non': 359,\n",
              " 'aware': 17,\n",
              " 'meditative': 222,\n",
              " 'rested': 863,\n",
              " 'natural': 323,\n",
              " 'placid': 576,\n",
              " 'lurid': 149,\n",
              " 'arresting': 10,\n",
              " 'sensationalistic': 889,\n",
              " 'bizarre': 30,\n",
              " 'memorable': 224,\n",
              " 'riveting': 878,\n",
              " 'outrageous': 448,\n",
              " 'titillating': 936,\n",
              " 'perceptual': 536,\n",
              " 'sense': 890,\n",
              " 'resent': 848,\n",
              " 'publicized': 726,\n",
              " 'marred': 192,\n",
              " 'misses': 263,\n",
              " 'perceptible': 533,\n",
              " 'outing': 438,\n",
              " 'perceptive': 535,\n",
              " 'questionable': 748,\n",
              " 'macabre': 158,\n",
              " 'feel': 91,\n",
              " 'moments': 274,\n",
              " 'peaty': 523,\n",
              " 'nasty': 316,\n",
              " 'tactile': 924,\n",
              " 'modality': 269,\n",
              " 'overshadowed': 466,\n",
              " 'touchy': 955,\n",
              " 'mistakes': 266,\n",
              " 'mediocre': 221,\n",
              " 'olfactory': 408,\n",
              " 'match': 207,\n",
              " 'posting': 631,\n",
              " 'nociception': 352,\n",
              " 'smell': 903,\n",
              " 'perception': 534,\n",
              " 'nasute': 317,\n",
              " 'noisome': 357,\n",
              " 'macrosmatic': 159,\n",
              " 'pong': 611,\n",
              " 'redolent': 801,\n",
              " 'low': 142,\n",
              " 'scoring': 883,\n",
              " 'odour': 387,\n",
              " 'perceptualize': 537,\n",
              " 'five': 94,\n",
              " 'set': 893,\n",
              " 'olfaction': 406,\n",
              " 'stink': 910,\n",
              " 'odor': 383,\n",
              " 'marvelous': 199,\n",
              " 'scent': 882,\n",
              " 'nonfeeling': 360,\n",
              " 'phenomenal': 559,\n",
              " 'aroma': 9,\n",
              " 'sniff': 904,\n",
              " 'percipient': 538,\n",
              " 'pathos': 515,\n",
              " 'perceive': 531,\n",
              " 'palpable': 484,\n",
              " 'miscue': 260,\n",
              " 'touch': 950,\n",
              " 'nonsensical': 362,\n",
              " 'awareness': 18,\n",
              " 'malodorous': 173,\n",
              " 'needlefelt': 328,\n",
              " 'emotionally': 82,\n",
              " 'olfactometer': 407,\n",
              " 'pungency': 732,\n",
              " 'toucher': 952,\n",
              " 'pongy': 612,\n",
              " 'odorous': 386,\n",
              " 'striking': 913,\n",
              " 'conscious': 57,\n",
              " 'ostentatious': 432,\n",
              " 'perfume': 542,\n",
              " 'title': 938,\n",
              " 'proprioception': 703,\n",
              " 'odorant': 384,\n",
              " 'palp': 483,\n",
              " 'niffy': 344,\n",
              " 'malodor': 172,\n",
              " 'odorize': 385,\n",
              " 'mouthgasm': 290,\n",
              " 'touchable': 951,\n",
              " 'perceivedness': 532,\n",
              " 'splendid': 907,\n",
              " 'mesmerizing': 238,\n",
              " 'gutsy': 115,\n",
              " 'masterful': 204,\n",
              " 'nerveless': 333,\n",
              " 'masterly': 205,\n",
              " 'miraculous': 259,\n",
              " 'torrid': 949,\n",
              " 'outlandish': 439,\n",
              " 'mesmeric': 236,\n",
              " 'palpate': 486,\n",
              " 'over': 459,\n",
              " 'titillation': 937,\n",
              " 'reek': 804,\n",
              " 'misperceive': 262,\n",
              " 'rat': 774,\n",
              " 'six': 900,\n",
              " 'fee': 90,\n",
              " 'down': 74,\n",
              " 'property': 699,\n",
              " 'common': 51,\n",
              " 'topic': 947,\n",
              " 'mental': 228,\n",
              " 'object': 373,\n",
              " 'nose': 364,\n",
              " 'multi': 292,\n",
              " 'abandon': 0,\n",
              " 'to': 941,\n",
              " 'anti': 7,\n",
              " 'beefed': 23,\n",
              " 'better': 27,\n",
              " 'rimmed': 877,\n",
              " 'ranging': 771,\n",
              " 'chosen': 46,\n",
              " 'going': 110,\n",
              " 'away': 19,\n",
              " 'shaking': 895,\n",
              " 'far': 88,\n",
              " 'length': 137,\n",
              " 'year': 997,\n",
              " 'old': 405,\n",
              " 'four': 100,\n",
              " 'genre': 105,\n",
              " 'bending': 25,\n",
              " 'grade': 111,\n",
              " 'lidded': 138,\n",
              " 'tech': 927,\n",
              " 'quality': 745,\n",
              " 'impressionism': 123,\n",
              " 'life': 139,\n",
              " 'shattering': 896,\n",
              " 'lowball': 143,\n",
              " 'lowliest': 144,\n",
              " 'lucky': 145,\n",
              " 'lunar': 146,\n",
              " 'lunchbox': 148,\n",
              " 'luridly': 150,\n",
              " 'lusty': 152,\n",
              " 'lutenist': 153,\n",
              " 'lynx': 155,\n",
              " 'lyricist': 156,\n",
              " 'mabbutt': 157,\n",
              " 'magician': 163,\n",
              " 'mahmudvand': 165,\n",
              " 'mailserver': 166,\n",
              " 'mailto': 167,\n",
              " 'mainframe': 168,\n",
              " 'maintainable': 169,\n",
              " 'malignaggi': 171,\n",
              " 'mama': 175,\n",
              " 'mammoth': 176,\n",
              " 'managerial': 177,\n",
              " 'mandatum': 178,\n",
              " 'manhattan': 179,\n",
              " 'manly': 181,\n",
              " 'mannersmith': 182,\n",
              " 'manorial': 183,\n",
              " 'manufacturing': 184,\n",
              " 'mapping': 185,\n",
              " 'marijuana': 186,\n",
              " 'marksmanship': 189,\n",
              " 'marksmen': 190,\n",
              " 'marquessate': 191,\n",
              " 'married': 194,\n",
              " 'marry': 195,\n",
              " 'marsh': 196,\n",
              " 'marshland': 197,\n",
              " 'martian': 198,\n",
              " 'marxism': 200,\n",
              " 'marxist': 201,\n",
              " 'masonry': 202,\n",
              " 'masterclass': 203,\n",
              " 'mastodon': 206,\n",
              " 'matinee': 208,\n",
              " 'matured': 209,\n",
              " 'maximal': 211,\n",
              " 'maximize': 212,\n",
              " 'mdewakanton': 213,\n",
              " 'meaning': 214,\n",
              " 'measurable': 215,\n",
              " 'mechanical': 216,\n",
              " 'medical': 219,\n",
              " 'medicinally': 220,\n",
              " 'membrane': 223,\n",
              " 'menarche': 225,\n",
              " 'menopause': 226,\n",
              " 'menstruate': 227,\n",
              " 'mentality': 229,\n",
              " 'mercifulness': 230,\n",
              " 'merger': 231,\n",
              " 'merino': 232,\n",
              " 'merlin': 233,\n",
              " 'mesclun': 235,\n",
              " 'mesmerise': 237,\n",
              " 'metaphysician': 239,\n",
              " 'metastable': 240,\n",
              " 'methodically': 241,\n",
              " 'methodology': 242,\n",
              " 'meticulously': 243,\n",
              " 'midair': 245,\n",
              " 'middle': 246,\n",
              " 'middling': 247,\n",
              " 'migration': 248,\n",
              " 'milkmaid': 251,\n",
              " 'miniaturist': 256,\n",
              " 'minimalistic': 257,\n",
              " 'mladost': 267,\n",
              " 'modal': 268,\n",
              " 'mode': 270,\n",
              " 'modern': 271,\n",
              " 'day': 69,\n",
              " 'modish': 272,\n",
              " 'molestation': 273,\n",
              " 'momentum': 275,\n",
              " 'moneybelt': 277,\n",
              " 'mood': 278,\n",
              " 'moon': 279,\n",
              " 'moondust': 280,\n",
              " 'moonshine': 281,\n",
              " 'moonshiner': 282,\n",
              " 'moralist': 283,\n",
              " 'mordancy': 284,\n",
              " 'motivate': 286,\n",
              " 'motto': 289,\n",
              " 'much': 291,\n",
              " 'multidimensional': 293,\n",
              " 'multidimensionality': 294,\n",
              " 'multidisciplinary': 295,\n",
              " 'multimillionaire': 296,\n",
              " 'multimodal': 297,\n",
              " 'multimodality': 298,\n",
              " 'mummy': 299,\n",
              " 'musclehead': 301,\n",
              " 'mushroom': 303,\n",
              " 'music': 304,\n",
              " 'musician': 305,\n",
              " 'musicologist': 306,\n",
              " 'muskrat': 307,\n",
              " 'mythology': 308,\n",
              " 'nab': 309,\n",
              " 'nambla': 311,\n",
              " 'napredak': 312,\n",
              " 'narration': 314,\n",
              " 'nasalization': 315,\n",
              " 'natatorium': 318,\n",
              " 'nattily': 322,\n",
              " 'naturally': 324,\n",
              " 'necessarily': 326,\n",
              " 'needly': 329,\n",
              " 'needs': 330,\n",
              " 'neferkare': 331,\n",
              " 'nest': 334,\n",
              " 'netiquette': 335,\n",
              " 'neuron': 336,\n",
              " 'neutrally': 337,\n",
              " 'newletter': 338,\n",
              " 'newsletter': 340,\n",
              " 'newsline': 341,\n",
              " 'newsworthy': 343,\n",
              " 'nightie': 345,\n",
              " 'nikah': 347,\n",
              " 'nipping': 349,\n",
              " 'nirvana': 350,\n",
              " 'noctural': 353,\n",
              " 'nocturnal': 354,\n",
              " 'nocturnality': 355,\n",
              " 'nocturnally': 356,\n",
              " 'nomological': 358,\n",
              " 'negotiable': 332,\n",
              " 'nonnegotiable': 361,\n",
              " 'northsouthnet': 363,\n",
              " 'nous': 368,\n",
              " 'novella': 369,\n",
              " 'oblate': 374,\n",
              " 'obsequious': 377,\n",
              " 'observer': 378,\n",
              " 'obsessiveness': 379,\n",
              " 'obstruent': 380,\n",
              " 'occasionalism': 381,\n",
              " 'occultist': 382,\n",
              " 'offensif': 390,\n",
              " 'offering': 392,\n",
              " 'officemax': 393,\n",
              " 'officiant': 396,\n",
              " 'oh': 398,\n",
              " 'okeh': 401,\n",
              " 'okey': 402,\n",
              " 'okonomiyaki': 404,\n",
              " 'ontological': 411,\n",
              " 'ontologically': 412,\n",
              " 'oot': 413,\n",
              " 'operate': 416,\n",
              " 'optimal': 417,\n",
              " 'optimality': 418,\n",
              " 'optimist': 419,\n",
              " 'optimize': 421,\n",
              " 'optimum': 422,\n",
              " 'orbit': 423,\n",
              " 'orbital': 424,\n",
              " 'organdy': 426,\n",
              " 'organza': 428,\n",
              " 'ornately': 429,\n",
              " 'orthodoxly': 430,\n",
              " 'oscillococcinum': 431,\n",
              " 'outdistance': 436,\n",
              " 'outdo': 437,\n",
              " 'outpace': 441,\n",
              " 'outperform': 442,\n",
              " 'outperformance': 443,\n",
              " 'outperformer': 444,\n",
              " 'outpoint': 445,\n",
              " 'outputted': 446,\n",
              " 'outrun': 450,\n",
              " 'outshine': 451,\n",
              " 'outshout': 452,\n",
              " 'outstay': 453,\n",
              " 'outstayed': 454,\n",
              " 'outstaying': 455,\n",
              " 'outstays': 456,\n",
              " 'outstrip': 457,\n",
              " 'oval': 458,\n",
              " 'overabundance': 460,\n",
              " 'overconsumption': 461,\n",
              " 'overgraze': 462,\n",
              " 'overindulgence': 463,\n",
              " 'overindulging': 464,\n",
              " 'overstays': 467,\n",
              " 'overturn': 468,\n",
              " 'owl': 470,\n",
              " 'owlish': 471,\n",
              " 'oxygen': 472,\n",
              " 'pabulum': 473,\n",
              " 'pachinko': 474,\n",
              " 'pacifism': 475,\n",
              " 'paedophilia': 476,\n",
              " 'paedophilic': 477,\n",
              " 'painstakingly': 479,\n",
              " 'palatalization': 480,\n",
              " 'palindrome': 481,\n",
              " 'palmy': 482,\n",
              " 'palpably': 485,\n",
              " 'pander': 487,\n",
              " 'paneer': 488,\n",
              " 'pannage': 490,\n",
              " 'pantheist': 491,\n",
              " 'pantie': 492,\n",
              " 'panties': 493,\n",
              " 'papalii': 494,\n",
              " 'par': 495,\n",
              " 'parabolica': 496,\n",
              " 'paradigmatically': 497,\n",
              " 'paradoxalement': 498,\n",
              " 'pared': 500,\n",
              " 'parisian': 502,\n",
              " 'parramatta': 503,\n",
              " 'participate': 504,\n",
              " 'participation': 505,\n",
              " 'partizan': 507,\n",
              " 'passionflower': 509,\n",
              " 'pasted': 510,\n",
              " 'pasturage': 511,\n",
              " 'paternalism': 512,\n",
              " 'paternalistic': 513,\n",
              " 'patriarchal': 517,\n",
              " 'patriot': 518,\n",
              " 'patron': 519,\n",
              " 'pederast': 524,\n",
              " 'pedophile': 525,\n",
              " 'pedophilia': 526,\n",
              " 'penny': 528,\n",
              " 'peppermint': 529,\n",
              " 'pepsi': 530,\n",
              " 'perenelle': 539,\n",
              " 'performer': 541,\n",
              " 'peri': 543,\n",
              " 'perigee': 544,\n",
              " 'perimenopause': 546,\n",
              " 'periodical': 547,\n",
              " 'perl': 549,\n",
              " 'persevering': 552,\n",
              " 'person': 553,\n",
              " 'personnel': 554,\n",
              " 'perspicuous': 555,\n",
              " 'petal': 557,\n",
              " 'petsmart': 558,\n",
              " 'philosopher': 560,\n",
              " 'physiotherapist': 563,\n",
              " 'pianist': 564,\n",
              " 'piece': 565,\n",
              " 'pillow': 566,\n",
              " 'pioneer': 567,\n",
              " 'pitch': 568,\n",
              " 'perfect': 540,\n",
              " 'pithily': 569,\n",
              " 'pithy': 570,\n",
              " 'pitlane': 571,\n",
              " 'pixelation': 572,\n",
              " 'placenta': 574,\n",
              " 'placental': 575,\n",
              " 'plastering': 577,\n",
              " 'plastid': 578,\n",
              " 'plato': 579,\n",
              " 'please': 582,\n",
              " 'plenary': 584,\n",
              " 'pleseant': 585,\n",
              " 'plotline': 587,\n",
              " 'pluralize': 589,\n",
              " 'plush': 590,\n",
              " 'plushly': 591,\n",
              " 'plushness': 592,\n",
              " 'plushy': 593,\n",
              " 'plutocrat': 594,\n",
              " 'podiatrist': 595,\n",
              " 'policeman': 599,\n",
              " 'political': 602,\n",
              " 'pollination': 605,\n",
              " 'polly': 606,\n",
              " 'polygon': 607,\n",
              " 'polyhedron': 608,\n",
              " 'polymodal': 609,\n",
              " 'polysemy': 610,\n",
              " 'pontiac': 613,\n",
              " 'ponzu': 614,\n",
              " 'pool': 615,\n",
              " 'poppy': 616,\n",
              " 'pornography': 618,\n",
              " 'pornstar': 619,\n",
              " 'portraitist': 620,\n",
              " 'posit': 621,\n",
              " 'positively': 622,\n",
              " 'positiveness': 623,\n",
              " 'positiver': 624,\n",
              " 'positivity': 625,\n",
              " 'possess': 626,\n",
              " 'possessively': 627,\n",
              " 'post': 628,\n",
              " 'poster': 629,\n",
              " 'postgraduate': 630,\n",
              " 'poteen': 632,\n",
              " 'potpourri': 633,\n",
              " 'potty': 634,\n",
              " 'pouf': 635,\n",
              " 'powder': 637,\n",
              " 'powerade': 639,\n",
              " 'pranayama': 642,\n",
              " 'preach': 643,\n",
              " 'precede': 644,\n",
              " 'predominantly': 645,\n",
              " 'preemie': 646,\n",
              " 'preface': 647,\n",
              " 'preform': 648,\n",
              " 'pregnant': 649,\n",
              " 'premise': 650,\n",
              " 'preparation': 651,\n",
              " 'preponderant': 653,\n",
              " 'prepubertal': 655,\n",
              " 'prescience': 656,\n",
              " 'prescient': 657,\n",
              " 'present': 658,\n",
              " 'presuppose': 661,\n",
              " 'presupposition': 662,\n",
              " 'prewash': 663,\n",
              " 'primordially': 667,\n",
              " 'princess': 668,\n",
              " 'printmaker': 669,\n",
              " 'pristine': 671,\n",
              " 'prizefighter': 675,\n",
              " 'probability': 676,\n",
              " 'probity': 677,\n",
              " 'proboscis': 678,\n",
              " 'proceed': 679,\n",
              " 'prof': 681,\n",
              " 'professed': 682,\n",
              " 'professedly': 683,\n",
              " 'professor': 684,\n",
              " 'professorship': 685,\n",
              " 'profuse': 687,\n",
              " 'prolific': 690,\n",
              " 'promethean': 691,\n",
              " 'prominent': 692,\n",
              " 'promotional': 695,\n",
              " 'propeller': 698,\n",
              " 'proportionately': 700,\n",
              " 'propre': 702,\n",
              " 'prospectively': 704,\n",
              " 'prospering': 705,\n",
              " 'prosperity': 706,\n",
              " 'protection': 708,\n",
              " 'protocol': 709,\n",
              " 'proton': 710,\n",
              " 'provability': 711,\n",
              " 'providential': 713,\n",
              " 'providentially': 714,\n",
              " 'prutot': 718,\n",
              " 'pseudoknot': 720,\n",
              " 'psychatog': 722,\n",
              " 'psychologiquement': 723,\n",
              " 'pub': 724,\n",
              " 'puberty': 725,\n",
              " 'puir': 727,\n",
              " 'pun': 728,\n",
              " 'puncher': 729,\n",
              " 'punchline': 730,\n",
              " 'punctual': 731,\n",
              " 'pup': 734,\n",
              " 'purgative': 735,\n",
              " 'purge': 736,\n",
              " 'purposeful': 737,\n",
              " 'purposefully': 738,\n",
              " 'purposive': 739,\n",
              " 'push': 740,\n",
              " 'pushing': 741,\n",
              " 'quaintly': 743,\n",
              " 'qualify': 744,\n",
              " 'quantitatively': 746,\n",
              " 'questioningly': 750,\n",
              " 'quickener': 753,\n",
              " 'quileute': 755,\n",
              " 'quintessential': 756,\n",
              " 'quintessentially': 757,\n",
              " 'quizzically': 758,\n",
              " 'quote': 759,\n",
              " 'racy': 761,\n",
              " 'radiancy': 762,\n",
              " 'radicalism': 764,\n",
              " 'radniki': 765,\n",
              " 'raingear': 767,\n",
              " 'rakishly': 769,\n",
              " 'raper': 773,\n",
              " 'readably': 776,\n",
              " 'reaffirmation': 777,\n",
              " 'realist': 778,\n",
              " 'reasonably': 780,\n",
              " 'recent': 783,\n",
              " 'reclaim': 785,\n",
              " 'recognizable': 786,\n",
              " 'recolonization': 787,\n",
              " 'reconceive': 789,\n",
              " 'reconstitute': 790,\n",
              " 'recreation': 793,\n",
              " 'recreational': 794,\n",
              " 'rect': 795,\n",
              " 'rectangle': 796,\n",
              " 'redbud': 797,\n",
              " 'redemption': 798,\n",
              " 'redemptive': 799,\n",
              " 'redhead': 800,\n",
              " 'reducible': 803,\n",
              " 'reeperbahn': 805,\n",
              " 'reflectance': 806,\n",
              " 'reflectivity': 809,\n",
              " 'refridgerator': 812,\n",
              " 'regurgitated': 815,\n",
              " 'rehab': 816,\n",
              " 'rehabilitate': 817,\n",
              " 'rehabilitative': 818,\n",
              " 'rehearsal': 819,\n",
              " 'reinstate': 820,\n",
              " 'reinstated': 821,\n",
              " 'relativism': 822,\n",
              " 'relaxation': 823,\n",
              " 'relentlessness': 825,\n",
              " 'reliably': 828,\n",
              " 'reliever': 830,\n",
              " 'religionist': 831,\n",
              " 'relish': 832,\n",
              " 'remediate': 834,\n",
              " 'remedy': 835,\n",
              " 'renovation': 836,\n",
              " 'repair': 837,\n",
              " 'reponsible': 838,\n",
              " 'representative': 839,\n",
              " 'rescue': 845,\n",
              " 'resend': 847,\n",
              " 'resistless': 851,\n",
              " 'resoluteness': 852,\n",
              " 'resource': 853,\n",
              " 'respectful': 855,\n",
              " 'respiration': 856,\n",
              " 'resplendent': 857,\n",
              " 'responsibile': 860,\n",
              " 'restructure': 865,\n",
              " 'restructuring': 866,\n",
              " 'retell': 869,\n",
              " 'retold': 870,\n",
              " 'retroactive': 871,\n",
              " 'retroactively': 872,\n",
              " 'retrospectively': 873,\n",
              " 'reusable': 874,\n",
              " 'rich': 876,\n",
              " 'mill': 252,\n",
              " 'salutation': 879,\n",
              " 'second': 884,\n",
              " 'reflection': 807,\n",
              " 'reliant': 829,\n",
              " 'titled': 939,\n",
              " 'mindedness': 255,\n",
              " 'sky': 901,\n",
              " 'three': 932,\n",
              " 'tko': 940,\n",
              " 'toehold': 942,\n",
              " 'tomato': 943,\n",
              " 'tonkatsu': 944,\n",
              " 'toovey': 945,\n",
              " 'notch': 366,\n",
              " 'touchpad': 953,\n",
              " 'touchscreen': 954,\n",
              " 'tough': 956,\n",
              " 'tourism': 957,\n",
              " 'tram': 961,\n",
              " 'transcription': 962,\n",
              " 'transliterate': 963,\n",
              " 'transliterated': 964,\n",
              " 'transliterating': 965,\n",
              " 'transliteration': 966,\n",
              " 'trapezoid': 970,\n",
              " 'trapper': 972,\n",
              " 'treat': 973,\n",
              " 'trellis': 974,\n",
              " 'trenchant': 977,\n",
              " 'trendiest': 978,\n",
              " 'trepidly': 980,\n",
              " 'tribal': 982,\n",
              " 'tribe': 983,\n",
              " 'well': 994,\n",
              " 'placed': 573,\n",
              " 'yo': 998}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RpPvs1LX4r1x",
        "colab_type": "code",
        "outputId": "c3ce0487-bdd0-4ce2-836f-807d078fbe84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "train_tfidf = tfidfVec.fit_transform(train_news['clean'])\n",
        "print('Training dim:', train_tfidf.shape)\n",
        "print(train_tfidf.A[:10])\n",
        "\n",
        "\n",
        "test_tfidf = tfidfVec.fit_transform(test_news['clean'])\n",
        "print('Test dim:', test_tfidf.shape)\n",
        "print(test_tfidf.A[:10])\n",
        "\n",
        "\n",
        "valid_tfidf = tfidfVec.fit_transform(valid_news['clean'])\n",
        "print('Valid dim:', valid_tfidf.shape)\n",
        "print(valid_tfidf.A[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dim: (10239, 1000)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Test dim: (1266, 1000)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Valid dim: (1284, 1000)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyPLArCu4r1z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Cosine Similarity Score\n",
        "\n",
        "> The cosine similarity between two vectors (or two documents on the Vector Space) is a measure that calculates the cosine of the angle between them. This metric is a measurement of orientation and not magnitude, it can be seen as a comparison between documents on a normalized space because we’re not taking into the consideration only the magnitude of each word count (tf-idf) of each document, but the angle between the documents.\n",
        "\n",
        "> I have compared the sentiment vector of each doucment and estimated a similarity score which is saved as a column in the training and test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouK3V-Zv4r10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "similarity_score = []\n",
        "for i in range(len(train_tfidf.toarray())):\n",
        "    similarity_score.append(1 - spatial.distance.cosine(tf_idf_senti[0].toarray(), tfidf_corpus[i].toarray()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZKKux9Xf4r11",
        "colab_type": "code",
        "outputId": "e46efec1-1a1e-4866-b626-9329a579e465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_news['sensational_score'] = similarity_score\n",
        "train_news.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>headline_text</th>\n",
              "      <th>clean</th>\n",
              "      <th>sensational_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <th>1</th>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>decline coal start start natur gas took start ...</td>\n",
              "      <td>0.061786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <th>2</th>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>hillary clinton agre john mccain vote give geo...</td>\n",
              "      <td>0.059743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <th>3</th>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health care reform legis like mandat free sex ...</td>\n",
              "      <td>0.052559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <th>4</th>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>econom turnaround start end term</td>\n",
              "      <td>0.066800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <th>5</th>\n",
              "      <td>The Chicago Bears have had more starting quart...</td>\n",
              "      <td>chicago bear start quarterback last year total...</td>\n",
              "      <td>0.048691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headline_text  ... sensational_score\n",
              "1.0 1  When did the decline of coal start? It started...  ...          0.061786\n",
              "2.0 2  Hillary Clinton agrees with John McCain \"by vo...  ...          0.059743\n",
              "3.0 3  Health care reform legislation is likely to ma...  ...          0.052559\n",
              "4.0 4  The economic turnaround started at the end of ...  ...          0.066800\n",
              "5.0 5  The Chicago Bears have had more starting quart...  ...          0.048691\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9_wfHwn4r17",
        "colab_type": "code",
        "outputId": "4baffbc2-f2ec-4007-a981-84cc1bd2235c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "corpus = []\n",
        "for i in test_news['clean']:\n",
        "    corpus.append(i)\n",
        "# corpus\n",
        "\n",
        "tfidf_corpus = tfidfVec.fit_transform(corpus)\n",
        "\n",
        "similarity_score = []\n",
        "for i in range(len(test_tfidf.toarray())):\n",
        "    similarity_score.append(1 - spatial.distance.cosine(tf_idf_senti[0].toarray(), tfidf_corpus[i].toarray()))\n",
        "    \n",
        "test_news['sensational_score'] = similarity_score\n",
        "test_news.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>headline_text</th>\n",
              "      <th>clean</th>\n",
              "      <th>sensational_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <th>1</th>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>wisconsin pace doubl number layoff year</td>\n",
              "      <td>0.019047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <th>2</th>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>say john mccain done noth help vet</td>\n",
              "      <td>0.026551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <th>3</th>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>suzanne bonamici support plan cut choice medic...</td>\n",
              "      <td>0.061022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <th>4</th>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>ask report whether he center crimini scheme vi...</td>\n",
              "      <td>0.065230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <th>5</th>\n",
              "      <td>Over the past five years the federal governmen...</td>\n",
              "      <td>past five year feder govern paid million retir...</td>\n",
              "      <td>0.066946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headline_text  ... sensational_score\n",
              "1.0 1  Wisconsin is on pace to double the number of l...  ...          0.019047\n",
              "2.0 2  Says John McCain has done nothing to help the ...  ...          0.026551\n",
              "3.0 3  Suzanne Bonamici supports a plan that will cut...  ...          0.061022\n",
              "4.0 4  When asked by a reporter whether hes at the ce...  ...          0.065230\n",
              "5.0 5  Over the past five years the federal governmen...  ...          0.066946\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIl9tClZ4r19",
        "colab_type": "text"
      },
      "source": [
        "##### Saved the latest dataset into a seperate CSV file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60bH8HHt4r19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "train_sensationalism_file = dataset_path + \"/train_sensationalism.csv\"\n",
        "test_sensationalism_file = dataset_path + \"/test_sensationalism.csv\"\n",
        "valid_sensationalism_file = dataset_path + \"/valid_sensationalism.csv\"\n",
        "train_news.to_csv(train_sensationalism_file, sep=',')\n",
        "test_news.to_csv(train_sensationalism_file, sep=',')\n",
        "valid_news.to_csv(valid_sensationalism_file, sep=',')\n",
        "\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FWQ6FqmUtVG",
        "colab_type": "code",
        "outputId": "76965074-5ff2-4103-bae9-208f94512147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "columnNames = [\"jsonid\", \"headline_text\", \"clean\", \"sensational_score\"]\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "train_sensationalism_file = dataset_path + \"/train_sensationalism.csv\"\n",
        "test_sensationalism_file = dataset_path + \"/test_sensationalism.csv\"\n",
        "dataTrain = pd.read_csv(train_sensationalism_file, sep=',', header=None, names = columnNames)\n",
        "dataTest = pd.read_csv(test_sensationalism_file, sep=',', header=None, names = columnNames)\n",
        "\n",
        "dataTrain = dataTrain.loc[1:]\n",
        "dataTest = dataTest.loc[1:]\n",
        "\n",
        "dataTrain.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jsonid</th>\n",
              "      <th>headline_text</th>\n",
              "      <th>clean</th>\n",
              "      <th>sensational_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Wisconsin is on pace to double the number of l...</td>\n",
              "      <td>wisconsin pace doubl number layoff year</td>\n",
              "      <td>0.019046839606136512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Says John McCain has done nothing to help the ...</td>\n",
              "      <td>say john mccain done noth help vet</td>\n",
              "      <td>0.026551492373704022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
              "      <td>suzanne bonamici support plan cut choice medic...</td>\n",
              "      <td>0.061021958191452175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>When asked by a reporter whether hes at the ce...</td>\n",
              "      <td>ask report whether he center crimini scheme vi...</td>\n",
              "      <td>0.065229738576433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Over the past five years the federal governmen...</td>\n",
              "      <td>past five year feder govern paid million retir...</td>\n",
              "      <td>0.06694575545774362</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     jsonid  ...     sensational_score\n",
              "1.0     1.0  ...  0.019046839606136512\n",
              "2.0     2.0  ...  0.026551492373704022\n",
              "3.0     3.0  ...  0.061021958191452175\n",
              "4.0     4.0  ...     0.065229738576433\n",
              "5.0     5.0  ...   0.06694575545774362\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgPy1xWhAW7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SensationalismFeature():\n",
        "\n",
        "    def __init__(self):        \n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        train_sensationalism_file = dataset_path + \"/train_sensationalism.csv\"\n",
        "        test_sensationalism_file = dataset_path + \"/test_sensationalism.csv\"\n",
        "        columnNames = [\"jsonid\", \"headline_text\", \"clean\", \"sensational_score\"]\n",
        "        dataTrain = pd.read_csv(train_sensationalism_file, sep=',', header=None, names = columnNames)\n",
        "        dataTest = pd.read_csv(test_sensationalism_file, sep=',', header=None, names = columnNames)\n",
        "        dataTrain = dataTrain.loc[1:]\n",
        "        dataTest = dataTest.loc[1:]\n",
        "            \n",
        "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df = 30, use_idf = True, smooth_idf = True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "                ('LogRCV', tfidfV),\n",
        "                ('LogR_clf', LogisticRegression(solver='liblinear', C = 32/100))\n",
        "                ])\n",
        "\n",
        "        self.logR_pipeline.fit(dataTrain['headline_text'], dataTrain['sensational_score'].astype(str))\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
        "        score = metrics.accuracy_score(dataTest['sensational_score'].astype(str), predicted_LogR)\n",
        "        print(\"Sensationalism Model Trained - accuracy:   %0.6f\" % score)\n",
        "    \n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return bool(predicted), float(predicedProb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHkit1JDAgSa",
        "colab_type": "code",
        "outputId": "6aea8f97-c6e8-46cb-e6fe-2962c709662a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getSensationalismScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/sensational.pkl','rb') as f:\n",
        "        sf = pickle.load(f)\n",
        "    binaryValue, probValue = sf.predict(text)\n",
        "    return (float(probValue*100))\n",
        "\n",
        "DATAMINERS_getSensationalismScore(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0812206937482198"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guN9j-pA4r2A",
        "colab_type": "text"
      },
      "source": [
        "#4: Political Affiliation Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hTYVCT3E4r2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "class PartyAffiliation():\n",
        "    \n",
        "    # API to check whether the subject(Headline) is present in the \n",
        "    # - democrats most used words if the party affiliation is democrat\n",
        "    # - republicans most used words if the part affiliation is republican\n",
        "    def partyAffiliationFromHeadline(self, r):\n",
        "        v = r['subject_str']\n",
        "        p = r['party_str']\n",
        "        if (p =='democrat'):\n",
        "            s2 = set(self.countDemV.get_feature_names())\n",
        "        if (p =='republican'):\n",
        "            s2 = set(self.countRepV.get_feature_names())\n",
        "        if (p != 'democract' and p !='republican'):\n",
        "            return 1 #'true'        \n",
        "        if set(v).intersection(s2):\n",
        "            return 1 #'true'\n",
        "        else:\n",
        "            return 0 #'false'\n",
        "\n",
        "    #API to convert true, mostly-true and half-true to true\n",
        "    # false, barely-true and pants-fire to false\n",
        "    def convertMulticlassToBinaryclass(self, r):\n",
        "        v = r['label']\n",
        "        if (v == 'true'):\n",
        "            return 1 #'true'\n",
        "        if (v == 'mostly-true'):\n",
        "            return 1 #'true'\n",
        "        if (v == 'half-true'):\n",
        "            return 1 #'true'\n",
        "        if (v == 'barely-true'):\n",
        "            return 0 #'false'\n",
        "        if (v == 'false'):\n",
        "            return 0 #'false'\n",
        "        if (v == 'pants-fire'):\n",
        "            return 0 #'false'\n",
        "            \n",
        "            \n",
        "            \n",
        "    def plot_confusion_matrix(self, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45)\n",
        "        plt.yticks(tick_marks, classes)\n",
        "\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            print(\"Normalized confusion matrix\")\n",
        "        else:\n",
        "            print('Confusion matrix, without normalization')\n",
        "\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, cm[i, j],\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')       \n",
        "            \n",
        "    \n",
        "    def __init__(self):        \n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        columnNamesPar = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n",
        "        dataTrainPar = pd.read_csv(dataset_path + \"/train.tsv\", sep='\\t', header=None, names = columnNamesPar)\n",
        "        dataValidatePar = pd.read_csv(dataset_path + \"/valid.tsv\", sep='\\t', header=None, names = columnNamesPar)\n",
        "        dataTestPar = pd.read_csv(dataset_path + \"/test.tsv\", sep='\\t', header=None, names = columnNamesPar)\n",
        "        \n",
        "    \n",
        "        # Remove unwanted columns in the dataset\n",
        "        columnsToRemovePar = ['id', 'speaker', 'context','speaker_job_title', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n",
        "        dataTrainPar = dataTrainPar.drop(columns=columnsToRemovePar)\n",
        "        dataValidatePar = dataValidatePar.drop(columns=columnsToRemovePar)\n",
        "        dataTestPar = dataTestPar.drop(columns=columnsToRemovePar)\n",
        "        \n",
        "        # convert the labels to true and false only\n",
        "        dataTrainPar['label'] = dataTrainPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
        "        dataValidatePar['label'] = dataValidatePar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
        "        dataTestPar['label'] = dataTestPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
        "        \n",
        "        # display all the party affiliations and show the count of each party \n",
        "#         dataTrainPar.groupby('party_affiliation').count()[['state_info']].rename(\n",
        "#         columns={'state_info': 'count'}).sort_values(\n",
        "#         'count', ascending=False).reset_index().plot.bar(\n",
        "#         x='party_affiliation', y='count', figsize=(16, 10), fontsize=18);\n",
        "        \n",
        "        # As we are considering only democrat, republican and none (top 3 party affiliations),\n",
        "        # ignoring other party affiliations\n",
        "        rowsToRemove = ['Moderate', 'activist', 'business-leader', 'columnist', 'constitution-party', 'democratic-farmer-labor', 'education-official', 'government-body', 'green', 'independent', 'journalist', 'labor-leader', 'liberal-party-canada', 'libertarian', 'nan', 'newsmaker', 'ocean-state-tea-party-action', 'organization', 'state-official', 'talk-show-host', 'tea-party-member']\n",
        "\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'Moderate']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'activist']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'business-leader']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'columnist']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'constitution-party']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'democratic-farmer-labor']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'education-official']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'government-body']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'green']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'independent']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'journalist']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'labor-leader']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'liberal-party-canada']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'libertarian']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'nan']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'newsmaker']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'ocean-state-tea-party-action']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'organization']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'state-official']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'talk-show-host']\n",
        "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'tea-party-member']\n",
        "\n",
        "        # As we are considering only democrat, republican and none (top 3 party affiliations),\n",
        "        # ignoring other party affiliations\n",
        "\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'Moderate']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'activist']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'business-leader']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'columnist']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'constitution-party']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'democratic-farmer-labor']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'education-official']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'government-body']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'green']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'independent']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'journalist']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'labor-leader']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'liberal-party-canada']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'libertarian']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'nan']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'newsmaker']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'ocean-state-tea-party-action']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'organization']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'state-official']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'talk-show-host']\n",
        "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'tea-party-member']\n",
        "\n",
        "        \n",
        "        dataTrainPar['party_str'] = dataTrainPar['party_affiliation'].astype(str)\n",
        "        dataTestPar['party_str'] = dataTestPar['party_affiliation'].astype(str)\n",
        "        \n",
        "\n",
        "        #predicting truth level\n",
        "#        dataTrainPar.groupby('label').count()[['party_affiliation']].reset_index().plot.bar(x='label', y='party_affiliation')\n",
        "        \n",
        "        # get the most used democrat words\n",
        "        self.countDemV = CountVectorizer(stop_words='english', min_df=40, max_df=80, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "        dataTrainDem= dataTrainPar\n",
        "        dataTrainDem = dataTrainPar.loc[dataTrainPar['party_str'] == 'democrat']\n",
        "        dem_count = self.countDemV.fit_transform(dataTrainDem['statement'].values)\n",
        "        \n",
        "        #get the republican most used words\n",
        "        \n",
        "        self.countRepV = CountVectorizer(stop_words='english', min_df=20, max_df=40, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "        dataTrainRep= dataTrainPar\n",
        "        dataTrainRep = dataTrainPar.loc[dataTrainPar['party_str'] == 'republican']\n",
        "        rep_count = self.countRepV.fit_transform(dataTrainRep['statement'].values)\n",
        "\n",
        "        dataTestDem= dataTestPar\n",
        "        dataTestDem = dataTestPar.loc[dataTestPar['party_str'] == 'democrat']\n",
        "        \n",
        "        dataTrainPar['subject_str'] = dataTrainPar['subject'].astype(str).str.split() \n",
        "        dataTrainPar['label_str'] = dataTrainPar.apply(self.partyAffiliationFromHeadline, axis=1)\n",
        "\n",
        "        dataTestPar['subject_str'] = dataTestPar['subject'].astype(str).str.split() \n",
        "        dataTestPar['label_str'] = dataTestPar.apply(self.partyAffiliationFromHeadline, axis=1)\n",
        "\n",
        "        dataTrainDem['subject_str'] = dataTrainDem['subject'].astype(str).str.split() \n",
        "        dataTrainDem['label_str'] = dataTrainDem.apply(self.partyAffiliationFromHeadline, axis=1)\n",
        "    \n",
        "        dataTestDem['subject_str'] = dataTestDem['subject'].astype(str).str.split() \n",
        "        dataTestDem['label_str'] = dataTestDem.apply(self.partyAffiliationFromHeadline, axis=1)\n",
        "        \n",
        "        \n",
        "        self.model = LogisticRegression()\n",
        "        self.model = self.model.fit(dataTrainPar['label_str'].values.reshape(-1, 1), dataTrainPar['label'].values)\n",
        "        predicted_LogR = self.model.predict(dataTestPar['label_str'].values.reshape(-1, 1))\n",
        "        score = metrics.accuracy_score(dataTestPar['label'], predicted_LogR)\n",
        "        # print(\"Party Affiliation Model Trained - accuracy:   %0.6f\" % score)\n",
        "\n",
        "    \n",
        "    def predict(self, headline, party):\n",
        "                \n",
        "        #creating the dataframe with our text so we can leverage the existing code\n",
        "        dfrme = pd.DataFrame(index=[0], columns=['subject', 'party_str'])\n",
        "        dfrme['subject_str'] = headline\n",
        "        dfrme['party_str'] = party        \n",
        "\n",
        "        dfrme['subject'] = headline\n",
        "        dfrme['subject_str'] = dfrme['subject'].astype(str).str.split() \n",
        "        dfrme['label_str'] = dfrme.apply(self.partyAffiliationFromHeadline, axis=1)\n",
        "        \n",
        "        x = dfrme['label_str'].values.reshape(-1, 1)\n",
        "        predicted = self.model.predict(x)\n",
        "        predicedProb = self.model.predict_proba(x)[:,1]\n",
        "        return predicted, predicedProb\n",
        "                    \n",
        "    \n",
        "##testing code\n",
        "f = PartyAffiliation()\n",
        "#pf.predict(\"Says the Annies List political group supports third-trimester abortions on demand\", \"republican\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dQw4enys4r2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def loadJsonFiles(directory, veracity):    \n",
        "    shouldAppend = False\n",
        "    for filename in os.listdir(directory):\n",
        "        df2 = pd.read_json(directory + filename, lines=True)\n",
        "        if (shouldAppend):\n",
        "            df = df.append(df2, ignore_index=True, sort=True)      \n",
        "        else:\n",
        "            df = df2\n",
        "        df['veracity'] = veracity\n",
        "        shouldAppend = True\n",
        "        \n",
        "            \n",
        "    # removing nan values    \n",
        "    df['source'].fillna(\"\", inplace=True)\n",
        "    for index, row in df.iterrows():\n",
        "        if (type(row['authors']) == float):\n",
        "            df.at[index, 'authors'] = []\n",
        "\n",
        "            \n",
        "    #removing unnecessary columns\n",
        "    df = df.drop(columns=['keywords','meta_data','movies', 'keywords', 'summary', 'publish_date','top_img'])\n",
        "    return df\n",
        "\n",
        "def loadDataset():\n",
        "    dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "    dataFake = loadJsonFiles(dataset_path + \"/FakeNewsContent/\", 0)\n",
        "    dataReal = loadJsonFiles(dataset_path + \"/RealNewsContent/\", 1)\n",
        "    return dataReal, dataFake\n",
        "\n",
        "dataFake, dataReal = loadDataset()\n",
        "\n",
        "dataTrainFake = dataFake[:100]\n",
        "dataTrainReal = dataReal[:100]\n",
        "dataTestFake = dataFake[101:]\n",
        "dataTestReal = dataReal[101:]\n",
        "\n",
        "dataTest = dataTestFake.append(dataTestReal,ignore_index=True, sort=True)      \n",
        "dataTrain = dataTrainFake.append(dataTrainReal,ignore_index=True, sort=True)    \n",
        "dataAll = dataFake.append(dataReal, ignore_index=True, sort=True)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTz9VA394r2J",
        "colab_type": "code",
        "outputId": "6963b240-456a-44fd-9b60-bf55735e9c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle \n",
        "\n",
        "def DATAMINERS_getPartyAffiliationScore(headline, partyName): # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    if ( (headline == \"\") | (partyName == \"\") ):\n",
        "        return 0\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/party_affiliation.pkl', 'rb') as f:\n",
        "        partyAffiliation = pickle.load(f)\n",
        "    binaryValue, probValue = partyAffiliation.predict(headline, partyName)\n",
        "    return 1 - float(probValue)\n",
        "\n",
        "DATAMINERS_getPartyAffiliationScore(\"Says the Annies List political group supports third-trimester abortions on demand\", \"republican\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3956047752353582"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "829olfDr4r2L",
        "colab_type": "text"
      },
      "source": [
        "#5: Click Bait "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1p69TeZ4r2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class Clickbait():\n",
        "    \n",
        "    question_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n",
        "            'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n",
        "            'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n",
        "            'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n",
        "\n",
        "    contractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n",
        "                    'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n",
        "                    'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n",
        "                    'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n",
        "                    'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n",
        "                    'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n",
        "                    'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n",
        "                    'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n",
        "                    'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n",
        "                    'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n",
        "                    'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n",
        "                    'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n",
        "                    'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n",
        "                    'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n",
        "                    'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n",
        "                    'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n",
        "                    'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n",
        "                    'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n",
        "                    'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n",
        "    \n",
        "    def process_text(self, text):\n",
        "        result = text.replace('/', '').replace('\\n', '')\n",
        "        result = re.sub(r'[1-9]+', 'number', result)\n",
        "        result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)\n",
        "        result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n",
        "        result = ''.join(t for t in result if t not in punctuation)\n",
        "        result = re.sub(r' +', ' ', result).lower().strip()\n",
        "        return result\n",
        "    \n",
        "    def cnt_stop_words(self, text):\n",
        "        s = text.split()\n",
        "        num = len([word for word in s if word in self.stop])\n",
        "        return num\n",
        "\n",
        "    def num_contract(self, text):\n",
        "        s = text.split()\n",
        "        num = len([word for word in s if word in self.contractions])\n",
        "        return num\n",
        "\n",
        "    def question_word(self, text):\n",
        "        s = text.split()\n",
        "        if s[0] in self.question_words:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def part_of_speech(self, text):\n",
        "        s = text.split()\n",
        "        nonstop = [word for word in s if word not in self.stop]\n",
        "        pos = [part[1] for part in nltk.pos_tag(nonstop)]\n",
        "        pos = ' '.join(pos)\n",
        "        return pos\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        df_ycb = pd.read_csv(dataset_path + \"/clickbait_data.txt\", sep=\"\\n\", header=None, names=['text'])\n",
        "        df_ycb['clickbait'] = 1\n",
        "\n",
        "        df_ncb = pd.read_csv(dataset_path + \"/non_clickbait_data.txt\", sep=\"\\n\", header=None, names=['text'])\n",
        "        df_ncb['clickbait'] = 0\n",
        "\n",
        "        df = df_ycb.append(df_ncb, ignore_index=True).reset_index(drop=True)\n",
        "       \n",
        "        self.stop = stopwords.words('english')\n",
        "       \n",
        "        # Creating some latent variables from the data\n",
        "        df['text']     = df['text'].apply(self.process_text)\n",
        "        df['question'] = df['text'].apply(self.question_word)\n",
        "\n",
        "        df['num_words']       = df['text'].apply(lambda x: len(x.split()))\n",
        "        df['part_speech']     = df['text'].apply(self.part_of_speech)\n",
        "        df['num_contract']    = df['text'].apply(self.num_contract)\n",
        "        df['num_stop_words']  = df['text'].apply(self.cnt_stop_words)\n",
        "        df['stop_word_ratio'] = df['num_stop_words']/df['num_words']\n",
        "        df['contract_ratio']  = df['num_contract']/df['num_words']\n",
        "        \n",
        "        df.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
        "\n",
        "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
        "\n",
        "        self.tfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
        "                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,5),\n",
        "                                   use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
        "\n",
        "        X_train_text = self.tfidf.fit_transform(df_train['text'])\n",
        "        X_test_text  = self.tfidf.transform(df_test['text'])\n",
        "\n",
        "        self.cvec = CountVectorizer()\n",
        "\n",
        "        X_train_pos = self.cvec.fit_transform(df_train['part_speech'])\n",
        "        X_test_pos  = self.cvec.transform(df_test['part_speech'])\n",
        "\n",
        "        self.scNoMean = StandardScaler(with_mean=False)  # we pass with_mean=False to preserve the sparse matrix\n",
        "        X_train_pos_sc = self.scNoMean.fit_transform(X_train_pos)\n",
        "        X_test_pos_sc  = self.scNoMean.transform(X_test_pos)\n",
        "\n",
        "        X_train_val = df_train.drop(['clickbait','text','part_speech'], axis=1).values\n",
        "        X_test_val  = df_test.drop(['clickbait','text','part_speech'], axis=1).values\n",
        "\n",
        "        self.sc = StandardScaler()\n",
        "        X_train_val_sc = self.sc.fit(X_train_val).transform(X_train_val)\n",
        "        X_test_val_sc  = self.sc.transform(X_test_val)\n",
        "\n",
        "        y_train = df_train['clickbait'].values\n",
        "        y_test  = df_test['clickbait'].values\n",
        "\n",
        "\n",
        "        X_train = sparse.hstack([X_train_val_sc, X_train_text, X_train_pos_sc]).tocsr()\n",
        "        X_test  = sparse.hstack([X_test_val_sc, X_test_text, X_test_pos_sc]).tocsr()\n",
        "\n",
        "        self.model = LogisticRegression(penalty='l2', C=98.94736842105263)\n",
        "        self.model = self.model.fit(X_train, y_train)\n",
        "        \n",
        "        predicted_LogR = self.model.predict(X_test)\n",
        "        score = metrics.accuracy_score(y_test, predicted_LogR)\n",
        "        print(\"Clickbait Model Trained - accuracy:   %0.6f\" % score)\n",
        "\n",
        "#     predict = model.predict(X_test)\n",
        "#     print(classification_report(y_test, predict))\n",
        "\n",
        "\n",
        "    def predict(self, text):\n",
        "        #creating the dataframe with our text so we can leverage the existing code\n",
        "        dfrme = pd.DataFrame(index=[0], columns=['text'])\n",
        "        dfrme['text'] = text\n",
        "\n",
        "        #processing text\n",
        "        dfrme['text']     = dfrme['text'].apply(self.process_text)\n",
        "\n",
        "        #adding latent variables\n",
        "        dfrme['question'] = dfrme['text'].apply(self.question_word)\n",
        "        dfrme['num_words']       = dfrme['text'].apply(lambda x: len(x.split()))\n",
        "        dfrme['part_speech']     = dfrme['text'].apply(self.part_of_speech)\n",
        "        dfrme['num_contract']    = dfrme['text'].apply(self.num_contract)\n",
        "        dfrme['num_stop_words']  = dfrme['text'].apply(self.cnt_stop_words)\n",
        "        dfrme['stop_word_ratio'] = dfrme['num_stop_words']/dfrme['num_words']\n",
        "        dfrme['contract_ratio']  = dfrme['num_contract']/dfrme['num_words']\n",
        "\n",
        "        #removing latent variables that have high colinearity with other features\n",
        "        dfrme.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "        Xtxt_val  = dfrme.drop(['text','part_speech'], axis=1).values\n",
        "        Xtxt_val_sc  = self.sc.transform(Xtxt_val)\n",
        "\n",
        "        Xtxt_text  = self.tfidf.transform(dfrme['text'])\n",
        "\n",
        "        Xtxt_pos  = self.cvec.transform(dfrme['part_speech'])\n",
        "        Xtxt_pos_sc  = self.scNoMean.transform(Xtxt_pos)\n",
        "        Xtxt  = sparse.hstack([Xtxt_val_sc, Xtxt_text, Xtxt_pos_sc]).tocsr()\n",
        "\n",
        "        predicted = self.model.predict(Xtxt)\n",
        "        predicedProb = self.model.predict_proba(Xtxt)[:,1]\n",
        "        return predicted, predicedProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W4yMDtU4r2Q",
        "colab_type": "code",
        "outputId": "b6004175-4f49-4510-e95a-da1818ed007f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle \n",
        "\n",
        "def DATAMINERS_getClickbaitScore(headline): # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    if (headline == \"\"):\n",
        "        return 0\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/click_bait.pkl', 'rb') as f:\n",
        "        clickBait = pickle.load(f)\n",
        "    binaryValue, probValue = clickBait.predict(headline)\n",
        "    return float(probValue)\n",
        "\n",
        "DATAMINERS_getClickbaitScore(\"Should You bring the money now\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9986359885225317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO0JVV_r4r2U",
        "colab_type": "text"
      },
      "source": [
        "#6 : Spam Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5JCGPvp4r2c",
        "colab_type": "text"
      },
      "source": [
        "#### Function to simplify label classes\n",
        "\n",
        "* Original --\tTrue\n",
        "* True\t--\tTrue\n",
        "* Mostly-true\t-- True\n",
        "* Half-true\t-- True\n",
        "* Barely-true\t-- False\n",
        "* False\t-- False\n",
        "* Pants-fire\t-- False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcap6C8_DxbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class SpamScoreFeature():\n",
        "    def __init__(self): \n",
        "        #load the dataset\n",
        "        columnNames = [\"encoded_label\", \"headline_text\", \"sensational_vector\"]\n",
        "        dataTrain = pd.read_csv('/content/drive/My Drive/junteng_dev/data_set/train_sensational_feature.csv', sep=',', header=None, names = columnNames)\n",
        "        dataTest = pd.read_csv('/content/drive/My Drive/junteng_dev/data_set/test_sensational_feature.csv', sep=',', header=None, names = columnNames)\n",
        "        dataTrain = dataTrain.loc[1:]\n",
        "        dataTest = dataTest.loc[1:]\n",
        "        \n",
        "        #load the spam dictionary\n",
        "        spam_dict = pd.read_csv('/content/drive/My Drive/junteng_dev/data_set/spam_dict.csv', usecols= [1], names = ['spamword'], encoding='latin-1', error_bad_lines=False)\n",
        "        spam_dict = spam_dict.fillna(0)\n",
        "        spam_dict = spam_dict.iloc[1:]\n",
        "        spam_dict = spam_dict.drop_duplicates()\n",
        "\n",
        "        # spam_dict.head(5)\n",
        "        #Count vector for train data\n",
        "        spamcountV = CountVectorizer(vocabulary=list(set(spam_dict['spamword'])))\n",
        "        train_count = spamcountV.fit_transform(dataTrain['headline_text'])\n",
        "       \n",
        "   \n",
        "        self.logR_pipeline = Pipeline([\n",
        "            ('NBCV',spamcountV),\n",
        "            ('nb_clf',MultinomialNB())])\n",
        "\n",
        "        self.logR_pipeline.fit(dataTrain['headline_text'], dataTrain['encoded_label'])\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
        "        score = metrics.accuracy_score(dataTest['encoded_label'], predicted_LogR)\n",
        "        # print(\"Spam Score Model Trained - accuracy:   %0.6f\" % score)\n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return bool(predicted), float(predicedProb)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOnAGCHwD0Yn",
        "colab_type": "code",
        "outputId": "f3e77f2b-0afd-4ce7-a077-e740e04e01fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getSpamScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    #print(clickBait.predict(\"Should You bring the money now\"))\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/spam_score.pkl', 'rb') as f:\n",
        "        spamscore = pickle.load(f)\n",
        "    binaryValue, probValue = spamscore.predict(text)\n",
        "    return float(probValue)\n",
        "\n",
        "DATAMINERS_getSpamScore(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.91802144586649e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usv18KNz4r2l",
        "colab_type": "text"
      },
      "source": [
        "#7 : Author Credibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "poiopOEc4r2l",
        "colab_type": "code",
        "outputId": "9957dbac-0295-4600-b984-bf7864ef107b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "dataAllAuthorsVeracity = dataAll.copy()\n",
        "\n",
        "fakeZero = 0\n",
        "fakeOne = 0\n",
        "falseMoreThanOne = 0\n",
        "trueZero = 0\n",
        "trueOne = 0\n",
        "trueMoreThanOne = 0\n",
        "for index, row in dataAllAuthorsVeracity.iterrows():\n",
        "    authorsCount = len(row['authors'])\n",
        "    dataAllAuthorsVeracity.at[index, 'authors_count'] = len(row['authors'])\n",
        "    if (authorsCount == 0):\n",
        "        if (row['veracity'] == 1):\n",
        "            trueZero += 1\n",
        "        else:\n",
        "            fakeZero += 1\n",
        "    elif (authorsCount == 1):\n",
        "        if (row['veracity'] == 1):\n",
        "            trueOne += 1\n",
        "        else:\n",
        "            fakeOne += 1\n",
        "    elif (authorsCount > 1):\n",
        "        if (row['veracity'] == 1):\n",
        "            trueMoreThanOne += 1\n",
        "        else:\n",
        "            falseMoreThanOne += 1\n",
        "\n",
        "print(\"trueZeroAuthors=\", trueZero)\n",
        "print(\"fakeZeroAuthors=\", fakeZero)\n",
        "print(\"trueOneAuthors=\", trueOne)\n",
        "print(\"fakeOneAuthors=\", fakeOne)\n",
        "print(\"trueMoreThanOneAuthors=\", trueMoreThanOne)\n",
        "print(\"fakeMoreThanOneAuthors=\", falseMoreThanOne)\n",
        "\n",
        "columnsToRemove = ['authors', 'canonical_link', 'images', 'source','url', 'text', 'title']\n",
        "dataAllAuthorsVeracity = dataAllAuthorsVeracity.drop(columns=columnsToRemove)\n",
        "dataAllAuthorsVeracity.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trueZeroAuthors= 13\n",
            "fakeZeroAuthors= 85\n",
            "trueOneAuthors= 36\n",
            "fakeOneAuthors= 24\n",
            "trueMoreThanOneAuthors= 71\n",
            "fakeMoreThanOneAuthors= 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>veracity</th>\n",
              "      <th>authors_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   veracity  authors_count\n",
              "0         1            4.0\n",
              "1         1            1.0\n",
              "2         1            2.0\n",
              "3         1            4.0\n",
              "4         1            2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3KHGrhX4r2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTrainAuthorsVeracity = dataTrain.copy()\n",
        "dataTestAuthorsVeracity = dataTest.copy()\n",
        "\n",
        "for index, row in dataTrainAuthorsVeracity.iterrows():\n",
        "    dataTrainAuthorsVeracity.at[index, 'authors_count'] = len(row['authors'])\n",
        "\n",
        "for index, row in dataTestAuthorsVeracity.iterrows():\n",
        "    dataTestAuthorsVeracity.at[index, 'authors_count'] = len(row['authors'])\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug3nWBim4r2t",
        "colab_type": "code",
        "outputId": "288e92df-04b8-4dd0-b078-e3d30e56f965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X_train = dataTrainAuthorsVeracity['authors_count'].values.reshape(-1, 1)\n",
        "Y_train = dataTrainAuthorsVeracity['veracity'].values\n",
        "X_test = dataTestAuthorsVeracity['authors_count'].values.reshape(-1, 1)\n",
        "Y_test = dataTestAuthorsVeracity['veracity'].values.reshape(-1, 1)\n",
        "\n",
        "from sklearn import linear_model\n",
        "logClassifierAuthorsCount = linear_model.LogisticRegression(solver='liblinear', C=1, random_state=111)\n",
        "logClassifierAuthorsCount.fit(X_train, Y_train)\n",
        "predicted = logClassifierAuthorsCount.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(\"accuracy=\", metrics.accuracy_score(Y_test, predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy= 0.7105263157894737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5LMPY-e4r20",
        "colab_type": "code",
        "outputId": "06f47c0e-758b-4b1c-eafd-50a6c835b88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getAuthorScore(numAuthors): # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    x = np.array(numAuthors).reshape(-1, 1)\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/log_classifier_authors_count.pkl', 'rb') as f:\n",
        "        logClassifierAuthorsCount = pickle.load(f)\n",
        "    predicted = logClassifierAuthorsCount.predict(x)\n",
        "    predicedProbTrue = logClassifierAuthorsCount.predict_proba(x)[:,1]\n",
        "    #return int(predicted), float(predicedProb)\n",
        "    return 1 - float(predicedProbTrue)\n",
        "\n",
        "DATAMINERS_getAuthorScore(4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.009035042324745945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTczEZMc4r24",
        "colab_type": "text"
      },
      "source": [
        "#8 : Source Reputation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pihj8XIy4r25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "dataFakeNewsSites = pd.read_csv(\"/content/drive/My Drive/junteng_dev/data_set/politifact-fakenews-sites.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofg0xEpH4r28",
        "colab_type": "code",
        "outputId": "3a0141f9-7f4d-4f5d-9e44-ca55533c794d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataFakeNewsSites['type of site'].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['imposter site', 'fake news', 'parody site', 'some fake stories'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfNcFAZx4r3C",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the sources are classified in different categories. Almost are all fake (fake news, parody,..) except the category 'some fake stories'. So let's hot encode those categories as 1 for fake news and 0.5 for some fake news."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBnG5HrY4r3D",
        "colab_type": "code",
        "outputId": "a874dc92-3ff1-40ba-cb9e-83bf9b8b1363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "for index, row in dataFakeNewsSites.iterrows():\n",
        "    score = 1\n",
        "    if (row['type of site'] == 'some fake stories'):\n",
        "        score = 0.5\n",
        "    dataFakeNewsSites.at[index, 'fake_score'] = score\n",
        "\n",
        "dataFakeNewsSites.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site name</th>\n",
              "      <th>type of site</th>\n",
              "      <th>registration</th>\n",
              "      <th>fake_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16wmpo.com</td>\n",
              "      <td>imposter site</td>\n",
              "      <td>scottsdale, ariz. **</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24online.news</td>\n",
              "      <td>imposter site</td>\n",
              "      <td>panama, pa. **</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24wpn.com</td>\n",
              "      <td>fake news</td>\n",
              "      <td>veles, macedonia</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24x365live.com</td>\n",
              "      <td>fake news</td>\n",
              "      <td>kobenhavn, denmark</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>247newsmedia.com</td>\n",
              "      <td>fake news</td>\n",
              "      <td>kumanovo, macedonia</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          site name   type of site          registration  fake_score\n",
              "0        16wmpo.com  imposter site  scottsdale, ariz. **         1.0\n",
              "1     24online.news  imposter site        panama, pa. **         1.0\n",
              "2         24wpn.com      fake news      veles, macedonia         1.0\n",
              "3    24x365live.com      fake news   kobenhavn, denmark          1.0\n",
              "4  247newsmedia.com      fake news   kumanovo, macedonia         1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPgf-i0J4r3I",
        "colab_type": "code",
        "outputId": "6afba0ef-c3b2-420f-8c74-5d21b5bd2ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def DATAMINERS_getSourceReputationScore(source): # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    if (source == \"\"):\n",
        "        return 0\n",
        "    d = dataFakeNewsSites[dataFakeNewsSites['site name'].str.match(source)]\n",
        "    if (d['fake_score'].empty):\n",
        "        return 0\n",
        "    return int(d['fake_score'].values)\n",
        "\n",
        "DATAMINERS_getSourceReputationScore('24wpn')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTiU0wl-4r3J",
        "colab_type": "text"
      },
      "source": [
        "#9 : Content Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4LJfqn24r3K",
        "colab_type": "code",
        "outputId": "c20d9417-ae78-4caa-ffbd-7c8b7d7a1a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "dataAllBodyLength = dataAll.copy()\n",
        "for index, row in dataAllBodyLength.iterrows():\n",
        "    textLength = len(row['text'])\n",
        "    dataAllBodyLength.at[index, 'text_length'] = textLength\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linearRegressionBodyLength = LinearRegression(fit_intercept=True)\n",
        "\n",
        "A = np.array(list(dataAllBodyLength.text_length))\n",
        "B = np.array(list(dataAllBodyLength.veracity))\n",
        "\n",
        "linearRegressionBodyLength.fit(A[:, np.newaxis], B)\n",
        "\n",
        "xfit = np.linspace(-1, max(dataAllBodyLength.text_length), 1000)\n",
        "yfit = linearRegressionBodyLength.predict(xfit[:, np.newaxis])\n",
        "\n",
        "plt.scatter(A, B, s=1, c=\"orange\")\n",
        "plt.plot(xfit, yfit);\n",
        "\n",
        "print(\"Model slope:    \", linearRegressionBodyLength.coef_[0])\n",
        "print(\"Model intercept:\", linearRegressionBodyLength.intercept_)\n",
        "print(\"R2 score:\", linearRegressionBodyLength.score(A[:, np.newaxis], B))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model slope:     2.8672921260228e-05\n",
            "Model intercept: 0.41180985978637996\n",
            "R2 score: 0.04612397796223411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HOZYzqh4r3S",
        "colab_type": "code",
        "outputId": "2ac06f48-423e-42fc-c67e-54edd1e26dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for index, row in dataTrain.iterrows():\n",
        "    textLength = len(row['text'])\n",
        "    dataTrain.at[index, 'text_length'] = textLength\n",
        "\n",
        "for index, row in dataTest.iterrows():\n",
        "    textLength = len(row['text'])\n",
        "    dataTest.at[index, 'text_length'] = textLength\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import linear_model\n",
        "\n",
        "logClassifierBodyLength = linear_model.LogisticRegression(solver='liblinear', C=17/1000, random_state=111)\n",
        "logClassifierBodyLength.fit(dataTrain['text_length'].values.reshape(-1, 1), dataTrain['veracity'].values)\n",
        "\n",
        "predicted = logClassifierBodyLength.predict(dataTest['text_length'].values.reshape(-1, 1))\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(dataTest['veracity'].values.reshape(-1, 1), predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAYr7MkJ4r3U",
        "colab_type": "code",
        "outputId": "e607ec0e-8273-4459-b8f5-c670df66c5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getBodyLengthScore(length): # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    x = np.array(length).reshape(-1, 1)\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/log_classifier_body_length.pkl' , 'rb') as f:\n",
        "        logClassifierBodyLength = pickle.load(f)\n",
        "    predicted = logClassifierBodyLength.predict(x)\n",
        "    predicedProb = logClassifierBodyLength.predict_proba(x)[:,1]\n",
        "    #return int(predicted), float(predicedProb)\n",
        "    return 1 - float(predicedProb)\n",
        "\n",
        "DATAMINERS_getBodyLengthScore(12000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08017203680544827"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAUwo2ht4r3W",
        "colab_type": "text"
      },
      "source": [
        "#10 : Word Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRJWd3-w4r3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import metrics\n",
        "\n",
        "class WordFrequency():\n",
        "\n",
        "    def __init__(self):        \n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        columnNames = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n",
        "        dataTrain = pd.read_csv(dataset_path + \"/train.tsv\", sep='\\t', header=None, names = columnNames)\n",
        "        dataValidate = pd.read_csv(dataset_path  + \"/valid.tsv\", sep='\\t', header=None, names = columnNames)\n",
        "        dataTest = pd.read_csv(dataset_path + \"/test.tsv\", sep='\\t', header=None, names = columnNames)\n",
        "        \n",
        "        #dropping columns\n",
        "        columnsToRemove = ['id','subject', 'speaker', 'context','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n",
        "        dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
        "        dataValidate = dataValidate.drop(columns=columnsToRemove)\n",
        "        dataTest = dataTest.drop(columns=columnsToRemove)\n",
        "\n",
        "        def convertMulticlassToBinaryclass(r):\n",
        "            v = r['label']\n",
        "            if (v == 'true'):\n",
        "                return 'true'\n",
        "            if (v == 'mostly-true'):\n",
        "                return 'true'\n",
        "            if (v == 'half-true'):\n",
        "                return 'true'\n",
        "            if (v == 'barely-true'):\n",
        "                return 'false'\n",
        "            if (v == 'false'):\n",
        "                return 'false'\n",
        "            if (v == 'pants-fire'):\n",
        "                return 'false'\n",
        "        dataTrain['label'] = dataTrain.apply(convertMulticlassToBinaryclass, axis=1)\n",
        "        dataValidate['label'] = dataValidate.apply(convertMulticlassToBinaryclass, axis=1)\n",
        "        dataTest['label'] = dataTest.apply(convertMulticlassToBinaryclass, axis=1)\n",
        "        \n",
        "\n",
        "    \n",
        "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "        train_tfidf = tfidfV.fit_transform(dataTrain['statement'].values)\n",
        "        test_tfidf = tfidfV.fit_transform(dataTest['statement'].values)\n",
        "\n",
        "#         print('TF-IDF VECTORIZER')\n",
        "\n",
        "        ## Removing plurals for the tokens using PorterStemmer\n",
        "        stemmer = PorterStemmer()\n",
        "        tfidfVPlurals= tfidfV.get_feature_names()\n",
        "        tfidfVSingles= [stemmer.stem(plural) for plural in tfidfVPlurals]\n",
        "\n",
        "        # Applying Set to remove duplicates\n",
        "        tfidfVTokens = list(set(tfidfVSingles))\n",
        "#         print('TFIDFV Tokens')\n",
        "#         print(tfidfVTokens)\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "                ('LogRCV', tfidfV),\n",
        "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
        "                ])\n",
        "\n",
        "        self.logR_pipeline.fit(dataTrain['statement'],dataTrain['label'])\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['statement'])\n",
        "        score = metrics.accuracy_score(dataTest['label'], predicted_LogR)\n",
        "        print(\"Word Frequency Model Trained - accuracy:   %0.6f\" % score)\n",
        "        \n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return bool(predicted), float(predicedProb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2wpEcWi4r3c",
        "colab_type": "code",
        "outputId": "f344a5ad-0d06-43ca-8e2d-3a608b84650e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "def DATAMINERS_getWordFrequencyScore(text):  # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    #print(clickBait.predict(\"Should You bring the money now\"))\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/word_frequency.pkl', 'rb') as f:\n",
        "        wordFrequency = pickle.load(f)\n",
        "    binaryValue, probValue = wordFrequency.predict(text)\n",
        "    return (1 - float(probValue))\n",
        "\n",
        "print(DATAMINERS_getWordFrequencyScore(\"Says the Annies List political group supports third-trimester abortions on demand.\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5104703891221551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dCE3mUjmPVoc"
      },
      "source": [
        "#11: Bias\n",
        "\n",
        "https://pdfs.semanticscholar.org/3722/40da2a416abfb406426af71f084c988ff7d9.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNyDs0gtZTJ5",
        "colab_type": "code",
        "outputId": "57dd602f-6196-430e-fd74-6f97725d27ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone -b development https://github.com/clips/pattern\n",
        "%cd pattern\n",
        "!python setup.py install "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pattern'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 10989 (delta 0), reused 0 (delta 0), pack-reused 10986\u001b[K\n",
            "Receiving objects: 100% (10989/10989), 51.04 MiB | 24.05 MiB/s, done.\n",
            "Resolving deltas: 100% (7303/7303), done.\n",
            "/content/pattern\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating Pattern.egg-info\n",
            "writing Pattern.egg-info/PKG-INFO\n",
            "writing dependency_links to Pattern.egg-info/dependency_links.txt\n",
            "writing requirements to Pattern.egg-info/requires.txt\n",
            "writing top-level names to Pattern.egg-info/top_level.txt\n",
            "writing manifest file 'Pattern.egg-info/SOURCES.txt'\n",
            "writing manifest file 'Pattern.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/pattern\n",
            "copying pattern/__init__.py -> build/lib/pattern\n",
            "copying pattern/metrics.py -> build/lib/pattern\n",
            "copying pattern/helpers.py -> build/lib/pattern\n",
            "creating build/lib/pattern/text\n",
            "copying pattern/text/__init__.py -> build/lib/pattern/text\n",
            "copying pattern/text/search.py -> build/lib/pattern/text\n",
            "copying pattern/text/tree.py -> build/lib/pattern/text\n",
            "creating build/lib/pattern/web\n",
            "copying pattern/web/utils.py -> build/lib/pattern/web\n",
            "copying pattern/web/__init__.py -> build/lib/pattern/web\n",
            "copying pattern/web/api.py -> build/lib/pattern/web\n",
            "creating build/lib/pattern/web/cache\n",
            "copying pattern/web/cache/__init__.py -> build/lib/pattern/web/cache\n",
            "creating build/lib/pattern/web/imap\n",
            "copying pattern/web/imap/__init__.py -> build/lib/pattern/web/imap\n",
            "creating build/lib/pattern/web/locale\n",
            "copying pattern/web/locale/__init__.py -> build/lib/pattern/web/locale\n",
            "creating build/lib/pattern/web/oauth\n",
            "copying pattern/web/oauth/__init__.py -> build/lib/pattern/web/oauth\n",
            "creating build/lib/pattern/db\n",
            "copying pattern/db/__init__.py -> build/lib/pattern/db\n",
            "creating build/lib/pattern/text/de\n",
            "copying pattern/text/de/__main__.py -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/inflect.py -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/__init__.py -> build/lib/pattern/text/de\n",
            "creating build/lib/pattern/text/en\n",
            "copying pattern/text/en/inflect_quantify.py -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/__main__.py -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/modality.py -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/inflect.py -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/__init__.py -> build/lib/pattern/text/en\n",
            "creating build/lib/pattern/text/en/wordlist\n",
            "copying pattern/text/en/wordlist/__init__.py -> build/lib/pattern/text/en/wordlist\n",
            "creating build/lib/pattern/text/en/wordnet\n",
            "copying pattern/text/en/wordnet/__init__.py -> build/lib/pattern/text/en/wordnet\n",
            "creating build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/__main__.py -> build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/__init__.py -> build/lib/pattern/text/ru\n",
            "creating build/lib/pattern/text/ru/wordlist\n",
            "copying pattern/text/ru/wordlist/__init__.py -> build/lib/pattern/text/ru/wordlist\n",
            "creating build/lib/pattern/text/es\n",
            "copying pattern/text/es/__main__.py -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/inflect.py -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/__init__.py -> build/lib/pattern/text/es\n",
            "creating build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/__main__.py -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/inflect.py -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/__init__.py -> build/lib/pattern/text/fr\n",
            "creating build/lib/pattern/text/it\n",
            "copying pattern/text/it/__main__.py -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/inflect.py -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/__init__.py -> build/lib/pattern/text/it\n",
            "creating build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/__main__.py -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/inflect.py -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/__init__.py -> build/lib/pattern/text/nl\n",
            "creating build/lib/pattern/vector\n",
            "copying pattern/vector/stemmer.py -> build/lib/pattern/vector\n",
            "copying pattern/vector/__init__.py -> build/lib/pattern/vector\n",
            "creating build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/libsvm.py -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/liblinear.py -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/libsvmutil.py -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/liblinearutil.py -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/__init__.py -> build/lib/pattern/vector/svm\n",
            "creating build/lib/pattern/graph\n",
            "copying pattern/graph/__init__.py -> build/lib/pattern/graph\n",
            "copying pattern/graph/commonsense.py -> build/lib/pattern/graph\n",
            "creating build/lib/pattern/server\n",
            "copying pattern/server/__init__.py -> build/lib/pattern/server\n",
            "copying pattern/canvas.js -> build/lib/pattern\n",
            "copying pattern/text/de/de-lexicon.txt -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/de-verbs.txt -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/de-spelling.txt -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/de-morphology.txt -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/de-context.txt -> build/lib/pattern/text/de\n",
            "copying pattern/text/de/de-frequency.txt -> build/lib/pattern/text/de\n",
            "copying pattern/text/en/en-frequency.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-context.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-lexicon.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-verbs.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-spelling.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-morphology.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-entities.txt -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-sentiment.xml -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/en-model.slp -> build/lib/pattern/text/en\n",
            "copying pattern/text/en/wordlist/basic.txt -> build/lib/pattern/text/en/wordlist\n",
            "copying pattern/text/en/wordlist/academic.txt -> build/lib/pattern/text/en/wordlist\n",
            "copying pattern/text/en/wordlist/time.txt -> build/lib/pattern/text/en/wordlist\n",
            "copying pattern/text/en/wordlist/profanity.txt -> build/lib/pattern/text/en/wordlist\n",
            "copying pattern/text/en/wordlist/stopwords.txt -> build/lib/pattern/text/en/wordlist\n",
            "copying pattern/text/en/wordnet/resnik-ic3.txt -> build/lib/pattern/text/en/wordnet\n",
            "copying pattern/text/en/wordnet/resnik-ic2.txt -> build/lib/pattern/text/en/wordnet\n",
            "creating build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/data.adv -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/data.verb -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/LICENSE.txt -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/data.adj -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/index.sense -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/data.noun1 -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/index.verb -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/index.noun -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/index.adv -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/lexnames -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/data.noun2 -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/index.32 -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/en/wordnet/dict/index.adj -> build/lib/pattern/text/en/wordnet/dict\n",
            "copying pattern/text/ru/ru-lexicon.txt -> build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/ru-entities.txt -> build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/ru-spelling.txt -> build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/ru-frequency.txt -> build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/ru-model.slp -> build/lib/pattern/text/ru\n",
            "copying pattern/text/ru/wordlist/stopwords.txt -> build/lib/pattern/text/ru/wordlist\n",
            "copying pattern/text/es/es-morphology.txt -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/es-verbs.txt -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/es-spelling.txt -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/es-context.txt -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/es-lexicon.txt -> build/lib/pattern/text/es\n",
            "copying pattern/text/es/es-frequency.txt -> build/lib/pattern/text/es\n",
            "copying pattern/text/fr/fr-lexicon.txt -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/fr-context.txt -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/fr-spelling.txt -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/fr-frequency.txt -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/fr-verbs.txt -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/fr-morphology.txt -> build/lib/pattern/text/fr\n",
            "copying pattern/text/fr/fr-sentiment.xml -> build/lib/pattern/text/fr\n",
            "copying pattern/text/it/it-verbs.txt -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/it-lexicon.txt -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/it-morphology.txt -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/it-spelling.txt -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/it-frequency.txt -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/it-context.txt -> build/lib/pattern/text/it\n",
            "copying pattern/text/it/it-sentiment.xml -> build/lib/pattern/text/it\n",
            "copying pattern/text/nl/nl-spelling.txt -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/nl-morphology.txt -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/nl-context.txt -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/nl-verbs.txt -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/nl-lexicon.txt -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/nl-frequency.txt -> build/lib/pattern/text/nl\n",
            "copying pattern/text/nl/nl-sentiment.xml -> build/lib/pattern/text/nl\n",
            "copying pattern/vector/stopwords-es.txt -> build/lib/pattern/vector\n",
            "copying pattern/vector/stopwords-fr.txt -> build/lib/pattern/vector\n",
            "copying pattern/vector/stopwords-nl.txt -> build/lib/pattern/vector\n",
            "copying pattern/vector/stopwords-de.txt -> build/lib/pattern/vector\n",
            "copying pattern/vector/stopwords-en.txt -> build/lib/pattern/vector\n",
            "copying pattern/vector/svm/COPYRIGHT-liblinear.txt -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/COPYRIGHT-libsvm.txt -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/README.txt -> build/lib/pattern/vector/svm\n",
            "copying pattern/vector/svm/INSTALL.txt -> build/lib/pattern/vector/svm\n",
            "copying pattern/graph/graph.js -> build/lib/pattern/graph\n",
            "copying pattern/graph/commonsense.csv -> build/lib/pattern/graph\n",
            "creating build/lib/pattern/server/static\n",
            "copying pattern/server/static/robots.txt -> build/lib/pattern/server/static\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pattern\n",
            "copying build/lib/pattern/metrics.py -> build/bdist.linux-x86_64/egg/pattern\n",
            "creating build/bdist.linux-x86_64/egg/pattern/web\n",
            "creating build/bdist.linux-x86_64/egg/pattern/web/imap\n",
            "copying build/lib/pattern/web/imap/__init__.py -> build/bdist.linux-x86_64/egg/pattern/web/imap\n",
            "copying build/lib/pattern/web/utils.py -> build/bdist.linux-x86_64/egg/pattern/web\n",
            "creating build/bdist.linux-x86_64/egg/pattern/web/oauth\n",
            "copying build/lib/pattern/web/oauth/__init__.py -> build/bdist.linux-x86_64/egg/pattern/web/oauth\n",
            "creating build/bdist.linux-x86_64/egg/pattern/web/locale\n",
            "copying build/lib/pattern/web/locale/__init__.py -> build/bdist.linux-x86_64/egg/pattern/web/locale\n",
            "creating build/bdist.linux-x86_64/egg/pattern/web/cache\n",
            "copying build/lib/pattern/web/cache/__init__.py -> build/bdist.linux-x86_64/egg/pattern/web/cache\n",
            "copying build/lib/pattern/web/__init__.py -> build/bdist.linux-x86_64/egg/pattern/web\n",
            "copying build/lib/pattern/web/api.py -> build/bdist.linux-x86_64/egg/pattern/web\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-context.txt -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/inflect.py -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-verbs.txt -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-morphology.txt -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/fr/fr-sentiment.xml -> build/bdist.linux-x86_64/egg/pattern/text/fr\n",
            "copying build/lib/pattern/text/tree.py -> build/bdist.linux-x86_64/egg/pattern/text\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/es-morphology.txt -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/es-verbs.txt -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/es-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/es-context.txt -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/es-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/es-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/inflect.py -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "copying build/lib/pattern/text/es/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/es\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-context.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/inflect_quantify.py -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/modality.py -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/wordlist/basic.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/wordlist/academic.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/wordlist/time.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/wordlist/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/wordlist/profanity.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/wordlist/stopwords.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordlist\n",
            "copying build/lib/pattern/text/en/en-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-sentiment.xml -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-verbs.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-model.slp -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/inflect.py -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/en/wordnet\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/data.adv -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/data.verb -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/LICENSE.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/data.adj -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/index.sense -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/data.noun1 -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/index.verb -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/index.noun -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/index.adv -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/lexnames -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/data.noun2 -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/index.32 -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/dict/index.adj -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/dict\n",
            "copying build/lib/pattern/text/en/wordnet/resnik-ic3.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet\n",
            "copying build/lib/pattern/text/en/wordnet/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet\n",
            "copying build/lib/pattern/text/en/wordnet/resnik-ic2.txt -> build/bdist.linux-x86_64/egg/pattern/text/en/wordnet\n",
            "copying build/lib/pattern/text/en/en-morphology.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "copying build/lib/pattern/text/en/en-entities.txt -> build/bdist.linux-x86_64/egg/pattern/text/en\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-morphology.txt -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-context.txt -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-sentiment.xml -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-verbs.txt -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/nl-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/inflect.py -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/nl/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/nl\n",
            "copying build/lib/pattern/text/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/de-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/de-verbs.txt -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/de-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/inflect.py -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/de-morphology.txt -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/de-context.txt -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/de/de-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/de\n",
            "copying build/lib/pattern/text/search.py -> build/bdist.linux-x86_64/egg/pattern/text\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-verbs.txt -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-morphology.txt -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-sentiment.xml -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/inflect.py -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "copying build/lib/pattern/text/it/it-context.txt -> build/bdist.linux-x86_64/egg/pattern/text/it\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/text/ru/ru-lexicon.txt -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/text/ru/ru-entities.txt -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/text/ru/ru-spelling.txt -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/text/ru/__main__.py -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/text/ru/ru-model.slp -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "creating build/bdist.linux-x86_64/egg/pattern/text/ru/wordlist\n",
            "copying build/lib/pattern/text/ru/wordlist/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/ru/wordlist\n",
            "copying build/lib/pattern/text/ru/wordlist/stopwords.txt -> build/bdist.linux-x86_64/egg/pattern/text/ru/wordlist\n",
            "copying build/lib/pattern/text/ru/ru-frequency.txt -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/text/ru/__init__.py -> build/bdist.linux-x86_64/egg/pattern/text/ru\n",
            "copying build/lib/pattern/canvas.js -> build/bdist.linux-x86_64/egg/pattern\n",
            "copying build/lib/pattern/helpers.py -> build/bdist.linux-x86_64/egg/pattern\n",
            "creating build/bdist.linux-x86_64/egg/pattern/graph\n",
            "copying build/lib/pattern/graph/graph.js -> build/bdist.linux-x86_64/egg/pattern/graph\n",
            "copying build/lib/pattern/graph/__init__.py -> build/bdist.linux-x86_64/egg/pattern/graph\n",
            "copying build/lib/pattern/graph/commonsense.py -> build/bdist.linux-x86_64/egg/pattern/graph\n",
            "copying build/lib/pattern/graph/commonsense.csv -> build/bdist.linux-x86_64/egg/pattern/graph\n",
            "creating build/bdist.linux-x86_64/egg/pattern/vector\n",
            "copying build/lib/pattern/vector/stopwords-es.txt -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "copying build/lib/pattern/vector/stopwords-fr.txt -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "creating build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/COPYRIGHT-liblinear.txt -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/COPYRIGHT-libsvm.txt -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/libsvm.py -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/liblinear.py -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/README.txt -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/libsvmutil.py -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/liblinearutil.py -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/__init__.py -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/svm/INSTALL.txt -> build/bdist.linux-x86_64/egg/pattern/vector/svm\n",
            "copying build/lib/pattern/vector/stopwords-nl.txt -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "copying build/lib/pattern/vector/stemmer.py -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "copying build/lib/pattern/vector/stopwords-de.txt -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "copying build/lib/pattern/vector/__init__.py -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "copying build/lib/pattern/vector/stopwords-en.txt -> build/bdist.linux-x86_64/egg/pattern/vector\n",
            "creating build/bdist.linux-x86_64/egg/pattern/db\n",
            "copying build/lib/pattern/db/__init__.py -> build/bdist.linux-x86_64/egg/pattern/db\n",
            "copying build/lib/pattern/__init__.py -> build/bdist.linux-x86_64/egg/pattern\n",
            "creating build/bdist.linux-x86_64/egg/pattern/server\n",
            "creating build/bdist.linux-x86_64/egg/pattern/server/static\n",
            "copying build/lib/pattern/server/static/robots.txt -> build/bdist.linux-x86_64/egg/pattern/server/static\n",
            "copying build/lib/pattern/server/__init__.py -> build/bdist.linux-x86_64/egg/pattern/server\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/imap/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/oauth/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/locale/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/cache/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/web/api.py to api.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/fr/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/fr/inflect.py to inflect.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/fr/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/tree.py to tree.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/es/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/es/inflect.py to inflect.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/es/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/inflect_quantify.py to inflect_quantify.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/modality.py to modality.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/wordlist/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/inflect.py to inflect.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/wordnet/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/en/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/nl/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/nl/inflect.py to inflect.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/nl/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/de/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/de/inflect.py to inflect.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/de/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/search.py to search.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/it/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/it/inflect.py to inflect.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/it/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/ru/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/ru/wordlist/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/text/ru/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/helpers.py to helpers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/graph/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/graph/commonsense.py to commonsense.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/svm/libsvm.py to libsvm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/svm/liblinear.py to liblinear.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/svm/libsvmutil.py to libsvmutil.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/svm/liblinearutil.py to liblinearutil.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/svm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/stemmer.py to stemmer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/vector/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/db/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pattern/server/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Pattern.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Pattern.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Pattern.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Pattern.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Pattern.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Pattern.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating dist\n",
            "creating 'dist/Pattern-3.6.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Pattern-3.6.1-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/Pattern-3.6.1-py3.6.egg\n",
            "Extracting Pattern-3.6.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Pattern 3.6.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Pattern-3.6.1-py3.6.egg\n",
            "Processing dependencies for Pattern==3.6.1\n",
            "Searching for cherrypy\n",
            "Reading https://pypi.org/simple/cherrypy/\n",
            "Downloading https://files.pythonhosted.org/packages/a8/f9/e11f893dcabe6bc222a1442bf5e14f0322a2d363c92910ed41947078a35a/CherryPy-18.6.0-py2.py3-none-any.whl#sha256=c0a7283f02a384c112a0a18404fd3abd849fc7fd4bec19378067150a2573d2e4\n",
            "Best match: CherryPy 18.6.0\n",
            "Processing CherryPy-18.6.0-py2.py3-none-any.whl\n",
            "Installing CherryPy-18.6.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding CherryPy 18.6.0 to easy-install.pth file\n",
            "Installing cherryd script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/CherryPy-18.6.0-py3.6.egg\n",
            "Searching for python-docx\n",
            "Reading https://pypi.org/simple/python-docx/\n",
            "Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz#sha256=bc76ecac6b2d00ce6442a69d03a6f35c71cd72293cd8405a7472dfe317920024\n",
            "Best match: python-docx 0.8.10\n",
            "Processing python-docx-0.8.10.tar.gz\n",
            "Writing /tmp/easy_install-ycai51dv/python-docx-0.8.10/setup.cfg\n",
            "Running python-docx-0.8.10/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ycai51dv/python-docx-0.8.10/egg-dist-tmp-14at50pl\n",
            "no previously-included directories found matching 'docs/.build'\n",
            "warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
            "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "docx.__pycache__.api.cpython-36: module references __file__\n",
            "docx.parts.__pycache__.hdrftr.cpython-36: module references __file__\n",
            "docx.parts.__pycache__.settings.cpython-36: module references __file__\n",
            "docx.parts.__pycache__.styles.cpython-36: module references __file__\n",
            "creating /usr/local/lib/python3.6/dist-packages/python_docx-0.8.10-py3.6.egg\n",
            "Extracting python_docx-0.8.10-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding python-docx 0.8.10 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/python_docx-0.8.10-py3.6.egg\n",
            "Searching for pdfminer.six\n",
            "Reading https://pypi.org/simple/pdfminer.six/\n",
            "Downloading https://files.pythonhosted.org/packages/0b/04/f62d5834c2bdf90afcaeb23bb5241033c44e27000de64ad8472253daa4a8/pdfminer.six-20200402-py3-none-any.whl#sha256=f5ab6aae4999c1460aa569c716cde75c1585e3f4f2e3fcaf6c950696937d1741\n",
            "Best match: pdfminer.six 20200402\n",
            "Processing pdfminer.six-20200402-py3-none-any.whl\n",
            "Installing pdfminer.six-20200402-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pdfminer.six 20200402 to easy-install.pth file\n",
            "Installing dumppdf.py script to /usr/local/bin\n",
            "Installing pdf2txt.py script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pdfminer.six-20200402-py3.6.egg\n",
            "Searching for feedparser\n",
            "Reading https://pypi.org/simple/feedparser/\n",
            "Downloading https://files.pythonhosted.org/packages/d2/08/f0f340ebd052b791fe1faefb0ef281bf11be793208eb39bd5da69f518cfe/feedparser-6.0.0b3-py2.py3-none-any.whl#sha256=d4885f34627140c9076f43022e3be49e6b80ddb25a6dd1a4d40f795546faec1f\n",
            "Best match: feedparser 6.0.0b3\n",
            "Processing feedparser-6.0.0b3-py2.py3-none-any.whl\n",
            "Installing feedparser-6.0.0b3-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding feedparser 6.0.0b3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/feedparser-6.0.0b3-py3.6.egg\n",
            "Searching for mysqlclient\n",
            "Reading https://pypi.org/simple/mysqlclient/\n",
            "Downloading https://files.pythonhosted.org/packages/d0/97/7326248ac8d5049968bf4ec708a5d3d4806e412a42e74160d7f266a3e03a/mysqlclient-1.4.6.tar.gz#sha256=f3fdaa9a38752a3b214a6fe79d7cae3653731a53e577821f9187e67cbecb2e16\n",
            "Best match: mysqlclient 1.4.6\n",
            "Processing mysqlclient-1.4.6.tar.gz\n",
            "Writing /tmp/easy_install-xgovhbmu/mysqlclient-1.4.6/setup.cfg\n",
            "Running mysqlclient-1.4.6/setup.py -q bdist_egg --dist-dir /tmp/easy_install-xgovhbmu/mysqlclient-1.4.6/egg-dist-tmp-ed3scbvr\n",
            "/bin/sh: 1: mysql_config: not found\n",
            "/bin/sh: 1: mariadb_config: not found\n",
            "/bin/sh: 1: mysql_config: not found\n",
            "error: mysql_config not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BXqwLs_YNHx",
        "colab_type": "text"
      },
      "source": [
        "__Sentiment Analysis__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oVbzHlbEPVpB",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SentimentAnalysis():\n",
        "\n",
        "    def __init__(self):        \n",
        "        dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\"]\n",
        "        dataTrain = pd.read_csv(dataset_path + \"/train_sentiment.csv\", sep=',', header=None, names = columnNames)\n",
        "        dataTest = pd.read_csv(dataset_path + \"/test_sentiment.csv\", sep=',', header=None, names = columnNames)\n",
        "        \n",
        "        #dropping columns\n",
        "        columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector']\n",
        "        dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
        "        dataTest = dataTest.drop(columns=columnsToRemove)\n",
        "        dataTrain = dataTrain.loc[1:] \n",
        "        dataTest = dataTest.loc[1:]\n",
        "        dataTrain = dataTrain.dropna()  \n",
        "        dataTest = dataTest.dropna() \n",
        "    \n",
        "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "                ('LogRCV', tfidfV),\n",
        "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
        "                ])\n",
        "\n",
        "        self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['vader_polarity'])\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
        "        score = metrics.accuracy_score(dataTest['vader_polarity'], predicted_LogR)\n",
        "        print(\"Sentiment Analysis Model Trained - accuracy:   %0.6f\" % score)\n",
        "\n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return float(predicedProb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5d1Of--PPVpF"
      },
      "source": [
        "__[Flesch–Kincaid Grade Level](https://https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests)__\n",
        "<br>\n",
        "It is a readability tests designed to indicate how difficult a passage in English is to understand. \n",
        "\n",
        "Flesch–Kincaid grade levels are used extensively in the field of education. The \"Flesch–Kincaid Grade Level Formula\" instead presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts.\n",
        "\n",
        "Due to the grade level formula's construction, the score does not have an upper bound. The formula emphasises sentence length over word length. By creating one-word strings with hundreds of random characters, grade levels may be attained that are hundreds of times larger than high school completion in the United States."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Frc_0N2xPVpF",
        "colab": {}
      },
      "source": [
        "from builtins import zip\n",
        "from builtins import str\n",
        "from builtins import range\n",
        "from past.utils import old_div\n",
        "from builtins import object\n",
        "import json\n",
        "import multiprocessing\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "from decorator import contextmanager"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GI5hKjZPPVpH",
        "colab": {}
      },
      "source": [
        "def split_into_sentences(text):\n",
        "    caps = \"([A-Z])\"\n",
        "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "    websites = \"[.](com|net|org|io|gov)\"\n",
        "    digits = \"([0-9])\"\n",
        "\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
        "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
        "    if \"Ph.D\" in text:\n",
        "        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
        "    if \"e.g.\" in text:\n",
        "        text = text.replace(\"e.g.\", \"e<prd>g<prd>\")\n",
        "    if \"i.e.\" in text:\n",
        "        text = text.replace(\"i.e.\", \"i<prd>e<prd>\")\n",
        "    text = re.sub(\"\\s\" + caps + \"[.] \", \" \\\\1<prd> \", text)\n",
        "    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n",
        "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n",
        "    text = re.sub(caps + \"[.]\" + caps + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
        "    text = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n",
        "    text = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n",
        "    text = re.sub(\" \" + caps + \"[.]\", \" \\\\1<prd>\", text)\n",
        "    text = re.sub(digits + \"[.]\" + digits, \"\\\\1<prd>\\\\2\", text)\n",
        "    if \"”\" in text:\n",
        "        text = text.replace(\".”\", \"”.\")\n",
        "    if \"\\\"\" in text:\n",
        "        text = text.replace(\".\\\"\", \"\\\".\")\n",
        "    if \"!\" in text:\n",
        "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
        "    if \"?\" in text:\n",
        "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
        "    text = text.replace(\"\\n\", \" <stop>\")\n",
        "    text = text.replace(\".\", \".<stop>\")\n",
        "    text = text.replace(\"?\", \"?<stop>\")\n",
        "    text = text.replace(\"!\", \"!<stop>\")\n",
        "    text = text.replace(\"<prd>\", \".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s) >= 2]\n",
        "    return sentences\n",
        "\n",
        "def sentence_count(text):\n",
        "    ignore_count = 0\n",
        "    sentences = split_into_sentences(text)\n",
        "    for sentence in sentences:\n",
        "        if lexicon_count(sentence) <= 2:\n",
        "            ignore_count = ignore_count + 1\n",
        "    sentence_cnt = len(sentences) - ignore_count\n",
        "    if sentence_cnt < 1:\n",
        "        sentence_cnt = 1\n",
        "    return sentence_cnt\n",
        "\n",
        "def syllable_count(text):\n",
        "    exclude = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
        "    count = 0\n",
        "    vowels = 'aeiouy'\n",
        "    text = text.lower()\n",
        "    text = \"\".join(x for x in text if x not in exclude)\n",
        "\n",
        "    if text is None:\n",
        "        return 0\n",
        "    elif len(text) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        if text[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(text)):\n",
        "            if text[index] in vowels and text[index - 1] not in vowels:\n",
        "                count += 1\n",
        "        if text.endswith('e'):\n",
        "            count -= 1\n",
        "        if text.endswith('le'):\n",
        "            count += 1\n",
        "        if count == 0:\n",
        "            count += 1\n",
        "        count = count - (0.1 * count)\n",
        "        return count\n",
        "\n",
        "def lexicon_count(text, removepunct=True):\n",
        "    exclude = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
        "    if removepunct:\n",
        "        text = ''.join(ch for ch in text if ch not in exclude)\n",
        "    count = len(text.split())\n",
        "    return count\n",
        "\n",
        "def avg_sentence_length(text):\n",
        "    lc = lexicon_count(text)\n",
        "    sc = sentence_count(text)\n",
        "    a_s_l = float(old_div(lc, sc))\n",
        "    return round(a_s_l, 1)\n",
        "\n",
        "def avg_syllables_per_word(text):\n",
        "    syllable = syllable_count(text)\n",
        "    words = lexicon_count(text)\n",
        "    try:\n",
        "        a_s_p_w = old_div(float(syllable), float(words))\n",
        "        return round(a_s_p_w, 1)\n",
        "    except ZeroDivisionError:\n",
        "        # print \"Error(ASyPW): Number of words are zero, cannot divide\"\n",
        "        return 1\n",
        "\n",
        "def flesch_kincaid_grade(text):\n",
        "    if sys.version_info < (3, 0):\n",
        "        text = text.decode('ascii', 'ignore')\n",
        "        \n",
        "    a_s_l = avg_sentence_length(text)\n",
        "    a_s_w = avg_syllables_per_word(text)\n",
        "    f_k_r_a = float(0.39 * a_s_l) + float(11.8 * a_s_w) - 15.59\n",
        "    return round(f_k_r_a, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KQIVg0qqPVpJ"
      },
      "source": [
        "__Negative Perspective Analysis__\n",
        "\n",
        "Determine the degree of negative perspective of text\n",
        "Returns an float for score (higher is more negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yd-KESIWPVpK",
        "colab": {}
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def check_neg_persp(input_text, include_nt=True):\n",
        "    if sys.version_info < (3, 0):\n",
        "        text = text.decode('ascii', 'ignore')\n",
        "    sa = SentimentIntensityAnalyzer()\n",
        "    text_nohyph = input_text.replace(\"-\", \" \")  \n",
        "    txt_lwr = str(text_nohyph).lower()\n",
        "    input_words = ''.join(ch for ch in txt_lwr if ch not in '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~').split()\n",
        "    vader_score = sa.polarity_scores(input_text)\n",
        "    vader_neg =  vader_score['neg']\n",
        "    vader_compound = vader_score['compound']\n",
        "\n",
        "    neg_persp_score = 0.0\n",
        "    neg_words = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
        "                  \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
        "                  \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
        "                  \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
        "                  \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
        "                  \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
        "                  \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
        "                  \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
        "\n",
        "    for word in neg_words:\n",
        "        if word in input_words:\n",
        "            neg_persp_score += 1\n",
        "    if include_nt:\n",
        "        for word in input_words:\n",
        "            if \"n't\" in word and word not in neg_words:\n",
        "                neg_persp_score += 1\n",
        "    if vader_neg > 0.0:\n",
        "        neg_persp_score += vader_neg\n",
        "    if vader_compound < 0.0:\n",
        "        neg_persp_score += abs(vader_compound)\n",
        "    return neg_persp_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kXjBPCgGPVpL"
      },
      "source": [
        "__Degree of Certainty Analysis__\n",
        "\n",
        "Checking if a Statement is a Fact.\n",
        "It returns a value between -1 to 1. For facts, it returns a value greater than 0.5.\n",
        "Similarly, for a sentence which is not certain returned a value around 0.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EhzNibs-PVpM",
        "colab": {}
      },
      "source": [
        "from pattern.en import Sentence, parse, modality, pprint\n",
        "\n",
        "def check_certainty(text):\n",
        "    if sys.version_info < (3, 0):\n",
        "        text = text.decode('ascii', 'ignore')\n",
        "        \n",
        "    sentence = parse(text, relations=True, lemmata=True)\n",
        "    sentence_obj = Sentence(sentence)\n",
        "    return round(modality(sentence_obj), 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fSLCG4M5PVpO"
      },
      "source": [
        "__Lexicon Analysis__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6i_bHg0PVpO",
        "colab": {}
      },
      "source": [
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "src = dataset_path + \"/lexicon.json\"\n",
        "exampleJSONFile = open(src)\n",
        "with open(src) as file:\n",
        "    ref_lexicons = json.loads(file.read())\n",
        "\n",
        "##### List of presupposition verbs (comprised of Factive, Implicative, Coherence, Causation, & Assertion markers):\n",
        "### Factive verbs derived from:\n",
        "# Paul Kiparsky and Carol Kiparsky. 1970. Fact. In M.Bierwisch and K.E.Heidolph, editors, Progress in\n",
        "#  Linguistics, pages 143–173.Mouton, The Hague.\n",
        "### Implicative verbs derived from\n",
        "# Lauri Karttunen. 1971. Implicative verbs. Language, 47(2):340–358.\n",
        "##### List of coherence markers derived from:\n",
        "# Knott, Alistair. 1996. A Data-Driven Methodology for Motivating a Set of\n",
        "#  Coherence Relations. Ph.D. dissertation, University of Edinburgh, UK.\n",
        "##### List of assertive derived from:\n",
        "# Joan B. Hooper. 1975. On assertive predicates. In J. Kimball, editor,\n",
        "#  Syntax and Semantics, volume 4, pages 91–124. Academic Press, New York.\n",
        "##### List of Causation words from LIWC\n",
        "#########################################################################\n",
        "presup = ref_lexicons['presupposition']\n",
        "\n",
        "##### List of hedge words derived from:\n",
        "# Ken Hyland. 2005. Metadiscourse: Exploring Interaction in Writing.\n",
        "# Continuum, London and New York.\n",
        "##### List of tentative words from LIWC\n",
        "##### List of NPOV hedge & \"weasel\" words to watch from\n",
        "# https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Words_to_watch\n",
        "#########################################################################\n",
        "doubt = ref_lexicons['doubt_markers']\n",
        "\n",
        "##### List of biased/partisan words derived from:\n",
        "# Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic Models for\n",
        "#     Analyzing and Detecting Biased Language. Proceedings of ACL 2013.\n",
        "# and\n",
        "# Gentzkow, Econometrica 2010: What Drives Media Slant? Evidence from U.S. Daily Newspapers\n",
        "#########################################################################\n",
        "partisan = ref_lexicons['partisan']\n",
        "\n",
        "##### List of opinion laden words extracted from:\n",
        "# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
        "#  Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
        "#  Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
        "##### List of strong/weak subjective words extracted from:\n",
        "# Theresa Wilson, Janyce Wiebe and Paul Hoffmann (2005). Recognizing Contextual\n",
        "# Polarity in Phrase-Level Sentiment Analysis. Proceedings of HLT/EMNLP 2005,\n",
        "# Vancouver, Canada.\n",
        "##### List of degree modifiers derived from:\n",
        "# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
        "#  Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
        "#  Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
        "#########################################################################\n",
        "value_laden = ref_lexicons['value_laden']\n",
        "\n",
        "##### List of figurative expressions derived from:\n",
        "###English-language idioms\n",
        "# https://en.wikipedia.org/wiki/English-language_idioms.\n",
        "# and\n",
        "### List of English-language metaphors\n",
        "# https://en.wikipedia.org/wiki/List_of_English-language_metaphors\n",
        "# and\n",
        "### List of political metaphors\n",
        "# https://en.wikipedia.org/wiki/List_of_political_metaphors\n",
        "### List of NPOV \"puffery & peacock\" words to watch from\n",
        "# https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Words_to_watch\n",
        "#########################################################################\n",
        "figurative = ref_lexicons['figurative']\n",
        "\n",
        "##### Lists of attribution bias/actor-observer bias/ultimate attribution markers\n",
        "# LIWC 3rd person pronouns (combines S/he and They)\n",
        "# LIWC achievement words\n",
        "# LIWC work words\n",
        "attribution = ref_lexicons['attribution']\n",
        "\n",
        "#### List of self reference pronouns from LIWC\n",
        "self_refer = ref_lexicons['self_reference']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BlSj8lVRPVpR",
        "colab": {}
      },
      "source": [
        "def count_feature_freq(feature_list, input_text):\n",
        "    cnt = 0\n",
        "    text_nohyph = input_text.replace(\"-\", \" \")  \n",
        "    txt_lwr = str(text_nohyph).lower()\n",
        "    tokens_list = ''.join(ch for ch in txt_lwr if ch not in '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~').split()\n",
        "    # count unigrams\n",
        "    for w in tokens_list:\n",
        "        if w in feature_list:\n",
        "            cnt += 1\n",
        "        # count wildcard features\n",
        "        for feature in feature_list:\n",
        "            if str(feature).endswith('*') and str(w).startswith(feature[:-1]):\n",
        "                cnt += 1\n",
        "    # count n_gram phrase features\n",
        "    for feature in feature_list:\n",
        "        if \" \" in feature and feature in txt_lwr:\n",
        "            cnt += str(txt_lwr).count(feature)\n",
        "    return cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FlQgMTrWPVpT"
      },
      "source": [
        "__Extract Bias Features__\n",
        "\n",
        "get all the bias features score and return them in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g86v9sJCPVpT",
        "outputId": "dcc5c6c0-2c17-4922-b127-3a5811480a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk.sentiment\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sentiment_analyzer = SentimentAnalysis()\n",
        "sa = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment Analysis Model Trained - accuracy:   0.943918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tATpIKcrPVpX",
        "colab": {}
      },
      "source": [
        "def extract_bias_features_from_dataframe(df):\n",
        "    bias_df = df.copy()\n",
        "\n",
        "    bias_df['fk_gl'] = df['headline_text'].apply(flesch_kincaid_grade)\n",
        "    bias_df['vader_sentiment'] = df['headline_text'].apply(sentiment_analyzer.predict)\n",
        "    bias_df['neg_persp'] = df['headline_text'].apply(check_neg_persp)\n",
        "    bias_df['certainty'] = df['headline_text'].apply(check_certainty)\n",
        "\n",
        "    ## LEXICON LEVEL MEASURES\n",
        "    # presupposition markers\n",
        "    bias_df['presup_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(presup, x))\n",
        "\n",
        "    # doubt markers\n",
        "    bias_df['doubt_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(doubt, x))\n",
        "\n",
        "    # partisan words and phrases\n",
        "    bias_df['partisan_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(partisan, x))\n",
        "\n",
        "    # subjective value laden word count\n",
        "    bias_df['value_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(value_laden, x))\n",
        "\n",
        "    # figurative language markers\n",
        "    bias_df['figurative_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(figurative, x))\n",
        "\n",
        "    # attribution markers\n",
        "    bias_df['attribution_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(attribution, x))\n",
        "\n",
        "    # self reference pronouns\n",
        "    bias_df['self_refer_cnt'] = df['headline_text'].apply(lambda x: count_feature_freq(self_refer, x))\n",
        "\n",
        "    bias_df_std = bias_df.iloc[:, 2:]\n",
        "    bias_df_scale = StandardScaler().fit_transform(bias_df_std)\n",
        "    bias_df_std = pd.DataFrame(bias_df_scale , index=bias_df_std.index, columns=bias_df_std.columns)\n",
        "\n",
        "    bias_df_std['label'] = bias_df['label']\n",
        "    return bias_df, bias_df_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L6A2rqW2_iRz"
      },
      "source": [
        "###__Bias Fake News Dataset__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63ojD7gki88o",
        "colab_type": "text"
      },
      "source": [
        "[Fake News Datset Source](https://www.kaggle.com/mrisdal/fake-news/data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reZ717xa14xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "bias_extracted_std_file = dataset_path + \"/bias_extracted_std.csv\"\n",
        "bias_extracted_file = dataset_path + \"/bias_extracted.csv\"\n",
        "\n",
        "if path.exists(bias_extracted_file) or path.exists(bias_extracted_file):\n",
        "    bias_extracted = pd.read_csv(bias_extracted_file)\n",
        "    bias_extracted_std = pd.read_csv(bias_extracted_std_file)\n",
        "else:\n",
        "    bias_df = pd.read_csv(dataset_path + \"/bias.csv\")\n",
        "    bias_news_df = pd.DataFrame(columns = ['headline_text', 'label']) \n",
        "    bias_news_df['label'] = bias_df['type']\n",
        "    bias_news_df['headline_text'] = bias_df['text']\n",
        "    bias_news_df = bias_news_df.dropna()\n",
        "    bias_extracted, bias_extracted_std = extract_bias_features_from_dataframe(bias_news_df)\n",
        "    bias_extracted.to_csv(bias_extracted_file , sep=',', index=False)\n",
        "    bias_extracted_std.to_csv(bias_extracted_std_file, sep=',', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkrz7aKXcc0l",
        "colab_type": "text"
      },
      "source": [
        "####__Dataset balance__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDknwhbAYFjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample\n",
        "bias_bias = bias_extracted_std[bias_extracted_std['label'] == 'bias']\n",
        "bias_bs = bias_extracted_std[bias_extracted_std['label'] == 'bs']\n",
        "bias_conspiracy = bias_extracted_std[bias_extracted_std['label'] == 'conspiracy']\n",
        "bias_fake = bias_extracted_std[bias_extracted_std['label'] == 'fake']\n",
        "bias_hate = bias_extracted_std[bias_extracted_std['label'] == 'hate']\n",
        "bias_junksci = bias_extracted_std[bias_extracted_std['label'] == 'junksci']\n",
        "bias_satire = bias_extracted_std[bias_extracted_std['label'] == 'satire']\n",
        "bias_state = bias_extracted_std[bias_extracted_std['label'] == 'state']\n",
        "\n",
        "bias_bias_downsampled = resample(bias_bias, \n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=329,     # to match minority class\n",
        "                                 random_state=123) # reproducible result\n",
        "\n",
        "bias_bs_downsampled = resample(bias_bs, \n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=329,     # to match minority class\n",
        "                                 random_state=123) # reproducible result\n",
        "\n",
        "bias_fake_upsampled = resample(bias_fake, \n",
        "                                 replace=True,    # sample without replacement\n",
        "                                 n_samples=329,     # to match minority class\n",
        "                                 random_state=123) # reproducible result\n",
        "\n",
        "bias_junksci_upsampled = resample(bias_junksci, \n",
        "                                 replace=True,    # sample without replacement\n",
        "                                 n_samples=329,     # to match minority class\n",
        "                                 random_state=123) # reproducible result                        \n",
        "\n",
        "bias_state_upsampled = resample(bias_state, \n",
        "                                 replace=True,    # sample without replacement\n",
        "                                 n_samples=329,     # to match minority class\n",
        "                                 random_state=123) # reproducible result                        \n",
        "\n",
        "bias_satire_upsampled = resample(bias_satire, \n",
        "                                 replace=True,    # sample without replacement\n",
        "                                 n_samples=329,     # to match minority class\n",
        "                                 random_state=123) # reproducible result \n",
        "\n",
        "bias_extracted_std = pd.concat([bias_bias_downsampled, \n",
        "                                 bias_bs_downsampled, \n",
        "                                 bias_conspiracy, \n",
        "                                 bias_fake_upsampled, \n",
        "                                 bias_hate,\n",
        "                                 bias_junksci_upsampled,\n",
        "                                 bias_state_upsampled,\n",
        "                                 bias_satire_upsampled])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5-v59dpp_iR-",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = bias_extracted_std.iloc[:, :11]\n",
        "y = bias_extracted_std.iloc[:, 11]\n",
        "X_bias_train, X_bias_test, y_bias_train, y_bias_test = train_test_split(X, y, test_size=0.4, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dbwdx8cS_iSA"
      },
      "source": [
        "####__XGBoost Feature Importance and Feature Selection__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ft359K4evt5",
        "colab_type": "code",
        "outputId": "af778372-fe58-40b5-859b-a2de27e5f0f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from matplotlib import pyplot\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "xgb_model = XGBClassifier(max_depth=7)\n",
        "xgb_model.fit(X_bias_train, y_bias_train)\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "\n",
        "with open(dataset_path + '/bias_xgboost.pkl', 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "pred_xgboost = xgb_model.predict(X_bias_test)\n",
        "score = metrics.accuracy_score(y_bias_test, pred_xgboost)\n",
        "pre_score = metrics.precision_score(y_bias_test, pred_xgboost, average='weighted')\n",
        "f1 = metrics.f1_score(y_bias_test, pred_xgboost, average='weighted') \n",
        "print(\"XGBoot Model Trained - accuracy: {:.4f}   precision: {:.4f}   F1 Score: {:.4f}\".format(score, pre_score, f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoot Model Trained - accuracy: 0.6706   precision: 0.6551   F1 Score: 0.6593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yn8DzGxHw3w",
        "colab_type": "code",
        "outputId": "42c7665b-c033-4861-edf4-e03cf545b635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "pd.crosstab(y_bias_test, pred_xgboost, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Predicted</th>\n",
              "      <th>bias</th>\n",
              "      <th>bs</th>\n",
              "      <th>conspiracy</th>\n",
              "      <th>fake</th>\n",
              "      <th>hate</th>\n",
              "      <th>junksci</th>\n",
              "      <th>satire</th>\n",
              "      <th>state</th>\n",
              "      <th>All</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bias</th>\n",
              "      <td>57</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bs</th>\n",
              "      <td>20</td>\n",
              "      <td>49</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conspiracy</th>\n",
              "      <td>25</td>\n",
              "      <td>11</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fake</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <td>15</td>\n",
              "      <td>31</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>junksci</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>satire</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>state</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>All</th>\n",
              "      <td>120</td>\n",
              "      <td>127</td>\n",
              "      <td>141</td>\n",
              "      <td>136</td>\n",
              "      <td>70</td>\n",
              "      <td>145</td>\n",
              "      <td>130</td>\n",
              "      <td>151</td>\n",
              "      <td>1020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Predicted   bias   bs  conspiracy  fake  hate  junksci  satire  state   All\n",
              "True                                                                       \n",
              "bias          57   19          29     2     8        5       4     14   138\n",
              "bs            20   49          22     1    21       13       6     11   143\n",
              "conspiracy    25   11          67     1     4        3       2      5   118\n",
              "fake           0    0           0   131     0        0       0      0   131\n",
              "hate          15   31           9     1    32        4       5      4   101\n",
              "junksci        0    0           0     0     5      120       0      0   125\n",
              "satire         0    9          11     0     0        0     113      2   135\n",
              "state          3    8           3     0     0        0       0    115   129\n",
              "All          120  127         141   136    70      145     130    151  1020"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4_1wMSn_iSU"
      },
      "source": [
        "####__Classification Metrics__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t8adaSGGldi",
        "colab_type": "text"
      },
      "source": [
        "|Classificaion| Accuracy Score| Precision Score | F1 Score |\n",
        "|---|---|---|---|\n",
        "|XGBoost| 0.6706 | 0.6551 | 0.6593\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLH2hlp93WYI",
        "colab_type": "text"
      },
      "source": [
        "###__Political Social Media Dataset__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbdQ5yh5KZ-",
        "colab_type": "text"
      },
      "source": [
        "[Political Social Media Dataset Source](https://www.kaggle.com/crowdflower/political-social-media-posts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnudD6e8LrhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "media_extracted_std_file = dataset_path + \"/media_extracted_std.csv\"\n",
        "media_extracted_file = dataset_path + \"/media_extracted.csv\"\n",
        "\n",
        "if path.exists(bias_extracted_file) or path.exists(bias_extracted_file):\n",
        "    media_extracted = pd.read_csv(media_extracted_file)\n",
        "    media_extracted_std = pd.read_csv(media_extracted_std_file)\n",
        "else:\n",
        "    media_df = pd.read_csv(dataset_path  + \"/political_social_media.csv', encoding='latin1\")\n",
        "    media_news_df = pd.DataFrame(columns = ['headline_text', 'label']) \n",
        "    media_news_df['label'] = media_df['bias']\n",
        "    media_news_df['headline_text'] = media_df['text']\n",
        "    media_news_df = media_news_df.dropna()\n",
        "    media_extracted, media_extracted_std = extract_bias_features_from_dataframe(media_news_df)\n",
        "    media_extracted.to_csv(media_extracted_std_file, sep=',', index=False)\n",
        "    media_extracted_std.to_csv(media_extracted_file, sep=',', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh2S5lKscuXT",
        "colab_type": "text"
      },
      "source": [
        "####__Dataset balance__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PAxVS4eV2jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "media_neutral = media_extracted_std[media_extracted_std['label'] == 'neutral']\n",
        "media_partisan = media_extracted_std[media_extracted_std['label'] == 'partisan']\n",
        "\n",
        "media_neutral_downsampled = resample(media_neutral, \n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=1310,     # to match minority class\n",
        "                                 random_state=123) # reproducible result\n",
        "\n",
        "media_extracted_std = pd.concat([media_neutral_downsampled, media_partisan])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1vJUovQaKpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = media_extracted_std.iloc[:, :11]\n",
        "y = media_extracted_std.iloc[:, 11]\n",
        "X_media_train, X_media_test, y_media_train, y_media_test = train_test_split(X, y, test_size=0.4, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXoNJUmZttWs",
        "colab_type": "text"
      },
      "source": [
        "####__XGBoost Feature Importance and Feature Selection__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIw-6FajmLSX",
        "colab_type": "code",
        "outputId": "eba2d718-6ef4-4b11-c38b-060dc693a405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "\n",
        "xgb_model = XGBClassifier(max_depth=3)\n",
        "xgb_model.fit(X_media_train, y_media_train)\n",
        "\n",
        "with open(dataset_path + \"/media_xgboost.pkl\", 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "pred_xgboost = xgb_model.predict(X_media_test)\n",
        "score = metrics.accuracy_score(y_media_test, pred_xgboost)\n",
        "pre_score = metrics.precision_score(y_media_test, pred_xgboost, average='weighted')\n",
        "f1 = metrics.f1_score(y_media_test, pred_xgboost, average='weighted') \n",
        "print(\"XGBoot Model Trained - accuracy: {:.4f}   precision: {:.4f}   F1 Score: {:.4f}\".format(score, pre_score, f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoot Model Trained - accuracy: 0.6269   precision: 0.6270   F1 Score: 0.6265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ln2DudlH579",
        "colab_type": "code",
        "outputId": "bd779a54-33b6-4a13-83b4-e37a5338e45a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "pd.crosstab(y_media_test, pred_xgboost, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Predicted</th>\n",
              "      <th>neutral</th>\n",
              "      <th>partisan</th>\n",
              "      <th>All</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>349</td>\n",
              "      <td>182</td>\n",
              "      <td>531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>partisan</th>\n",
              "      <td>209</td>\n",
              "      <td>308</td>\n",
              "      <td>517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>All</th>\n",
              "      <td>558</td>\n",
              "      <td>490</td>\n",
              "      <td>1048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Predicted  neutral  partisan   All\n",
              "True                              \n",
              "neutral        349       182   531\n",
              "partisan       209       308   517\n",
              "All            558       490  1048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAJKRSkvH2ap",
        "colab_type": "code",
        "outputId": "c449b445-cbf3-4bfd-ff7c-00ff869f9e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "for i in range(len(X_media_train.columns)):\n",
        "    print('{}: {} - {:.6f}'.format(i, X_media_train.columns[i], xgb_model.feature_importances_[i]))\n",
        "pyplot.bar(range(len(xgb_model.feature_importances_)), xgb_model.feature_importances_)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: fk_gl - 0.083017\n",
            "1: vader_sentiment - 0.083639\n",
            "2: neg_persp - 0.283886\n",
            "3: certainty - 0.086178\n",
            "4: presup_cnt - 0.069015\n",
            "5: doubt_cnt - 0.093660\n",
            "6: partisan_cnt - 0.087762\n",
            "7: value_cnt - 0.060404\n",
            "8: figurative_cnt - 0.000000\n",
            "9: attribution_cnt - 0.068294\n",
            "10: self_refer_cnt - 0.084144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raHeQsFACGDG",
        "colab_type": "text"
      },
      "source": [
        "####__Classification Metrics__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq5IMmZcCFKt",
        "colab_type": "text"
      },
      "source": [
        "|Classificaion| Accuracy Score| Precision Score | F1 Score |\n",
        "|---|---|---|---|\n",
        "|XGBoost| 0.6269 | 0.6270| 0.6197"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ulqAYp4VSi",
        "colab_type": "text"
      },
      "source": [
        "###__Liar Liar Dataset__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSHJ-x6EMm3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/junteng_dev/data_set\"\n",
        "\n",
        "fake_train_std_file = \"/content/drive/My Drive/junteng_dev/data_set/train_extracted_std.csv\"\n",
        "fake_train_file = \"/content/drive/My Drive/junteng_dev/data_set/train_extracted.csv\"\n",
        "fake_test_std_file = \"/content/drive/My Drive/junteng_dev/data_set/test_extracted_std.csv\"\n",
        "fake_test_file = \"/content/drive/My Drive/junteng_dev/data_set/test_extracted.csv\"\n",
        "\n",
        "if all([path.exists(fake_train_std_file), path.exists(fake_train_file), path.exists(fake_test_std_file), path.exists(fake_test_file)]):\n",
        "    fake_train_extracted_std = pd.read_csv(fake_train_std_file)\n",
        "    fake_test_extracted_std = pd.read_csv(fake_test_std_file)\n",
        "    fake_train_extracted = pd.read_csv(fake_train_file)\n",
        "    fake_test_extracted = pd.read_csv(fake_test_file)\n",
        "else:\n",
        "    fake_train_processed = pd.read_csv('/content/drive/My Drive/junteng_dev/data_set/train_processed.csv', sep=',')\n",
        "    fake_test_processed = pd.read_csv('/content/drive/My Drive/junteng_dev/data_set/test_processed.csv', sep=',')\n",
        "    columns_to_remove = ['Unnamed: 0', \n",
        "                        'jsonid', \n",
        "                        'subject', \n",
        "                        'speaker',\n",
        "                        'speakerjobtitle', \n",
        "                        'stateinfo', \n",
        "                        'partyaffiliation',\n",
        "                        'barelytruecounts',\n",
        "                        'falsecounts', \n",
        "                        'halftruecounts', \n",
        "                        'mostlytrueocunts',\n",
        "                        'pantsonfirecounts', \n",
        "                        'context']\n",
        "    fake_train_processed = fake_train_processed.drop(columns=columns_to_remove)\n",
        "    fake_test_processed = fake_test_processed.drop(columns=columns_to_remove)\n",
        "    fake_train_processed = fake_train_processed.dropna()  \n",
        "    fake_test_processed = fake_test_processed.dropna() \n",
        "    fake_train_extracted_std, fake_train_extracted = extract_bias_features_from_dataframe(fake_train_processed)\n",
        "    fake_train_extracted.to_csv(\"/content/drive/My Drive/junteng_dev/data_set/train_extracted.csv\", sep=',', index=False)\n",
        "    fake_train_extracted_std.to_csv(\"/content/drive/My Drive/junteng_dev/data_set/train_extracted_std.csv\", sep=',', index=False)\n",
        "    fake_test_extracted, fake_test_extracted_std = extract_bias_features_from_dataframe(fake_test_processed)\n",
        "    fake_test_extracted.to_csv(\"/content/drive/My Drive/junteng_dev/data_set/test_extracted.csv\", sep=',', index=False)\n",
        "    fake_test_extracted_std.to_csv(\"/content/drive/My Drive/junteng_dev/data_set/test_extracted_std.csv\", sep=',', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9AHoOJHDYF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_fake_train = fake_train_extracted.iloc[:, 3:14]\n",
        "y_fake_train = fake_train_extracted.iloc[:, 0]\n",
        "X_fake_test = fake_test_extracted.iloc[:, 3:14]\n",
        "y_fake_test = fake_test_extracted.iloc[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxG6XIdhdmrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=14)\n",
        "xgb_model.fit(X_bias_train, y_bias_train)\n",
        "X_fake_train_bias= xgb_model.predict(X_fake_train)\n",
        "X_fake_test_bias = xgb_model.predict(X_fake_test)\n",
        "\n",
        "xgb_model = XGBClassifier(max_depth=3)\n",
        "xgb_model.fit(X_media_train, y_media_train)\n",
        "X_fake_train_media = xgb_model.predict(X_fake_train)\n",
        "X_fake_test_media = xgb_model.predict(X_fake_test)\n",
        "\n",
        "X_fake_train['bias'] = X_fake_train_bias\n",
        "X_fake_train['media'] = X_fake_train_media\n",
        "\n",
        "X_fake_test['bias'] = X_fake_test_bias\n",
        "X_fake_test['media'] = X_fake_test_media"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI940K_ynH8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "X_fake_train['bias label'] = le.fit_transform(X_fake_train['bias'])\n",
        "X_fake_train['media label'] = le.fit_transform(X_fake_train['media'])\n",
        "X_fake_test['bias label'] = le.fit_transform(X_fake_test['bias'])\n",
        "X_fake_test['media label'] = le.fit_transform(X_fake_test['media'])\n",
        "\n",
        "X_fake_train = X_fake_train.drop(['bias', 'media'] ,axis=1)\n",
        "X_fake_test = X_fake_test.drop(['bias', 'media'] ,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DxDkeq5KpZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "bias_df_scale = StandardScaler().fit_transform(X_fake_train)\n",
        "X_fake_train = pd.DataFrame(bias_df_scale , index=X_fake_train.index, columns=X_fake_train.columns)\n",
        "bias_df_scale = StandardScaler().fit_transform(X_fake_test)\n",
        "X_fake_test = pd.DataFrame(bias_df_scale , index=X_fake_test.index, columns=X_fake_test.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z6yHb4-4g9x",
        "colab_type": "text"
      },
      "source": [
        "#####__XGBoost Feature Importance and Feature Selection__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELqVDHuJFS17",
        "colab_type": "code",
        "outputId": "6b5f20cf-fa59-4757-9b81-a7abbb1a21e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xgb_model = XGBClassifier(max_depth=4)\n",
        "xgb_model.fit(X_fake_train, y_fake_train)\n",
        "with open('/content/drive/My Drive/junteng_dev/data_set/liar_xgboost.pkl', 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "pred_xgboost = xgb_model.predict(X_fake_test)\n",
        "score = metrics.accuracy_score(y_fake_test, pred_xgboost)\n",
        "pre_score = metrics.precision_score(y_fake_test, pred_xgboost, average='weighted')\n",
        "f1 = metrics.f1_score(y_fake_test, pred_xgboost, average='weighted') \n",
        "print(\"XGBoot Model Trained - accuracy: {:.4f}   precision: {:.4f}   F1 Score: {:.4f}\".format(score, pre_score, f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoot Model Trained - accuracy: 0.2115   precision: 0.1726   F1 Score: 0.1805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfD-HTgFFBQz",
        "colab_type": "text"
      },
      "source": [
        "Confustion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk4oLTw1IB1U",
        "colab_type": "code",
        "outputId": "f820e303-ea69-486e-cc6f-1b6fec685438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "pd.crosstab(y_fake_test, pred_xgboost, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Predicted</th>\n",
              "      <th>barely-true</th>\n",
              "      <th>false</th>\n",
              "      <th>half-true</th>\n",
              "      <th>mostly-true</th>\n",
              "      <th>pants-fire</th>\n",
              "      <th>true</th>\n",
              "      <th>All</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>barely-true</th>\n",
              "      <td>4</td>\n",
              "      <td>61</td>\n",
              "      <td>75</td>\n",
              "      <td>49</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>false</th>\n",
              "      <td>9</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>56</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>half-true</th>\n",
              "      <td>11</td>\n",
              "      <td>57</td>\n",
              "      <td>106</td>\n",
              "      <td>68</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mostly-true</th>\n",
              "      <td>9</td>\n",
              "      <td>59</td>\n",
              "      <td>81</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pants-fire</th>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true</th>\n",
              "      <td>12</td>\n",
              "      <td>55</td>\n",
              "      <td>75</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>All</th>\n",
              "      <td>49</td>\n",
              "      <td>326</td>\n",
              "      <td>451</td>\n",
              "      <td>322</td>\n",
              "      <td>10</td>\n",
              "      <td>109</td>\n",
              "      <td>1267</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Predicted    barely-true  false  half-true  mostly-true  pants-fire  true   All\n",
              "True                                                                           \n",
              "barely-true            4     61         75           49           3    20   212\n",
              "false                  9     69         89           56           3    23   249\n",
              "half-true             11     57        106           68           2    21   265\n",
              "mostly-true            9     59         81           72           2    18   241\n",
              "pants-fire             4     25         25           28           0    10    92\n",
              "true                  12     55         75           49           0    17   208\n",
              "All                   49    326        451          322          10   109  1267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDN8UlD7r92",
        "colab_type": "text"
      },
      "source": [
        "${normalized\\ accuracy} = \\frac {True Positive} {(True Positive + False Negative)} + \\frac {True Negative} {(True Negative + False Positive)} - 1$\n",
        "\n",
        "\n",
        "Most Accurate Predition is Half-True, we use it to calculate the weight: \n",
        "\n",
        "$\\frac {106} {(106 + 159 )} + \\frac {657} {(657 + 345 )} - 1 = 0.05$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPwjVxsOH-23",
        "colab_type": "code",
        "outputId": "a18c8689-1494-4980-91b6-bb181a672c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "for i in range(len(X_fake_train.columns)):\n",
        "    print('{}: {} - {:.6f}'.format(i, X_fake_train.columns[i], xgb_model.feature_importances_[i]))\n",
        "pyplot.bar(range(len(xgb_model.feature_importances_)), xgb_model.feature_importances_)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: fk_gl - 0.078082\n",
            "1: vader_sentiment - 0.087821\n",
            "2: neg_persp - 0.084985\n",
            "3: certainty - 0.099399\n",
            "4: presup_cnt - 0.075793\n",
            "5: doubt_cnt - 0.082891\n",
            "6: partisan_cnt - 0.071537\n",
            "7: value_cnt - 0.080153\n",
            "8: figurative_cnt - 0.124440\n",
            "9: attribution_cnt - 0.069431\n",
            "10: self_refer_cnt - 0.075040\n",
            "11: bias label - 0.070427\n",
            "12: media label - 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhrFX1lg4jyM",
        "colab_type": "text"
      },
      "source": [
        "####__Classification Metrics__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJdkJd8rGTVF",
        "colab_type": "text"
      },
      "source": [
        "|Classificaion| Accuracy Score| Precision Score | F1 Score |\n",
        "|---|---|---|---|\n",
        "|XGBoost|         0.2147 | 0.2245   | 0.1841"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdi6-tFzLn1S",
        "colab_type": "text"
      },
      "source": [
        "__Accuracy Score__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp-A14qTtmsb",
        "colab_type": "text"
      },
      "source": [
        "|Classificaion                 |  XGBoost\n",
        "|---|---\n",
        "|Bias Fake News Dataset        |  0.6706  \n",
        "|Political Social Media Dataset|  0.6269  \n",
        "|Liar Liar Dataset             |  0.2186  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSV50Xj4Ls6H",
        "colab_type": "text"
      },
      "source": [
        "__Precision Score__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy-pnwUsL_fu",
        "colab_type": "text"
      },
      "source": [
        "|Classificaion                 |  XGBoost | \n",
        "|---|---|\n",
        "|Bias Fake News Dataset        |  0.6551  | \n",
        "|Political Social Media Dataset|  0.6270  |\n",
        "|Liar Liar Dataset             |  0.2186  | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfz0XCprLxeK",
        "colab_type": "text"
      },
      "source": [
        "__F1 Score__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkIOTx_sMHhI",
        "colab_type": "text"
      },
      "source": [
        "|Classificaion                 |  XGBoost |\n",
        "|---|---|\n",
        "|Bias Fake News Dataset        |  0.6593  | \n",
        "|Political Social Media Dataset|  0.6197  |\n",
        "|Liar Liar Dataset             |  0.2186  | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etNupgHLs00z",
        "colab_type": "text"
      },
      "source": [
        "###__Feature Class__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSrWo7O0hi03",
        "colab_type": "code",
        "outputId": "f7e49b33-af59-4103-c83f-8a544c5d52df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "\n",
        "sentiment_analyzer = SentimentAnalysis()\n",
        "class BiasAnalysis():\n",
        "    def __init__(self):\n",
        "        with open('/content/drive/My Drive/junteng_dev/data_set/liar_xgboost.pkl', 'rb') as f:\n",
        "            self.fake_news_model = pickle.load(f)\n",
        "        with open('/content/drive/My Drive/junteng_dev/data_set/bias_xgboost.pkl', 'rb') as f:\n",
        "            self.bias_fake_xgboost = pickle.load(f)\n",
        "        with open('/content/drive/My Drive/junteng_dev/data_set/media_xgboost.pkl', 'rb') as f:\n",
        "            self.media_fake_xgboost = pickle.load(f)\n",
        "        self.le = preprocessing.LabelEncoder()\n",
        "\n",
        "    def predict(self, text):\n",
        "        df = pd.DataFrame({\n",
        "            \"fk_gl\": [flesch_kincaid_grade(text)], \n",
        "            \"vader_sentiment\": [sentiment_analyzer.predict(text)],\n",
        "            \"neg_persp\": [check_neg_persp(text)],\n",
        "            \"certainty\": [check_certainty(text)],\n",
        "            \"presup_cnt\": [count_feature_freq(presup, text)],\n",
        "            \"doubt_cnt\": [count_feature_freq(doubt, text)],\n",
        "            \"partisan_cnt\": [count_feature_freq(partisan, text)],\n",
        "            \"value_cnt\": [count_feature_freq(value_laden, text)],\n",
        "            \"figurative_cnt\": [count_feature_freq(figurative, text)],\n",
        "            \"attribution_cnt\": [count_feature_freq(attribution, text)],\n",
        "            \"self_refer_cnt\": [count_feature_freq(self_refer, text)],\n",
        "            }, index=[0])\n",
        "        bias_label = self.bias_fake_xgboost.predict(df)\n",
        "        media_label = self.media_fake_xgboost.predict(df)\n",
        "        df['bias label'] = bias_label \n",
        "        df['media label'] = media_label\n",
        "        df['bias label'] = self.le.fit_transform(df['bias label'])\n",
        "        df['media label'] = self.le.fit_transform(df['media label'])\n",
        "\n",
        "        result = self.fake_news_model.predict(df)[0]\n",
        "        result_proba = self.fake_news_model.predict_proba(df)[:,1][0]\n",
        "        return result, result_proba"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment Analysis Model Trained - accuracy:   0.943918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_-D3hpFjL5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ba = BiasAnalysis()\n",
        "liar_df = pd.read_csv(\"/content/drive/My Drive/junteng_dev/data_set/train_processed.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G32T5tcM1NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "def SEERs_getBiasSCore(text): \n",
        "    bias_df = pd.DataFrame({\n",
        "                    \"fk_gl\": [flesch_kincaid_grade(text)], \n",
        "                    \"vader_sentiment\": [sentiment_analyzer.predict(text)],\n",
        "                    \"neg_persp\": [check_neg_persp(text)],\n",
        "                    \"certainty\": [check_certainty(text)],\n",
        "                    \"presup_cnt\": [count_feature_freq(presup, text)],\n",
        "                    \"doubt_cnt\": [count_feature_freq(doubt, text)],\n",
        "                    \"partisan_cnt\": [count_feature_freq(partisan, text)],\n",
        "                    \"value_cnt\": [count_feature_freq(value_laden, text)],\n",
        "                    \"figurative_cnt\": [count_feature_freq(figurative, text)],\n",
        "                    \"attribution_cnt\": [count_feature_freq(attribution, text)],\n",
        "                    \"self_refer_cnt\": [count_feature_freq(self_refer, text)],\n",
        "                    'bias label': 1, \n",
        "                    'media label': 0\n",
        "                    }, index=[0])\n",
        "    with open('/content/drive/My Drive/junteng_dev/data_set/liar_xgboost.pkl', 'rb') as f:\n",
        "        fake_news_model = pickle.load(f)\n",
        "    result = fake_news_model.predict(bias_df)\n",
        "    result_proba = fake_news_model.predict_proba(bias_df)[:,1][0]\n",
        "    return result_proba"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw44sHmJoC_i",
        "colab_type": "code",
        "outputId": "90080c8d-6e92-492f-8049-da91e1059904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = liar_df.sample()\n",
        "\n",
        "SEERs_getBiasSCore(text['clean'].values[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07498383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLxfvNxQ4r3e",
        "colab_type": "text"
      },
      "source": [
        "# FINAL COMBINED MODEL\n",
        "\n",
        "###  <span style=\"color:red\"> ITEARTION 2: Need to add all 11 features and make a polynomial equation and do performance analysis on it. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4ZA-Jq44r3h",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial Equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ4dryPE4r3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def getRelevance(text, headline=\"\", numAuthors = 0, source = \"\", party =\"\"):\n",
        "    accur = [0.84, 0.56, 0.95, 0.35,  0.1 ,0.54, 0.98, 0.71, 0.6, 1, 0.05] # using the (normalized) accuracy as weigths\n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    prob.append(w[0] * DATAMINERS_getAuthorScore(numAuthors))\n",
        "    sumW += w[0]\n",
        "    if ( (headline != \"\") & (party != \"\")):\n",
        "        prob.append(w[1] * DATAMINERS_getPartyAffiliationScore(headline, party))\n",
        "        sumW += w[1]\n",
        "    if (headline != \"\"):\n",
        "        prob.append(w[2] * DATAMINERS_getClickbaitScore(headline))\n",
        "        sumW += w[2]\n",
        "    if (headline != \"\"):\n",
        "        prob.append(w[3] * DATAMINERS_getSentimentAnalysisScore(text))\n",
        "        sumW += w[3]\n",
        "    if (headline != \"\"):\n",
        "        prob.append(w[4] * DATAMINERS_getLDATopicModellingScore(headline))\n",
        "        sumW += w[4]\n",
        "    if (headline != \"\"):\n",
        "        prob.append(w[5] * DATAMINERS_getSensationalismScore(headline))\n",
        "        sumW += w[5]\n",
        "    if (headline != \"\"):\n",
        "        prob.append(w[6] * DATAMINERS_getSpamScore(headline))\n",
        "        sumW += w[6]\n",
        "    prob.append(w[7] * DATAMINERS_getBodyLengthScore(len(text)))\n",
        "    sumW += w[7]\n",
        "    prob.append(w[8] * DATAMINERS_getWordFrequencyScore(text))\n",
        "    sumW += w[8]\n",
        "    if (party != \"\"):\n",
        "        prob.append(w[9] * DATAMINERS_getSourceReputationScore(source))\n",
        "        sumW += w[9]\n",
        "\n",
        "    #Bias\n",
        "    prob.append(w[10] * SEERs_getBiasSCore(text))\n",
        "    sumW += w[10]\n",
        "\n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    return probTotal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVZK1535j6Sb",
        "colab_type": "text"
      },
      "source": [
        "###Google Search API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyyy4kKMflPX",
        "colab_type": "code",
        "outputId": "7af029fd-52ec-4ebd-d73c-e54732fbe5f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install google-api-python-client"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/f8/810ec35c31cca89bc4f1a02c14b042b9ec6c19dd21f7ef1876874ef069a6/tweet-preprocessor-0.5.0.tar.gz\n",
            "Building wheels for collected packages: tweet-preprocessor\n",
            "  Building wheel for tweet-preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.5.0-cp36-none-any.whl size=7947 sha256=30c0da8dde818c7273ece1a8994de8a80abc074eaa1d480fd87ce38871d2af76\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/27/cc/49938e98a2470802ebdefae9d2b3f524768e970c1ebbe2dc4a\n",
            "Successfully built tweet-preprocessor\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.5.0\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.12)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.7.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.17.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (46.1.3)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npru1rvOkNWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "import json\n",
        "from googleapiclient.discovery import build"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7DT4NL6kLBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GoogleSearch(object):\n",
        "    def __init__(self, api_key, cse_id):\n",
        "        self.__api_key = api_key\n",
        "        self.__cse_id = cse_id\n",
        "        self.service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    \n",
        "    def search(self, search_term, **kwargs):\n",
        "        self.__data = self.service.cse().list(q=search_term, cx=self.__cse_id, **kwargs).execute()\n",
        "    \n",
        "    def get_results(self):\n",
        "        return self.__data\n",
        "    \n",
        "    def get_search_url(self):\n",
        "        url_list = list()\n",
        "        if \"items\" in self.__data:\n",
        "            for item in self.__data['items']:\n",
        "                url_list.append(item['link'])\n",
        "        return url_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxzpLBiYJH9b",
        "colab_type": "text"
      },
      "source": [
        "#Dynomodb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnqgUpQ9CSMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AWS_ACCESS_KEY_ID = \" \"\n",
        "AWS_SECRET_ACCESS_KEY = \" \"\n",
        "AWS_REGION = 'us-east-1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYFaA3-cIeLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import uuid\n",
        "import boto3\n",
        "from datetime import datetime\n",
        "from boto3.dynamodb.conditions import Key, Attr\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "class DynamoDB(object):\n",
        "    def __init__(self, api_key, secret_key, table_name, aws_region='us-east-1'):\n",
        "        try:\n",
        "            self.__session = boto3.Session(\n",
        "                aws_access_key_id = api_key,\n",
        "                aws_secret_access_key = secret_key,\n",
        "                region_name = aws_region,\n",
        "            )\n",
        "            self.__dynamodb = self.__session.resource('dynamodb')\n",
        "            self.__table = self.__dynamodb.Table(table_name)\n",
        "        except ClientError as e:\n",
        "            print(e.response['Error']['Message'])\n",
        "\n",
        "    def _convert_timestamp(self, response_data):\n",
        "        for item in response_data:\n",
        "            time_str = datetime.fromtimestamp(item['timestamp']).strftime('%Y-%m-%d')\n",
        "            item['timestamp'] = time_str\n",
        "        return response_data\n",
        "\n",
        "    def _convert_timestring(self, timestring):\n",
        "        for format in ['%Y-%m-%d', '%Y/%m/%d', '%Y%m%d', '%Y %m %d']:\n",
        "            try:\n",
        "                timestamp = datetime.strptime(timestring, format).strftime('%s')\n",
        "                return int(timestamp)\n",
        "            except ValueError as e:\n",
        "                pass\n",
        "\n",
        "    def insert(self, **kwargs):\n",
        "        item_id = uuid.uuid4().hex\n",
        "        if 'item_id' not in kwargs.keys():\n",
        "            kwargs['item_id'] = item_id\n",
        "        if 'timestamp' not in kwargs.keys():\n",
        "            kwargs['timestamp'] = 0\n",
        "        else:\n",
        "            kwargs['timestamp'] = self._convert_timestring(kwargs['timestamp'])\n",
        "        try:\n",
        "            response = self.__table.put_item(\n",
        "                Item=kwargs\n",
        "            )\n",
        "            return response['ResponseMetadata']\n",
        "        except ClientError as e:\n",
        "            return e.response['Error']['Message']\n",
        "\n",
        "    def search_all(self):\n",
        "        try:\n",
        "            response = self.__table.scan()\n",
        "            data = response['Items']\n",
        "            while 'LastEvaluatedKey' in response:\n",
        "                response = self.__table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
        "                data.extend(response['Items'])\n",
        "            return data\n",
        "        except ClientError as e:\n",
        "            return e.response['Error']['Message']\n",
        "\n",
        "    def search_by_id(self, item_id):\n",
        "        try:\n",
        "            response = slef.__table.query(\n",
        "                KeyConditionExpression=Key('item_id').eq(item_id),\n",
        "                ScanIndexForward=True\n",
        "            )\n",
        "            return response['Items']\n",
        "        except ClientError as e:\n",
        "            return e.response['Error']['Message']            \n",
        "\n",
        "    def search_by_timestamp(self, timestamp):\n",
        "        timestamp = self._convert_timestring(timestamp)\n",
        "        try:\n",
        "            filtering_exp = Key('timestamp').eq(timestamp)\n",
        "            response = self.__table.scan(\n",
        "                FilterExpression=filtering_exp\n",
        "            )\n",
        "            return response['Items']\n",
        "        except ClientError as e:\n",
        "            return e.response['Error']['Message']\n",
        "\n",
        "    def search_by_timestamp_range(self, start=None, end=None):\n",
        "        if not start:\n",
        "            start = 0\n",
        "        else:\n",
        "            start = self._convert_timestring(start)\n",
        "        if not end:\n",
        "            end = int(datetime.now().strftime('%s'))\n",
        "        else:\n",
        "            end = self._convert_timestring(end)\n",
        "        try:\n",
        "            filtering_exp = Key('timestamp').between(start, end)\n",
        "            response = self.__table.scan(\n",
        "                FilterExpression=filtering_exp\n",
        "            )\n",
        "            return response['Items']\n",
        "        except ClientError as e:\n",
        "            return e.response['Error']['Message']\n",
        "\n",
        "    def delete_by_id(self, item_id):\n",
        "        records = self.search_by_id(item_id)\n",
        "        if not records:\n",
        "            return \"No Records Found\"\n",
        "        try:\n",
        "            response = self.__table.delete_item(\n",
        "                Key = {\n",
        "                    'item_id': records[0]['item_id'],\n",
        "                    'timestamp': records[0]['timestamp']\n",
        "                }\n",
        "            )\n",
        "            return response['ResponseMetadata']\n",
        "        except ClientError as e:\n",
        "            return e.response['Error']['Message']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYq7RUX_sODb",
        "colab_type": "text"
      },
      "source": [
        "# Article Parsing and Storing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzffsZxNkzJ9",
        "colab_type": "code",
        "outputId": "a2d9f725-b141-47ee-f044-bd8b45dd459a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install newspaper3k\n",
        "!pip uninstall feedparser -y\n",
        "!pip install feedparser"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages/feedparser-6.0.0b3-py3.6.egg (from newspaper3k) (6.0.0b3)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 7.2MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: feedfinder2, tinysegmenter, jieba3k, sgmllib3k\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=6ec517c32542fe1eecdda78c4c78ea736ab8823585a31d93d9fcdb6337c785e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=df7381969aa72315472b17d2e0845b4f197e44b42be9c58c098355b900f759e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=c797571ce623cc5238bf879559d5de9d4b5000e046c811c5a7f2d0a8863c9ee5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=ff63616f562326835db4381ed3b9dfd1383c41f60e425505b1e68ee9d556cabb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built feedfinder2 tinysegmenter jieba3k sgmllib3k\n",
            "Installing collected packages: feedfinder2, tinysegmenter, cssselect, requests-file, tldextract, jieba3k, newspaper3k, sgmllib3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-2.2.2\n",
            "Uninstalling feedparser-6.0.0b3:\n",
            "  Successfully uninstalled feedparser-6.0.0b3\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: feedparser\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=b57397f97a046d3cbab71d6a93aa497c65f65b8d52ee01cdecd18c1161d4cd57\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built feedparser\n",
            "\u001b[31mERROR: pattern 3.6.1 requires backports.csv, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: pattern 3.6.1 requires mysqlclient, which is not installed.\u001b[0m\n",
            "Installing collected packages: feedparser\n",
            "Successfully installed feedparser-5.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eibEh8Susg_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_term = \"new+york+covid19\"\n",
        "GOOGLE_API_KEY = \" \"\n",
        "GOOGLE_CSE_ID = \" \"\n",
        "\n",
        "google_search = GoogleSearch(GOOGLE_API_KEY, GOOGLE_CSE_ID)\n",
        "google_search.search(search_term)\n",
        "search_url = google_search.get_search_url()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQdW_23gl45T",
        "colab_type": "text"
      },
      "source": [
        "Extract Data From Articles URL and save into DynamoDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq0FbyglZMo4",
        "colab_type": "code",
        "outputId": "4b3e3f5d-6381-4b38-f904-54c409d63f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from newspaper import Article\n",
        "from datetime import datetime\n",
        "\n",
        "dynamodb_client = DynamoDB(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, 'timeline')\n",
        "\n",
        "for url in search_url:    \n",
        "    try: \n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        content_txt = article.text\n",
        "        content_title = article.title\n",
        "        num_authors = len(article.authors)\n",
        "        source_url = article.source_url\n",
        "        time_str = article.publish_date\n",
        "        if not time_str:\n",
        "            time_str = datetime.now().strftime('%Y-%m-%d')\n",
        "        else:\n",
        "            time_str = time_str.strftime('%Y-%m-%d')\n",
        "        relevance_sorce = str(getRelevance(content_txt, content_title, num_authors, source_url))\n",
        "        print(relevance_sorce)\n",
        "        article.nlp()\n",
        "        insert_item = {\n",
        "            \"abstract\": article.summary,\n",
        "            \"authors\": article.authors,\n",
        "            \"keywords\": article.keywords,\n",
        "            \"relevance\": relevance_sorce,\n",
        "            \"timestamp\": time_str,\n",
        "            \"title\": content_title\n",
        "            }\n",
        "        dynamodb_client.insert(**insert_item)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2591720589600871\n",
            "0.12961380866231548\n",
            "0.23715691996221452\n",
            "0.3524186206215495\n",
            "0.2091362874651128\n",
            "0.31573518388118765\n",
            "0.3176428533350044\n",
            "0.29258880244516583\n",
            "0.32169574752318014\n",
            "0.39562551172856597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JdryjpglKNY",
        "colab_type": "text"
      },
      "source": [
        "##Extract Everything from DynamoDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxqJ6Or-ltZP",
        "colab_type": "code",
        "outputId": "649c9e81-deb8-46fd-b2f9-75751e2d422e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dynamodb_client.search_all()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'abstract': 'Check the current stock market data, including prices and performance of the Dow Jones Industrial Average, S&P 500, Nasdaq and the Russell 2000.\\nXBut to make money in the stock market, you need to go beyond just the raw numbers.\\nCurrent Stock Market Data: Dow Jones, S&P 500, Nasdaq, Russell 2000How is the Dow Jones trading today?\\nCheck the charts below to get the current stock market data, updating throughout each trading day.\\nMarket Volatility, 10-Year Treasury Yield, Oil & Gold PricesWall Street pays close attention to market volatility and changes in the 10-year Treasury yield, as well as fluctuations in oil and gold prices.',\n",
              "  'authors': [\"Investor'S Business Daily\", 'Ibd Staff'],\n",
              "  'item_id': '9b1a7be867c94df0a81f2ab93be53bb9',\n",
              "  'keywords': ['track',\n",
              "   'stock',\n",
              "   'spdr',\n",
              "   'treasury',\n",
              "   'dow',\n",
              "   'sector',\n",
              "   'yield',\n",
              "   'nasdaq',\n",
              "   'data',\n",
              "   'jones',\n",
              "   'market',\n",
              "   'prices',\n",
              "   'etfs',\n",
              "   'ibd',\n",
              "   'volatility',\n",
              "   'stocks',\n",
              "   'sp'],\n",
              "  'relevance': '0.22105843674422776',\n",
              "  'timestamp': Decimal('1587513600'),\n",
              "  'title': 'Current Stock Market Data: See Prices For S&P 500, Nasdaq, Dow Jones, SPDR ETFs'},\n",
              " {'abstract': 'The Dow Jones Industrial Average dropped more than 600 points Tuesday, battered by plunging oil prices, in the latest bout of market turbulence sparked by the coronavirus pandemic.\\nMajor indexes opened sharply lower and continued tumbling in afternoon trading as the selloff in oil accelerated.\\nThe stock market’s losses were broad: 29 of the 30 stocks in the blue-chip index declined, as did all 11 sectors in the S&P 500.\\nBoth indexes suffered their first back-to-back sessions of losses since April 1.',\n",
              "  'authors': ['Caitlin Ostroff', 'Frances Yoon'],\n",
              "  'item_id': 'b41d0de025fa4f17a40e03483e2caa8a',\n",
              "  'keywords': ['stock',\n",
              "   'tumbling',\n",
              "   'turbulence',\n",
              "   'shudders',\n",
              "   'oil',\n",
              "   'sparked',\n",
              "   'market',\n",
              "   'losses',\n",
              "   'indexes',\n",
              "   'drop',\n",
              "   'trading',\n",
              "   'suffered',\n",
              "   'stocks'],\n",
              "  'relevance': '0.20888027177074572',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'U.S. Stocks Drop as Oil Market Shudders'},\n",
              " {'abstract': 'Billionaire investor Howard Marks said there is a sharp disconnect between the stock market and the reality the world is facing amid the coronavirus outbreak.\\nHowever, such rallies are characteristic of a bear market, Marks said.\\nMore than 2.4 million coronavirus cases have been confirmed globally, according to data from Johns Hopkins University.\\n\"People are traumatized, and not just because of the performance of their stocks,\" Marks said.\\nMarks is one of the most followed investors on Wall Street, with his memos to investors being widely read across the financial industry.',\n",
              "  'authors': ['Fred Imbert'],\n",
              "  'item_id': 'e55c3d0a578f442fb531d6f9919ac479',\n",
              "  'keywords': ['stock',\n",
              "   'marks',\n",
              "   'record',\n",
              "   'followed',\n",
              "   'reflecting',\n",
              "   'world',\n",
              "   'highs',\n",
              "   'took',\n",
              "   'widely',\n",
              "   'howard',\n",
              "   'data',\n",
              "   'market',\n",
              "   'reality',\n",
              "   'coronavirus',\n",
              "   'investor',\n",
              "   'cases',\n",
              "   'stocks',\n",
              "   'rebound'],\n",
              "  'relevance': '0.2630679373897162',\n",
              "  'timestamp': Decimal('1587340800'),\n",
              "  'title': 'Widely followed investor Howard Marks says the stock market rebound is not reflecting reality'},\n",
              " {'abstract': 'The United States Oil Fund, an exchange-traded security for retail investors that buys oil futures, recovered a bit on Wednesday, a day after sinking over 25% as the fund changed its structure to stave off a collapse.\\nKeeping a lid on the stock, earnings per share of $1.57 came in short of expectations.\\nAshley Stringer | CNBCThe House is set to vote Thursday on the $484 billion coronavirus relief package that unanimously passed the Senate on Tuesday evening.\\nThe measure injects another $310 billion into a key loan fund designed to keep employees on small company payrolls.\\nThe bill that passed Tuesday allocates $60 billion for small lenders and another $60 billion toward Small Business Administration disaster assistance loans and grants.',\n",
              "  'authors': ['Matthew J. Belvedere'],\n",
              "  'item_id': 'eaef9a7c69a246228ffb3c47ec75be66',\n",
              "  'keywords': ['house',\n",
              "   'stock',\n",
              "   'share',\n",
              "   'higher',\n",
              "   'know',\n",
              "   'fund',\n",
              "   'billion',\n",
              "   'oil',\n",
              "   'small',\n",
              "   'market',\n",
              "   'feb',\n",
              "   'things',\n",
              "   'coronavirus',\n",
              "   'revenue',\n",
              "   'opens'],\n",
              "  'relevance': '0.41223731860810636',\n",
              "  'timestamp': Decimal('1587513600'),\n",
              "  'title': '5 things to know before the stock market opens Wednesday'},\n",
              " {'abstract': '“This may be the tip of the iceberg”: Why Japan’s coronavirus crisis may be just beginningShare All sharing options for: “This may be the tip of the iceberg”: Why Japan’s coronavirus crisis may be just beginningOn Sunday afternoon in the Dai Nagoya Building in Nagoya, Japan’s industrial capital and one of the centers of the novel coronavirus outbreak in the country, Tully’s Coffee is shuttered.\\nOne arm of Japan’s coronavirus policy has been to build a firewall against the influx of cases from overseas.\\nThroughout the month of February, most of Japan’s cases were individuals connected to Wuhan, and the majority of cases were isolated and traced.\\nFor comparison, lockdowns began in Northern Italy on March 8, when more than 7,000 coronavirus cases had already been confirmed.\\nThese expansive and accessible health care options may be providing an additional safety net for Japan’s large elderly population.',\n",
              "  'authors': ['Eric Margolis'],\n",
              "  'item_id': '20c7ba9634da4e22bfdbadc79284d07a',\n",
              "  'keywords': ['japans',\n",
              "   'virus',\n",
              "   'health',\n",
              "   'infection',\n",
              "   'japan',\n",
              "   'care',\n",
              "   'crisis',\n",
              "   'beginning',\n",
              "   'case',\n",
              "   'iceberg',\n",
              "   'tip',\n",
              "   'coronavirus',\n",
              "   'cases',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.3096040293977318',\n",
              "  'timestamp': Decimal('1585353600'),\n",
              "  'title': '“This may be the tip of the iceberg”: Why Japan’s coronavirus crisis may be just beginning'},\n",
              " {'abstract': '“And with the price of oil so goes the fate of some energy companies,”The Energy Information Administration pegged total U.S. working storage capacity is at 653.4 million barrels.\\nNet stocks held at refineries and tank farms was at about 323.5 million barrels as of the week ended April 10, taking up 57% of storage capacity.\\nThe Cushing Complex is one of the world’s largest crude oil storage facilities.\\nEnbridge Inc. ENB, +0.73% , which is one of the largest operators at Cushing, has about 20 million barrels of total storage capacity at the Cushing crude-oil storage facility.\\nPrices for May WTI crude US:CLK20 lost 306% to settle at a negative $37.63 a barrel Monday, which implied that investors needed to pay buyers to take delivery of crude oil amid an excess of crude oil and lack of storage space.',\n",
              "  'authors': ['Myra P. Saefong'],\n",
              "  'item_id': 'a5c6b890fcb44acfbfcb47df7e481f70',\n",
              "  'keywords': ['capacity',\n",
              "   'demand',\n",
              "   'cuts',\n",
              "   'loom',\n",
              "   'oil',\n",
              "   'crude',\n",
              "   'running',\n",
              "   'production',\n",
              "   'storage',\n",
              "   'market',\n",
              "   'day',\n",
              "   'barrels',\n",
              "   'week',\n",
              "   'space',\n",
              "   'cushing',\n",
              "   'million'],\n",
              "  'relevance': '0.2231593812912429',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'The oil market is running out of storage space and production cuts loom'},\n",
              " {'abstract': 'We need to make up for this by changing the way we work, using remote working and moving businesses online, to keep the economy running.\\nCompanies are starting to realize that they can actually do remote work.\\nMoves by leading companies such as Panasonic and Unicharm to introduce remote working hit the headlines every day.\\nA new strain of Coronavirus, COVID 19, is spreading around the world, causing deaths and major disruption to the global economy.\\nCEPI is currently supporting the race to develop a vaccine against this strand of the coronavirus.',\n",
              "  'authors': ['Ryan Takeshita',\n",
              "   'Laila Kearney',\n",
              "   'Darrell Etherington',\n",
              "   'Christine Soares',\n",
              "   'Written By',\n",
              "   'Linda Lacina',\n",
              "   'Christopher',\n",
              "   'William James',\n",
              "   'Douglas Broom',\n",
              "   'Anna Bruce-Lockhart'],\n",
              "  'item_id': '0a6c5288e651437ea5846b2215517acb',\n",
              "  'keywords': ['covid19',\n",
              "   'view',\n",
              "   'doing',\n",
              "   'world',\n",
              "   'japan',\n",
              "   'work',\n",
              "   'remote',\n",
              "   'companies',\n",
              "   'working',\n",
              "   'lies',\n",
              "   'online',\n",
              "   'business',\n",
              "   'coronavirus',\n",
              "   'ahead',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.2667284020658608',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'What lies ahead after the coronavirus? A view from Japan'},\n",
              " {'abstract': 'Where that coronavirus text from “a friend in the military” really comes from',\n",
              "  'authors': [],\n",
              "  'item_id': '13764098eda840fdaaf0d657c31ec2f4',\n",
              "  'keywords': ['level',\n",
              "   '1940',\n",
              "   'friend',\n",
              "   'real',\n",
              "   'hit',\n",
              "   'rate',\n",
              "   'unemployment',\n",
              "   'text',\n",
              "   'military',\n",
              "   'really',\n",
              "   '147',\n",
              "   'coronavirus',\n",
              "   'likely',\n",
              "   'highest',\n",
              "   'comes'],\n",
              "  'relevance': '0.32778733260412446',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'U.S. unemployment rate: Real unemployment likely hit 14.7%, the highest level since 1940'},\n",
              " {'abstract': \"Bruce Aylward of the World Health Organization holds up a graphic showing China's plummeting coronavirus cases at a 24 February press conference in Beijing.\\nEven some on the team, organized jointly by the World Health Organization (WHO) and the Chinese government, say they were surprised.\\n“As a consequence of all of these measures, public life is very reduced,” the report notes.\\n“The one thing that's completely glossed over is the whole human rights dimension,” says Devi Sridhar, a global public health specialist at the University of Edinburgh.\\nReducing the peak number of cases buys a health system time to deal with later ones, public health experts say.\",\n",
              "  'authors': ['Kai Kupferschmidt', 'Jon Cohen'],\n",
              "  'item_id': '14624f5f282f4d169b4ad0999db5e1f4',\n",
              "  'keywords': ['covid19',\n",
              "   'health',\n",
              "   'world',\n",
              "   'country',\n",
              "   'work',\n",
              "   'chinas',\n",
              "   'strategy',\n",
              "   'china',\n",
              "   'measures',\n",
              "   'disease',\n",
              "   'report',\n",
              "   'cases',\n",
              "   'public'],\n",
              "  'relevance': '0.1862864065615966',\n",
              "  'timestamp': Decimal('1583452800'),\n",
              "  'title': \"Can China's COVID-19 strategy work elsewhere?\"},\n",
              " {'abstract': \"China Reports Its First Day With No New COVID-19 DeathsEnlarge this image toggle caption Yves Dean/Getty Images Yves Dean/Getty ImagesChina on Tuesday reported no deaths from COVID-19 for the first time since it began publishing data about the outbreak more than two months ago.\\nThe milestone comes a day before the government is set to lift outbound travel restrictions on people in Wuhan, the country's hardest-hit city.\\nIn recent days, most new cases in China have been imported from abroad, including 32 on Monday.\\nDeaths in China from the virus have been in the single digits since March 19, according to the National Health Commission.\\nIt said that as of midnight April 6, China had detected a total of 81,740 cases, with 3,331 deaths.\",\n",
              "  'authors': ['John Ruwitch'],\n",
              "  'item_id': '227a6fa44a6243abb327b6ea7067c472',\n",
              "  'keywords': ['covid19',\n",
              "   'reports',\n",
              "   'virus',\n",
              "   'deaths',\n",
              "   'yves',\n",
              "   'deangetty',\n",
              "   'day',\n",
              "   'wuhan',\n",
              "   'china',\n",
              "   'detected',\n",
              "   'cases',\n",
              "   'huangshan'],\n",
              "  'relevance': '0.25127865765231217',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'China Reports Its First Day With No New COVID-19 Deaths'},\n",
              " {'abstract': '“Harvard’s going to pay back the money,” President Trump said at a news conference on Tuesday.\\nHarvard, which has a $40 billion endowment, received $8 million in loan money.\\nShake Shack said this week that it would return its $10 million loan after a public uproar.',\n",
              "  'authors': [],\n",
              "  'item_id': '2394ba436f0e4f65a93e06e520e78bbc',\n",
              "  'keywords': ['loan',\n",
              "   'money',\n",
              "   'shake',\n",
              "   'producing',\n",
              "   'trump',\n",
              "   'oil',\n",
              "   'orders',\n",
              "   'chevron',\n",
              "   'venezuela',\n",
              "   'stop',\n",
              "   'uproar',\n",
              "   'tuesdayharvard',\n",
              "   'return',\n",
              "   'week',\n",
              "   'million',\n",
              "   'shack'],\n",
              "  'relevance': '0.326397447735013',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'U.S. Orders Chevron to Stop Producing Oil in Venezuela'},\n",
              " {'abstract': 'This is an indicator that the outbreak could now spread much more quickly in the U.S.\\n“It’s possible this could be an instance of community spread of COVID-19,” the CDC said.\\nIn California, there are at least 11 confirmed cases of COVID-19, including some repatriated citizens who have been quarantined at Marine Corps Air Station Miramar in San Diego.\\nThe decision was made because of the spread of the coronavirus.\\nMore new cases of coronavirus confirmed outside of China for the first time: WHO',\n",
              "  'authors': ['Jaimy Lee'],\n",
              "  'item_id': '480932483c45473da124702d9292d384',\n",
              "  'keywords': ['covid19',\n",
              "   'deaths',\n",
              "   'trump',\n",
              "   'china',\n",
              "   'response',\n",
              "   'scrutiny',\n",
              "   'update',\n",
              "   'possible',\n",
              "   'impact',\n",
              "   'outbreak',\n",
              "   'quarter',\n",
              "   'spread',\n",
              "   'including',\n",
              "   'coronavirus',\n",
              "   'cases',\n",
              "   'case'],\n",
              "  'relevance': '0.2632079284574854',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus update: 82,550 cases, 2,810 deaths, 1 possible case of spread in the U.S., Trump administration response under scrutiny'},\n",
              " {'abstract': 'New York Has Lost Another 783 Lives To Coronavirus, Cuomo SaysEnlarge this image toggle caption John Minchillo/AP John Minchillo/APNew York is flattening the curve, but the state still lost 783 lives over the last 24 hours, Gov.\\nIn total, as of early Saturday, the coronavirus has taken more than 8,600 lives in New York, Cuomo said.\\nReopening is both a public health question and an economic question, Cuomo said.\\n\"You can\\'t ask the people of this state, or this country, to choose between lives lost and dollars gained,\" he said.\\nCuomo said New York was recruiting attorneys state-wide to do pro-bono legal assistance on housing issues and Small Business Administration loan applications.',\n",
              "  'authors': ['Matthew S. Schwartz'],\n",
              "  'item_id': '21df21a3ec5f49f180c162721d2d98bd',\n",
              "  'keywords': ['york',\n",
              "   'stay',\n",
              "   'weeks',\n",
              "   'state',\n",
              "   'ask',\n",
              "   'lost',\n",
              "   '783',\n",
              "   'question',\n",
              "   'coronavirus',\n",
              "   'lives',\n",
              "   'cuomo',\n",
              "   'end'],\n",
              "  'relevance': '0.23715691996221452',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'New York Has Lost Another 783 Lives To Coronavirus, Cuomo Says'},\n",
              " {'abstract': 'Published on Apr 6, 2020Subscribe to our YouTube channel for free here:https://sc.mp/subscribe-youtubeJapan’s Prime Minister Shinzo Abe said the country would declare a state of emergency on April 7, 2020 in a bit to stop the coronavirus.\\nThe declaration, expected to last for a month, will apply in Tokyo and six other prefectures.\\nFull story: https://sc.mp/41474Follow us on:Website: https://scmp.comFacebook: https://facebook.com/scmpTwitter: https://twitter.com/scmpnewsInstagram: https://instagram.com/scmpnewsLinkedin: https://www.linkedin.com/company/sout...',\n",
              "  'authors': [],\n",
              "  'item_id': 'a7bbd380264647c081fb6bd259a33771',\n",
              "  'keywords': ['prefecturesfull',\n",
              "   'covid19',\n",
              "   'youtube',\n",
              "   'prime',\n",
              "   'state',\n",
              "   'shinzo',\n",
              "   'japan',\n",
              "   'emergency',\n",
              "   'declare',\n",
              "   'onwebsite',\n",
              "   'stop',\n",
              "   'published',\n",
              "   'month',\n",
              "   'pandemic',\n",
              "   'coronavirus',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.3222291928898489',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus: Japan to declare state of emergency over Covid-19 pandemic'},\n",
              " {'abstract': 'As the COVID-19 pandemic spreads and the numbers of infected and deaths skyrocket in various locations around the world, an ongoing puzzle has been the comparatively slow rise of those numbers in Japan.\\nThus some people being treated for what is likely COVID-19 won’t be counted as confirmed cases in the statistics.\\nSimilarly, a popular blog post on Japan’s COVID-19 strategy mentions the washing of hands and wearing of masks, as well as a lack of handshakes and hugs as possible reasons for Japan’s outlier status.\\nWhile masks may indeed be useful, and Japan’s general cleanliness is certainly something to be admired, it’s still hard to know whether these alone are responsible for Japan’s low COVID-19 numbers.\\nCultural factors working against testingA different way that cultural factors might be at work in relation to Japan’s low COVID-19 numbers is how they might cause Japanese people to avoid getting tested.',\n",
              "  'authors': ['Rochelle Kopp'],\n",
              "  'item_id': '817d061158864611a3637e9f7dab17c3',\n",
              "  'keywords': ['japans',\n",
              "   'masks',\n",
              "   'covid19',\n",
              "   'numbers',\n",
              "   'low',\n",
              "   'health',\n",
              "   'japanese',\n",
              "   'japan',\n",
              "   'explain',\n",
              "   'does',\n",
              "   'culture',\n",
              "   'infected'],\n",
              "  'relevance': '0.2540305552168215',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Does Japan’s culture explain its low COVID-19 numbers?'},\n",
              " {'abstract': 'LETIAdvertisement California COVID-19 testing backlog cut by two-thirds Share Shares Copy Link CopyCalifornia has cut its COVID-19 testing backlog by more than two-thirds, Gov.\\nI have a responsibility as your governor to do better,” the governor said during his daily press briefing.California has tested 126,000 people.\\nWe cannot allow people to start congregating again.”That appears to include major sports leagues such as the NFL, which has three teams in California.\\nI would like to thank the California State Association of Counties and the California Association of County Treasurers and Tax Collectors for committing to providing economic relief for residents and small businesses facing hardships due to COVID-19,” Newsom said.There have been about 12,700 confirmed cases of COVID-19 virus infections in California and about 280 deaths, according to data compiled by Johns Hopkins University.\\nYou won’t put them in a congregant setting to put their lives at risk.”___Nguyen reported from San Francisco.',\n",
              "  'authors': ['Adam Beam', 'Daisy Nguyen'],\n",
              "  'item_id': '034354de57d740b8b1b5344f7c8d43ec',\n",
              "  'keywords': ['backlog',\n",
              "   'covid19',\n",
              "   'virus',\n",
              "   'california',\n",
              "   'newsom',\n",
              "   'health',\n",
              "   'reported',\n",
              "   'state',\n",
              "   'cut',\n",
              "   'testing',\n",
              "   'twothirds',\n",
              "   'day',\n",
              "   'test'],\n",
              "  'relevance': '0.1691856992977166',\n",
              "  'timestamp': Decimal('1586044800'),\n",
              "  'title': 'California COVID-19 testing backlog cut by two-thirds'},\n",
              " {'abstract': 'Andrew Cuomo dismissed the possibility of an imminent shelter in place order Tuesday night after New York City Mayor Bill de Blasio urged New Yorkers to prepare for the measure.\\nI don\\'t think shelter in place really works for one locality,\" Cuomo, a Democrat, told CNN\\'s Jake Tapper.\\n\"I\\'m a New York City boy, born and raised if you can\\'t tell, and we\\'re very good at getting around the rules.\\nYou say shelter in place if you stay in New York City, I\\'ll go stay with my sister in Westchester, right?\\nEarlier Tuesday, de Blasio urged New Yorkers to prepare for the possibility that there could be a shelter in place order within the next two days.',\n",
              "  'authors': ['Chandelis Duster', 'Paul Leblanc'],\n",
              "  'item_id': '35d608d895154885b16a06400aa57137',\n",
              "  'keywords': ['york',\n",
              "   'city',\n",
              "   'state',\n",
              "   'blasio',\n",
              "   'policies',\n",
              "   'governor',\n",
              "   'think',\n",
              "   'mayor',\n",
              "   'urged',\n",
              "   'possibility',\n",
              "   'prepare',\n",
              "   'shelter',\n",
              "   'yorkers',\n",
              "   'cuomo',\n",
              "   'place',\n",
              "   'order'],\n",
              "  'relevance': '0.18944949543578413',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'New York governor dismisses possibility of shelter in place order after mayor urged New Yorkers to prepare for it'},\n",
              " {'abstract': 'There’s one country in the world that currently has the most knowledge of and experience with Covid-19: China.\\n[The lockdown] was concentrated in Wuhan and two or three other cities that also exploded [with Covid-19 cases].\\nThe faster you can find the cases, isolate the cases, and track their close contacts, the more successful you’re going to be.\\nThen enough public health infrastructure to investigate cases, identify the close contacts, and then make sure they remain under surveillance.\\nCorrection 3/2: A graphic in this piece previously misstated the percentage of cases in China that were not linked back to the Wuhan area.',\n",
              "  'authors': ['Julia Belluz', 'Mar'],\n",
              "  'item_id': 'e362bd71e4c048d884bb8f3dc8a2cfa7',\n",
              "  'keywords': ['sure',\n",
              "   'covid19',\n",
              "   'virus',\n",
              "   'know',\n",
              "   'lot',\n",
              "   'explains',\n",
              "   'world',\n",
              "   'expert',\n",
              "   'system',\n",
              "   'finally',\n",
              "   'big',\n",
              "   'chinas',\n",
              "   'china',\n",
              "   'disease',\n",
              "   'cases',\n",
              "   'declining'],\n",
              "  'relevance': '0.2996247859351919',\n",
              "  'timestamp': Decimal('1583107200'),\n",
              "  'title': 'China’s cases of Covid-19 are finally declining. A WHO expert explains why.'},\n",
              " {'abstract': 'But the United States is lagging on that front, in part due to faulty test kits and strict regulations, experts told Live Science.\\n\"We\\'re not remotely prepared,\" Dr. Alex Greninger, an assistant professor in the Department of Laboratory Medicine and an assistant director of the Clinical Virology Laboratory at the University of Washington Medical Center, told Live Science.\\nBut in order to diagnose SARS-CoV-2, scientists have to obtain, verify and run the current CDC test.\\n\"If the criteria is only to test people who have severe illness, we\\'re going to be missing other cases,\" Adalja told Live Science.\\nThe U.S. dropped the ball in getting tests out quickly and making them widely available, the experts told Live Science.',\n",
              "  'authors': ['Laura Geggel'],\n",
              "  'item_id': '2a5ac88a79854554a346e5d5baea7855',\n",
              "  'keywords': ['cases',\n",
              "   'remotely',\n",
              "   'live',\n",
              "   'virus',\n",
              "   'say',\n",
              "   'schaffner',\n",
              "   'isnt',\n",
              "   'testing',\n",
              "   'experts',\n",
              "   'prepared',\n",
              "   'science',\n",
              "   'kits',\n",
              "   'coronavirus',\n",
              "   'told',\n",
              "   'test',\n",
              "   'cdc'],\n",
              "  'relevance': '0.23800298528568078',\n",
              "  'timestamp': Decimal('1582675200'),\n",
              "  'title': \"US isn't 'remotely prepared' to test for coronavirus, experts say\"},\n",
              " {'abstract': 'YouTube has switched to standard definition streaming by default in Europe.\\nA YouTube spokesperson confirmed the switch, sending us this statement:People are coming to YouTube to find authoritative news, learning content and make connections during these uncertain times.\\nYesterday Netflix announced it would default to SD streaming in the region for 30 days for the same reason.\\n(The company routinely makes traffic data available in the Google Traffic and Disruptions Transparency Report.)\\nYouTube will be rolling out a campaign rolling across Europe that encourages people to follow health authorities’ guidance and stay home, she added.',\n",
              "  'authors': [],\n",
              "  'item_id': '008d92ed83ec4360aa5eedc0fa699fa2',\n",
              "  'keywords': ['covid19',\n",
              "   'sd',\n",
              "   'google',\n",
              "   'youtube',\n",
              "   'health',\n",
              "   'europe',\n",
              "   'streaming',\n",
              "   'internet',\n",
              "   'goes',\n",
              "   'default',\n",
              "   'traffic',\n",
              "   'company',\n",
              "   'content',\n",
              "   'usage',\n",
              "   'techcrunch',\n",
              "   'platforms'],\n",
              "  'relevance': '0.2979200768095908',\n",
              "  'timestamp': Decimal('1584662400'),\n",
              "  'title': 'YouTube goes SD streaming by default in Europe due to COVID-19 – TechCrunch'},\n",
              " {'abstract': 'At the same time, there are no treatments on hand for COVID-19 as there are for the seasonal flu, experts added.\\nInitially, estimates on the death rate in China hovered around 2%, but a report published last week in the New England Journal of Medicine arrived at a death rate of 1.4%.\\nThe apparent death rate of the new coronavirus is more in line with pandemic flu strains, where a new mutation causes the flu virus to become more contagious and dangerous, experts explained.\\nFor example, the 1918 Spanish flu pandemic, caused by the first emergence of the H1N1 flu strain, had a death rate greater than 2.5%, according to the CDC.\\nOne other important difference: The new coronavirus appears to be more infectious than the seasonal flu, experts said.',\n",
              "  'authors': ['March', 'At A.M.'],\n",
              "  'item_id': '954342089835426db370adea6774023e',\n",
              "  'keywords': ['covid19',\n",
              "   'virus',\n",
              "   'flu',\n",
              "   'javaid',\n",
              "   'experts',\n",
              "   'rate',\n",
              "   'coronavirus',\n",
              "   'really',\n",
              "   'death',\n",
              "   'heres',\n",
              "   'infectious',\n",
              "   'comparison',\n",
              "   'seasonal'],\n",
              "  'relevance': '0.36666429669748785',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"Is Coronavirus Really Like the Flu? Here's a Comparison\"},\n",
              " {'abstract': '\"Every day I come, what I see on a daily basis, is pain, despair, suffering and health care disparities.\"\\nTo separate the space from other wings of the hospital, health care workers hung plastic sheets from the walls, and used duct tape to prevent them from falling.\\n\"We need gowns, we need gloves we need masks we need more vents (ventilators),\" Mollette said.\\nHealth care workers have already begun preparing additional beds in anticipation of having even more patients in the coming weeks, Edwards said.\\n\"And certainly in this outbreak we are providing emotional support for each other, but it\\'s a very difficult time for health care.\"',\n",
              "  'authors': ['Miguel Marquez', 'Sonia Moghe'],\n",
              "  'item_id': 'bb1d5b1e0c9a4c0f9f518a85c6ff6f15',\n",
              "  'keywords': ['need',\n",
              "   'ventilators',\n",
              "   'covid19',\n",
              "   'family',\n",
              "   'hospitals',\n",
              "   'overwhelmed',\n",
              "   'brooklyn',\n",
              "   'health',\n",
              "   'care',\n",
              "   'hospital',\n",
              "   'patients',\n",
              "   'inside',\n",
              "   'deaths',\n",
              "   'workers',\n",
              "   'mollette'],\n",
              "  'relevance': '0.3524186206215495',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'Inside a Brooklyn hospital that is overwhelmed with Covid-19 patients and deaths'},\n",
              " {'abstract': \"Coming just a day after the global total of novel coronavirus cases passed 150,000, the Johns Hopkins online tracker shows 162,687 cases today, with 81,003 reported in China.\\nAs of today, New York has reported 729 cases, which includes 329 in New York City and 196 in Westchester County.\\nRegarding the latest confirmed cases, the Minnesota Department of Health today reported its first three community transmission cases and said the state's overall total is at 35.\\nFrance's total climbed to 5,400 cases, 120 of them fatal, Le Monde reported today.\\nElsewhere, Iran's health ministry today reported 1,209 cases, along with 133 more deaths, boosting its respective totals to 13,938 cases and 724 deaths.\",\n",
              "  'authors': [],\n",
              "  'item_id': '01099ac097ff42c7b9c5292f927716b7',\n",
              "  'keywords': ['york',\n",
              "   'covid19',\n",
              "   'reported',\n",
              "   'states',\n",
              "   'announced',\n",
              "   'europe',\n",
              "   'health',\n",
              "   'response',\n",
              "   'city',\n",
              "   'total',\n",
              "   'takes',\n",
              "   'federal',\n",
              "   'soar',\n",
              "   'steps',\n",
              "   'pandemic',\n",
              "   'today',\n",
              "   'cases',\n",
              "   'big'],\n",
              "  'relevance': '0.2867903287865613',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'US takes more big pandemic response steps; Europe COVID-19 cases soar'},\n",
              " {'abstract': \"Japan's sudden spike in coronavirus cases after Tokyo Olympics postponement raises eyebrows Japan reported more than 500 new positive cases for the first time Thursday.\\nTOKYO -- Japan reported more than 500 new positive cases of the novel coronavirus for the first time Thursday, the latest in a sudden spike in infections since the Tokyo Olympics were postponed till next year.\\nThere has been a threefold increase in deaths since the Tokyo Games were postponed.\\nThis picture shows the Olympic rings displayed outside the National Stadium, a venue for the Tokyo Games, in Tokyo, Japan, on April 7, 2020.\\nAbe on April 7 declared a monthlong state of emergency in Tokyo and six other prefectures over a spike in novel coronavirus cases.\",\n",
              "  'authors': ['Abc News',\n",
              "   'Anthony Trotter',\n",
              "   'Morgan Winsor',\n",
              "   'April',\n",
              "   'Min Read'],\n",
              "  'item_id': 'a72641f6407c44a3abf7edd496f582a5',\n",
              "  'keywords': ['japans',\n",
              "   'according',\n",
              "   'virus',\n",
              "   'olympics',\n",
              "   'postponement',\n",
              "   'japan',\n",
              "   'raises',\n",
              "   'eyebrows',\n",
              "   'spike',\n",
              "   'japanese',\n",
              "   'sudden',\n",
              "   'games',\n",
              "   'coronavirus',\n",
              "   'abc',\n",
              "   'cases',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.1555846136368041',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"Japan's sudden spike in coronavirus cases after Tokyo Olympics postponement raises eyebrows\"},\n",
              " {'abstract': '\"Japan needs to have the courage to change, when we are aware we are on the wrong path,\" Iwata said.\\n\"Testing people with a low probability of novel coronavirus would be a waste of resources,\" Japan\\'s Health Ministry told CNN in a statement.\\n#breaking Japan Health Ministry confirms 235 more infections and 3 deaths from coronavirus on Thursday.\\nTokyo Governor Yuriko Koike and Abe are pleading with the public to stay home, avoid travel, and practice social distancing.\\nGovernor Koike announced Friday that 628 of the 750 beds Tokyo has secured for coronavirus patients are occupied, mostly by people with mild symptoms.',\n",
              "  'authors': ['Will Ripley'],\n",
              "  'item_id': 'ccb4ea2e1bf7472090398edefc3b4164',\n",
              "  'keywords': ['japans',\n",
              "   'looms',\n",
              "   'stay',\n",
              "   'health',\n",
              "   'koike',\n",
              "   'course',\n",
              "   'japan',\n",
              "   'japanese',\n",
              "   'late',\n",
              "   'crisis',\n",
              "   'infections',\n",
              "   'coronavirus',\n",
              "   'fears',\n",
              "   'change',\n",
              "   'cases',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.3752183656392556',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'There are fears a coronavirus crisis looms in Tokyo. Is it too late to change course?'},\n",
              " {'abstract': 'When more than 20 million people file for unemployment in a month, even the hard-hearted will have trouble casting them as idlers or parasites.\\nWe are likely to see a repeat of the unemployment crisis of the Great Recession—but the underlying dynamics will be amplified.\\nDuring this second phase, companies will begin to rehire workers.\\nBut lowering such labor costs is easy when the economy is stalled and workers are grateful for any opportunity.\\nThey are likely to be left behind and remain in a jobless limbo long after the economy has picked up again.',\n",
              "  'authors': ['Victor Tan Chen',\n",
              "   'Ofer Sharone',\n",
              "   'Author Of',\n",
              "   'Professor Of Sociology At The University Of Massachusetts Amherst'],\n",
              "  'item_id': '6481e4d54d304f12b56fa6290e352e47',\n",
              "  'keywords': ['costs',\n",
              "   'employers',\n",
              "   'unemployment',\n",
              "   'jobs',\n",
              "   'remain',\n",
              "   'workers',\n",
              "   'labor',\n",
              "   'second',\n",
              "   'economy',\n",
              "   'likely',\n",
              "   'harsher',\n",
              "   'economic',\n",
              "   'phase'],\n",
              "  'relevance': '0.17007566429551982',\n",
              "  'timestamp': Decimal('1587254400'),\n",
              "  'title': 'The Second Phase of Unemployment Will Be Harsher'},\n",
              " {'abstract': 'The President lamented over negative media coverage of the White House reopening guidance unveiled on Thursday and that he didn\\'t understand why the narrative wasn\\'t more positive.\\nThe governors know that.\\nThe Democrat governors know that.\\nThat was the message conveyed by several administration officials at Friday\\'s White House briefing.\\n\"I think some things are too tough,\" Trump said Friday evening at the White House briefing, not offering any specifics.',\n",
              "  'authors': ['Kristen Holmes'],\n",
              "  'item_id': '2b53389802d94247bf2f641bb4a14711',\n",
              "  'keywords': ['house',\n",
              "   'governors',\n",
              "   'know',\n",
              "   'trump',\n",
              "   'reopen',\n",
              "   'country',\n",
              "   'testing',\n",
              "   'president',\n",
              "   'economy',\n",
              "   'coronavirus',\n",
              "   'chomping',\n",
              "   'white',\n",
              "   'bit',\n",
              "   'told',\n",
              "   'america',\n",
              "   'frustrated'],\n",
              "  'relevance': '0.3414048461935341',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"Trump is frustrated and 'chomping at the bit' to reopen America and the economy\"},\n",
              " {'abstract': 'Credit: NIAID-RMLHospitals in New York are giving Covid-19 patients heartburn medicine to see if it helps fight the virus, according to the doctor who initiated the trial.\\nLearning from Chinese patients\\' medicationsTracey and his colleagues got the idea to study famotidine after it was observed that some patients in China taking the drug fared better than patients not taking the drug.\\n\"The poor peasants really seemed to do well on famotidine,\" Tracey said.\\nThe famotidine study was first reported in Science Magazine.\\nThat could leave heartburn patients in the lurch, especially since the FDA pulled Zantac -- a different heartburn medicine -- off the market earlier this month due to possible contamination.',\n",
              "  'authors': [],\n",
              "  'item_id': '1e0b924a305d4032b6e8d59112d83fbf',\n",
              "  'keywords': ['taking',\n",
              "   'covid19',\n",
              "   'hospitals',\n",
              "   'study',\n",
              "   'drug',\n",
              "   'studying',\n",
              "   'york',\n",
              "   'treatment',\n",
              "   'hydroxychloroquine',\n",
              "   'trial',\n",
              "   'tracey',\n",
              "   'virus',\n",
              "   'patients',\n",
              "   'heartburn',\n",
              "   'famotidine'],\n",
              "  'relevance': '0.29258880244516583',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'Famotidine: New York hospitals studying heartburn drug as Covid-19 treatment'},\n",
              " {'abstract': 'While the world fights the coronavirus pandemic, China is fighting a propaganda war.\\nAt stake is China’s global reputation, as well as the potential of a fundamental shift away from China for trade and manufacturing.\\nThirdly, Washington must ensure that China does not capture the global semiconductor chip-making industry, which is a priority for Beijing.\\nTo surrender the crown jewel of the digital economy would put America in a position of permanent dependence vis-à-vis China.\\nThe coronavirus pandemic is a turning point for China and the world.',\n",
              "  'authors': [],\n",
              "  'item_id': '5b71fac8d8914a978e8ae2ca1e4b9b12',\n",
              "  'keywords': ['covid19',\n",
              "   'virus',\n",
              "   'globalization',\n",
              "   'turning',\n",
              "   'world',\n",
              "   'china',\n",
              "   'global',\n",
              "   'chinese',\n",
              "   'beijing',\n",
              "   'chinas',\n",
              "   'point',\n",
              "   'pandemic',\n",
              "   'coronavirus',\n",
              "   'fears',\n",
              "   'dependence'],\n",
              "  'relevance': '0.2579952283882097',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Beijing Fears COVID-19 Is Turning Point for China, Globalization'},\n",
              " {'abstract': \"Published on Apr 1, 2020ABC News' David Wright reports on how the disease has turned one of the biggest tourist destinations into a ghost town, and New Yorkers' continued struggle against COVID-19.\\nABC News Live Prime, Weekdays at 7EST & 9ESTWATCH the ABC News Live Stream Here: https://www.youtube.com/watch?v=w_Ma8...\\nSUBSCRIBE to ABC NEWS: https://bit.ly/2vZb6yPWatch More on http://abcnews.go.com/LIKE ABC News on FACEBOOK https://www.facebook.com/abcnewsFOLLOW ABC News on TWITTER: https://twitter.com/abc\",\n",
              "  'authors': [],\n",
              "  'item_id': '79632cb4ec284ca8a2353ba04c35e97e',\n",
              "  'keywords': ['covid19',\n",
              "   'yorkers',\n",
              "   'turned',\n",
              "   'wright',\n",
              "   'york',\n",
              "   'weekdays',\n",
              "   'abc',\n",
              "   'town',\n",
              "   'struggle',\n",
              "   'live',\n",
              "   'tourist',\n",
              "   'empties',\n",
              "   'twitter'],\n",
              "  'relevance': '0.32169574752318014',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'COVID-19 empties out New York'},\n",
              " {'abstract': 'After U.S. crude oil fell into negative territory yesterday, one expert has warned that the oil industry crisis could force many companies to stop operating.\\nThere is just no market for oil.\\nI haven\\'t used my automobile in a week, and I used to drive every day.\"\\nEarlier this week, Rystad Energy warned that as many as 533 U.S. oil companies go bankrupt if oil stays at around $20 a barrel.\\nHe added, \"At $10, almost every US E&P company that has debt will have to file Chapter 11 or consider strategic opportunities.\"',\n",
              "  'authors': ['Irina Slav',\n",
              "   'Irina Is A Writer For Oilprice.Com With Over A Decade Of Experience Writing On The Oil',\n",
              "   'Gas Industry.',\n",
              "   'More Info',\n",
              "   'Phil Mirzoev'],\n",
              "  'item_id': '34eaef2743634d01960f3467621654bd',\n",
              "  'keywords': ['mass',\n",
              "   'terzic',\n",
              "   'warned',\n",
              "   'used',\n",
              "   'oil',\n",
              "   'companies',\n",
              "   'wti',\n",
              "   'crash',\n",
              "   'market',\n",
              "   'bankruptcies',\n",
              "   'week',\n",
              "   'territory',\n",
              "   'trading',\n",
              "   'research',\n",
              "   'lead'],\n",
              "  'relevance': '0.17984296477873188',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Oil Market Crash Could Lead To Mass Bankruptcies'},\n",
              " {'abstract': 'Published on May 1, 2020The Washington Post spent a day with the Empress Emergency Medical Service in Yonkers as they responded to coronavirus calls.\\n‘It takes its toll on you,’ said paramedic AJ Briones.\\n‘We’ve all seen, especially lately, a lot of death.’ SPECIAL OFFER: To thank you for your support, here’s a deal on a Washington Post digital subscription: $29 for one year http://washingtonpost.com/youtubeoffer.\\nSubscribe to The Washington Post on YouTube: https://wapo.st/2QOdcqKFollow us:Twitter: https://twitter.com/washingtonpostInstagram: https://www.instagram.com/washingtonp...Facebook: https://www.facebook.com/washingtonpost/',\n",
              "  'authors': [],\n",
              "  'item_id': '6fb7050dd1464cf690f003556c0231a9',\n",
              "  'keywords': ['youtube',\n",
              "   'covid19',\n",
              "   'post',\n",
              "   'takes',\n",
              "   'support',\n",
              "   'yonkers',\n",
              "   'yorks',\n",
              "   'ustwitter',\n",
              "   'crisis',\n",
              "   'weve',\n",
              "   'thank',\n",
              "   'washington',\n",
              "   'toll',\n",
              "   'paramedic',\n",
              "   'eyes'],\n",
              "  'relevance': '0.3176428533350044',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': \"New York's covid-19 crisis through the eyes of a paramedic\"},\n",
              " {'abstract': 'The European Commission put out a call Friday for startups and small businesses which are developing technologies that could help combat the COVID-19 outbreak to apply for fast-track EU funding.\\nThe push is related to a €164M pot of money that’s being made available for R&D via the European Innovation Council (EIC) — a European Union funding vehicle which supports the commercialization of high risk, high impact technologies.\\nThe deadline for this call for applications to the EIC Accelerator is 17:00 on March 18 CET.\\nThe EIC is itself funded under the EU’s Horizon Europe research framework program.\\nIt also said there would be a one-off EIC Accelerator call for “green deal” start-ups and SMEs in May 2020 cut-off round to align with its push to make the bloc carbon neutral by 2050.',\n",
              "  'authors': [],\n",
              "  'item_id': 'c6e1b575d9864fe59b73ace26c80c9bf',\n",
              "  'keywords': ['fasttrack',\n",
              "   'track',\n",
              "   'funding',\n",
              "   'covid19',\n",
              "   'eic',\n",
              "   'combat',\n",
              "   'tech',\n",
              "   'startups',\n",
              "   'technologies',\n",
              "   'commission',\n",
              "   'eu',\n",
              "   'urged',\n",
              "   'techcrunch',\n",
              "   'european',\n",
              "   'innovations',\n",
              "   'project',\n",
              "   'developing',\n",
              "   'smes'],\n",
              "  'relevance': '0.31077909832946976',\n",
              "  'timestamp': Decimal('1584316800'),\n",
              "  'title': 'Startups developing tech to combat COVID-19 urged to apply for fast-track EU funding – TechCrunch'},\n",
              " {'abstract': 'Key among them were the multiple times China changed the manner in which it counted its victims, always in an effort to reduce the overall number of infected and those who succumbed to the disease.\\nAs the situation in Italy deteriorated and northern Italy was locked down, the China disinformation machine decided to muddy the waters further by accusing Italy of bringing COVID-19 to China via an unexplained strain of pneumonia.\\nThe Russian pieces pointed blame for a bio-weapon—COVID-19—that got loosed on China, the U.S. or the U.K., depending on the piece of disinformation.\\n#Disinformation flourishes during critical times of #COVID19.\\nSadly, the reality is with China cooking its books from the outset, China has set the world on fire.',\n",
              "  'authors': ['Christopher Burgess',\n",
              "   'Burgessct',\n",
              "   'Is A Writer',\n",
              "   'Speaker',\n",
              "   'Commentator On Security Issues. He Is A Former Senior Security Advisor To Cisco',\n",
              "   'Served',\n",
              "   'Years Within The Cia Which Awarded Him The Distinguished Career Intelligence Medal Upon His Retirement. Christopher Co-Authored The Book',\n",
              "   'Secrets Stolen',\n",
              "   'Fortunes Lost',\n",
              "   'Preventing Intellectual Property Theft'],\n",
              "  'item_id': '1fdcbd24eaa745a7b9cd6f5a1eb1e415',\n",
              "  'keywords': ['times',\n",
              "   'covid19',\n",
              "   'shenanigans',\n",
              "   'virus',\n",
              "   'world',\n",
              "   'united',\n",
              "   'russia',\n",
              "   'chinese',\n",
              "   'wuhan',\n",
              "   'russian',\n",
              "   'china',\n",
              "   'disinformation'],\n",
              "  'relevance': '0.14216972186549645',\n",
              "  'timestamp': Decimal('1586131200'),\n",
              "  'title': 'COVID-19: China and Russia Disinformation and Shenanigans'},\n",
              " {'abstract': \"Tracking The Pandemic: How Quickly Is The Coronavirus Spreading State By State?\\nTo see how quickly your state's case count is growing, click here.\\nLoading...Click here to see a global map of confirmed cases and deaths.\\nIn response to mounting cases, state and federal authorities have emphasized a social distancing strategy, widely seen as the best available means to slow the spread of the virus.\\nSince March 20, New York state, Connecticut and New Jersey have accounted for around 50% of all U.S. cases.\",\n",
              "  'authors': ['Daniel Wood', 'Elena Renken'],\n",
              "  'item_id': 'bc0fc772d593456e87da5dfa993a49e4',\n",
              "  'keywords': ['coronavirus',\n",
              "   'quickly',\n",
              "   'tracking',\n",
              "   'data',\n",
              "   'states',\n",
              "   'cases',\n",
              "   'york',\n",
              "   'rate',\n",
              "   'confirmed',\n",
              "   'spreading',\n",
              "   'state',\n",
              "   'pandemic',\n",
              "   'testing',\n",
              "   'case'],\n",
              "  'relevance': '0.2091362874651128',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'Tracking The Pandemic: How Quickly Is The Coronavirus Spreading State By State?'},\n",
              " {'abstract': 'The data is from county health officials across California and will be updated when we learn of new information.Mobile app users, click here to view the map.More on COVID-19 in CaliforniaLatest coronavirus information, helpful resourcesAs the number of confirmed COVID-19 cases in the United States continues to climb, we’re tracking the number of cases here in California.\\nThe coronavirus outbreak first started in Wuhan, China and has since spread across the globe, impacting Italy, Canada and the United States to name a few countries.\\nThe first case of COVID-19 in the United States was reported on January 14, according to the Centers for Disease Control and Prevention.\\nAdvertisementThe number of cases in the United States began to spike late February and by late March, the number of cases in the country eclipsed 100,000 with more than 2,000 deaths.\\nHere is a breakdown of the number of cases in California.',\n",
              "  'authors': [],\n",
              "  'item_id': '27c4f6e78e114a15b671ab33dd2c4244',\n",
              "  'keywords': ['latest',\n",
              "   'covid19',\n",
              "   'california',\n",
              "   'states',\n",
              "   'view',\n",
              "   'number',\n",
              "   'united',\n",
              "   'late',\n",
              "   'wuhan',\n",
              "   'county',\n",
              "   'coronavirus',\n",
              "   'map',\n",
              "   'cases'],\n",
              "  'relevance': '0.3154117395295469',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'COVID-19 map of California: Latest coronavirus cases by county'},\n",
              " {'abstract': 'CNBC\\'s Jim Cramer on Monday said the meltdown in the oil market is \"emblematic of everything\" that\\'s \"going wrong\" in the economy.\\nThat means oil producers are willing to pay traders to store the oil amid a coronavirus pandemic that has depleted demand for the substance.\\nThose industries would benefit from cheap oil prices in normal times, Cramer said.\\nOil stocks, however, are uninvestable here, given that many oil companies could find it hard to keep their businesses above water, Cramer warned.\\n\"The main takeaway from today is that our economy remains closed,\" he said, \"and when an economy is closed ... you don\\'t need a lot of fossil fuels.\"',\n",
              "  'authors': ['Tyler Clifford'],\n",
              "  'item_id': '07e002ab0812422ca3e30f061f9d3725',\n",
              "  'keywords': ['price',\n",
              "   'thats',\n",
              "   'oil',\n",
              "   'shopify',\n",
              "   'cramer',\n",
              "   'market',\n",
              "   'jim',\n",
              "   'wrong',\n",
              "   'economy',\n",
              "   'prices',\n",
              "   'vertex',\n",
              "   'simply',\n",
              "   'emblematic',\n",
              "   'today'],\n",
              "  'relevance': '0.25360073279330114',\n",
              "  'timestamp': Decimal('1587340800'),\n",
              "  'title': \"Oil market is 'emblematic' of everything wrong with the economy, Jim Cramer says\"},\n",
              " {'abstract': 'MIAMI – Although public health experts say the best way to prevent the coronavirus is to wash hands regularly and avoid travel to high-risk areas, there is a higher demand for disinfectants in Miami on Monday.\\nStore shelves at local stores from Little Havana to Fort Lauderdale were running low of Clorox, Lysol and Purell products.\\nRetailers are also struggling with supplies of products from China such as face masks.\\nExperts say people can also get infected when they touch a surface that has the virus and then touch their own mouth, nose or eyes.\\nTo be proactive, experts say people need to wash their hands for about 20 seconds regularly, avoid touching their faces and regularly clean surfaces.',\n",
              "  'authors': ['Christian De La Rosa', 'Trent Kelly', 'Andrea Torres'],\n",
              "  'item_id': 'bf38644f3fef483fa663210bc647ccb0',\n",
              "  'keywords': ['demand',\n",
              "   'wash',\n",
              "   'say',\n",
              "   'miami',\n",
              "   'regularly',\n",
              "   'hands',\n",
              "   'experts',\n",
              "   'increases',\n",
              "   'products',\n",
              "   'touch',\n",
              "   'way',\n",
              "   'coronavirus',\n",
              "   'spreads',\n",
              "   'disinfection',\n",
              "   'avoid'],\n",
              "  'relevance': '0.19225994844438826',\n",
              "  'timestamp': Decimal('1583107200'),\n",
              "  'title': 'Demand for disinfection products increases in Miami as coronavirus spreads in U.S.'},\n",
              " {'abstract': 'Wednesday, April 22Bloomberg partners with New York for testing, tracing programFormer New York City Mayor Mike Bloomberg will team up with New York to help build a what Gov.\\nNearly a thousand members of the New York State Nurses Association have so far tested positive for coronavirus.\\nPhoto: Justin Heiman/Getty ImagesTuesday, April 14\\ufeffThe New York City death toll jumps by almost 3,800 owing to new counting measures.\\nFriday, April 10New York State has more cases than any country other than the U.S.With at least 161,807 confirmed cases, New York State now has more confirmed coronavirus cases than any country outside the United States.\\nWow (in a good way): In New York state, coronavirus hospitalizations have only gone up by 200 in the past 24 hours, Gov.',\n",
              "  'authors': ['Chas Danner', 'Matt Stieb', 'Benjamin Hart'],\n",
              "  'item_id': '1f79498d4d414068a4ac8291b59f31af',\n",
              "  'keywords': ['latest',\n",
              "   'york',\n",
              "   'covid19',\n",
              "   'deaths',\n",
              "   'city',\n",
              "   'hospital',\n",
              "   'state',\n",
              "   'blasio',\n",
              "   'coronavirus',\n",
              "   'cuomo',\n",
              "   'updates',\n",
              "   'cases'],\n",
              "  'relevance': '0.13056735979192255',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus in New York: Latest Updates'},\n",
              " {'abstract': 'Federal guidelines give state and local authorities leeway in what they consider \"essential\" businesses during an emergency.\\nNorth Carolina decision \\'a matter of life or death\\'After some counties mandated that their residents stay home, North Carolina Gov.\\nIt states that residents can leave \"only for essential activities or to engage in the essential businesses and operations.\"\\nJohn Carney ordered Delaware residents to stay at home and closed nonessential businesses in the state.\\nAll nonessential businesses must close, and all people who can work from home must do so, Pritzker said.',\n",
              "  'authors': [],\n",
              "  'item_id': 'fcb2018b2319403abe278d5273ef0354',\n",
              "  'keywords': ['gov',\n",
              "   'stay',\n",
              "   'states',\n",
              "   'healthy',\n",
              "   'stayathome',\n",
              "   'travel',\n",
              "   'nonessential',\n",
              "   'went',\n",
              "   'effect',\n",
              "   'essential',\n",
              "   'residents',\n",
              "   'amid',\n",
              "   'coronavirus',\n",
              "   'ordered',\n",
              "   'avoid',\n",
              "   'order',\n",
              "   'businesses'],\n",
              "  'relevance': '0.41742437990384906',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"'Stay Home, Stay Healthy': These states have ordered residents to avoid nonessential travel amid coronavirus\"},\n",
              " {'abstract': 'But traders cautioned that this collapse into negative territory was not reflective of the true reality in the beaten-up oil market.\\nThe price of the nearest oil futures contract, which expires Tuesday, detached from later month futures contracts, which continued to trade above $20 per barrel.\\nThis negative price has never happened before for an oil futures contract.\\n\"Right now we don\\'t see any near-term relief for this oil market … we remain really concerned for the outlook on oil near-term,\" she added.\\nThis could be why there\\'s a steeper fall in the May futures contract compared to the June contract.',\n",
              "  'authors': ['Eustance Huang Pippa Stevens',\n",
              "   'Eustance Huang',\n",
              "   'Pippa Stevens'],\n",
              "  'item_id': 'e3fd0cf57d474d3191f30c95b07c70a5',\n",
              "  'keywords': ['futures',\n",
              "   'negative',\n",
              "   'expiring',\n",
              "   'bizarre',\n",
              "   'trade',\n",
              "   'showing',\n",
              "   'went',\n",
              "   'oil',\n",
              "   'crude',\n",
              "   'month',\n",
              "   'market',\n",
              "   'prices',\n",
              "   'barrel',\n",
              "   'contract',\n",
              "   'demand',\n",
              "   'collapse'],\n",
              "  'relevance': '0.1872681964687376',\n",
              "  'timestamp': Decimal('1587340800'),\n",
              "  'title': 'An oil futures contract expiring Tuesday went negative in bizarre move showing a demand collapse'},\n",
              " {'abstract': 'The European Commission is switching all staff in “non-critical functions” to remote working from next Monday in response to the COVID-19 pandemic.\\nEarlier this week the European Parliament also told staff to prep for mass remote working Monday.\\nBelgium, where the Commission is mainly based, has been reporting rising numbers of cases of COVID-19.\\nToday its federal health authority reported 85 new cases today — bringing the total number of confirmed cases in the country to 399.\\nThe Commission itself reported the first cases (two) of COVID-19 among staff earlier this month.',\n",
              "  'authors': [],\n",
              "  'item_id': '60126a9e11f948d7996527af5e355637',\n",
              "  'keywords': ['critical',\n",
              "   'covid19',\n",
              "   'teleworking',\n",
              "   'functions',\n",
              "   'today',\n",
              "   'commission',\n",
              "   'goes',\n",
              "   'default',\n",
              "   'working',\n",
              "   'week',\n",
              "   'techcrunch',\n",
              "   'european',\n",
              "   'told',\n",
              "   'staff',\n",
              "   'cases'],\n",
              "  'relevance': '0.31922664041249404',\n",
              "  'timestamp': Decimal('1583971200'),\n",
              "  'title': 'European Commission goes teleworking by default over COVID-19 – TechCrunch'},\n",
              " {'abstract': 'As of early April 9, Japan has conducted 61,498 tests for the virus, and confirmed 4,877 cases.\\nIf there seems to be a misapprehension about Japan, the Japanese government will dispatch “correct information” to those writing about it.\\nWhile we have confidence in Japan’s health care system today, we believe a significant increase in COVID-19 cases makes it difficult to predict how the system will be functioning in the coming weeks.\\nThere seems to be no interest in getting an indication of the prevalence of the virus in Japanese society.\\nJapan should know that artificial intelligence is great for censorship but you need actual intelligence to combat a coronavirus epidemic.',\n",
              "  'authors': ['Jake Adelstein'],\n",
              "  'item_id': '485f52d0b31c42a8b8019c05c2e7df0a',\n",
              "  'keywords': ['face',\n",
              "   'japans',\n",
              "   'virus',\n",
              "   'health',\n",
              "   'number',\n",
              "   'japan',\n",
              "   'japanese',\n",
              "   'ministry',\n",
              "   'crisis',\n",
              "   'priority',\n",
              "   'tokyo',\n",
              "   'saving',\n",
              "   'coronavirus',\n",
              "   'lives',\n",
              "   'makes'],\n",
              "  'relevance': '0.2086380554672405',\n",
              "  'timestamp': Decimal('1586390400'),\n",
              "  'title': 'Japan Makes Saving Face a Priority Over Saving Lives'},\n",
              " {'abstract': 'andrew cuomoOh, you’re right, you’re right, you’re right.\\nmichael barbaroAs one of the earliest states with confirmed cases of the coronavirus, and with the most confirmed cases so far, New York State has begun to aggressively move to control its spread.\\narchived recording Governor Cuomo signing an executive order closing all schools statewide for the next two weeks.\\nYou’re going to have to go back to the Great Depression to come up with a revival plan for the economy like we’re seeing now.\\nYou’re going to see mortgage foreclosures, you’re going to see bankruptcies, you’re going to see massive unemployment claims all across the board.',\n",
              "  'authors': ['Andy Newman'],\n",
              "  'item_id': '52a2d1ef03a24e0bb9cc94cbfc50b98f',\n",
              "  'keywords': ['youre',\n",
              "   'nyc',\n",
              "   'york',\n",
              "   'right',\n",
              "   'going',\n",
              "   'state',\n",
              "   'thats',\n",
              "   'governor',\n",
              "   'recording',\n",
              "   'president',\n",
              "   'dont',\n",
              "   'shelter',\n",
              "   'combat',\n",
              "   'coronavirus',\n",
              "   'drastic',\n",
              "   'place'],\n",
              "  'relevance': '0.20017207658708167',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Drastic ‘Shelter in Place’ May Be Next for N.Y.C. to Combat Coronavirus'},\n",
              " {'abstract': 'U.S. governors are pursuing different routes to restart local economies hit by the coronavirus pandemic, with several Southern leaders moving to reopen businesses while others voice concern about taking such steps without more robust testing capacity.\\nIn Georgia, gyms, bowling alleys, barbers and other nonessential businesses were preparing to open as early as Friday, even as some mayors pushed back against the governor’s new order.\\nSome retailers in South Carolina were already open, and affected businesses in most Tennessee...',\n",
              "  'authors': ['Jennifer Calfas', 'Arian Campo-Flores', 'Ruth Bender'],\n",
              "  'item_id': 'b6aa165a0926431b856141ddd6be38df',\n",
              "  'keywords': ['taking',\n",
              "   'southern',\n",
              "   'open',\n",
              "   'governors',\n",
              "   'states',\n",
              "   'tennessee',\n",
              "   'reopen',\n",
              "   'look',\n",
              "   'testing',\n",
              "   'voice',\n",
              "   'steps',\n",
              "   'south',\n",
              "   'businesses'],\n",
              "  'relevance': '0.20065824408818178',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Some States Look to Reopen, but Others Want More Testing First'},\n",
              " {'abstract': 'We should start looking forward to reopening quote, unquote.\\nBut reopening with a plan and a smart plan.\\nThe art form is going to be here is doing that smartly, and doing that productively and doing that in a coordinated way.\\nHis workforce is my workforce, my workforce is his workforce.\\nWhat this virus says is all of your lines and boundaries make no sense.',\n",
              "  'authors': [],\n",
              "  'item_id': '7251f79b57d04f95abc4a856c14bb1c3',\n",
              "  'keywords': ['reopening',\n",
              "   'york',\n",
              "   'virus',\n",
              "   'governors',\n",
              "   'authority',\n",
              "   'trump',\n",
              "   'doing',\n",
              "   'workforce',\n",
              "   'state',\n",
              "   'unquote',\n",
              "   'total',\n",
              "   'plan',\n",
              "   'consistent',\n",
              "   'insists',\n",
              "   'supersede',\n",
              "   'way'],\n",
              "  'relevance': '0.3351907692901066',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Trump Insists He Has ‘Total’ Authority to Supersede Governors'},\n",
              " {'abstract': 'The San Francisco Bay Area will be directed to \"shelter in place\" at 12:01 a.m. on Tuesday until April 7 in an attempt to contain the coronavirus outbreak.\\nThe order will require residents to remain indoors except for essential travel, like going to get groceries.\\nThe shelter-in-place order is different from a full lockdown, which would prohibit people from leaving their homes without government permission.\\nThe Bay Area, including San Francisco, will be directed to \"shelter in place\" until April 7 in an attempt to contain the coronavirus outbreak, the San Francisco Chronicle first reported on Monday.\\nThe directive, which will go into effect at 12:01 a.m. on Tuesday, affects six Bay Area counties, including the county and city of San Francisco.',\n",
              "  'authors': ['Katie Canales'],\n",
              "  'item_id': 'f243c5b4e6d74cd4b3b4c41b68a69668',\n",
              "  'keywords': ['order',\n",
              "   'place',\n",
              "   'chronicle',\n",
              "   'city',\n",
              "   'directed',\n",
              "   'area',\n",
              "   'outbreak',\n",
              "   'bay',\n",
              "   'shelter',\n",
              "   'coronavirus',\n",
              "   'lockdown',\n",
              "   'francisco',\n",
              "   'contain',\n",
              "   'ordered',\n",
              "   'san'],\n",
              "  'relevance': '0.30955403381985214',\n",
              "  'timestamp': Decimal('1584316800'),\n",
              "  'title': \"San Francisco Bay Area ordered to 'shelter in place' until April 7 as the city attempts to contain the coronavirus outbreak\"},\n",
              " {'abstract': 'The European Commission has launched a data portal for scientists studying the SARS-CoV-2 virus to speed up access to data sets and tools in order to bolster research efforts by encouraging data reuse and open science.\\nThe COVID-19 Data Portal is intended to accelerate regional efforts to combat the virus by creating a central repository for storing and sharing available research data, such as DNA sequences, protein structures, data from pre-clinical research and clinical trials and epidemiological data.\\n“Rapid and open sharing of data greatly accelerates research and discovery, which is essential to respond to the COVID-19 pandemic,” runs the blurb on the COVID-19 Data Portal website.\\n“To address this challenge, EMBL-EBI and partners have set up the COVID-19 Data Portal, which will bring together relevant datasets submitted to EMBL-EBI and other major centres for biomedical data.\\n“To rapidly populate the COVID-19 Data Portal, EMBL-EBI will bring together COVID-19 datasets that have been submitted to its public databases, including ENA, UniProt, PDBe, EMDB, Expression Atlas and Europe PMC,” is how it’s explained on the site.',\n",
              "  'authors': [],\n",
              "  'item_id': 'b8282f2c74db41f4b2728aca0db542ea',\n",
              "  'keywords': ['efforts',\n",
              "   'covid19',\n",
              "   'support',\n",
              "   'launches',\n",
              "   'open',\n",
              "   'sharing',\n",
              "   'european',\n",
              "   'portal',\n",
              "   'commission',\n",
              "   'eu',\n",
              "   'techcrunch',\n",
              "   'research',\n",
              "   'data'],\n",
              "  'relevance': '0.3015229808415731',\n",
              "  'timestamp': Decimal('1587340800'),\n",
              "  'title': 'EU data portal launches to support COVID-19 research – TechCrunch'},\n",
              " {'abstract': 'The collapse in crude prices is creating mounting losses for ordinary investors, a sign that the energy crisis is starting to ripple beyond the oil patch.\\nIndividual investors have piled into risky products like the United States Oil Fund, a popular exchange-traded fund designed to track the price of crude, despite warnings that such products are unsuitable for inexperienced traders.\\nHundreds...',\n",
              "  'authors': ['Amrith Ramkumar'],\n",
              "  'item_id': '0c25ff23e9ec4592b62f559afd91c94b',\n",
              "  'keywords': ['track',\n",
              "   'tradershundreds',\n",
              "   'fund',\n",
              "   'oil',\n",
              "   'crude',\n",
              "   'markets',\n",
              "   'united',\n",
              "   'crisis',\n",
              "   'products',\n",
              "   'individual',\n",
              "   'investors',\n",
              "   'spreads',\n",
              "   'warnings',\n",
              "   'unsuitable'],\n",
              "  'relevance': '0.25143428236227633',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Oil Market’s Crisis Spreads to Individual Investors'},\n",
              " {'abstract': 'That would represent around 4 million people added to the unemployment rolls for the foreseeable future — dousing hopes of a quick return to the pre-pandemic normal, where the unemployment rate dipped as low as 3.5 percent and pay gains were accelerating for lower-wage workers.\\nAfter jumping to perhaps 20 percent in coming months, Dynan said she anticipated the unemployment rate would come down “pretty quickly” into the single digits by the end of 2020.\\n“But even by the end of next year, I think we are still going to be looking at something like a 6 percent unemployment rate,” she said.\\nThe NABE forecast sees an unemployment rate of nearly 10 percent at the end of 2020, and 6 percent through 2021.\\nA 6 percent unemployment rate is well above what Federal Reserve officials view as full employment.',\n",
              "  'authors': ['April'],\n",
              "  'item_id': 'f7186e11c40c4f51a7e13390b4ded668',\n",
              "  'keywords': ['expect',\n",
              "   'economists',\n",
              "   'job',\n",
              "   'going',\n",
              "   'forecast',\n",
              "   '2021',\n",
              "   'rate',\n",
              "   'unemployment',\n",
              "   'economy',\n",
              "   'pandemic',\n",
              "   'end',\n",
              "   'economic',\n",
              "   'jobs'],\n",
              "  'relevance': '0.23598485313722928',\n",
              "  'timestamp': Decimal('1586476800'),\n",
              "  'title': 'US can expect 6 percent unemployment rate through 2021: economists'},\n",
              " {'abstract': 'Billionaire investor Howard Marks just told CNBC on Monday that “the world is more screwed up” than what the action in the stock market might lead you to believe.\\n“The market does appear to be looking forward to the other side,” he wrote in a blog post on Sunday.\\n“I would say basically we’re like the captain of a ship when the worst typhoon that’s ever happened comes,” Berkshire’s Charlie Munger said.\\n“We just want to get through the typhoon, and we’d rather come out of it with a whole lot of liquidity.\\n“This bounce is an incredible gift to rebalance, take some risk off, go to the virtual beach and wait this thing out,” he wrote.',\n",
              "  'authors': ['Shawn Langlois'],\n",
              "  'item_id': '2a1c3cf52ed04bca8f6eb15c824a37a7',\n",
              "  'keywords': ['buffetts',\n",
              "   'wrote',\n",
              "   'sitting',\n",
              "   'favorite',\n",
              "   'stockmarket',\n",
              "   'warren',\n",
              "   'bejeezus',\n",
              "   'worst',\n",
              "   'typhoon',\n",
              "   'goody',\n",
              "   'indicator',\n",
              "   'market',\n",
              "   'scares',\n",
              "   'evans',\n",
              "   'investor',\n",
              "   'seeing',\n",
              "   'valuation'],\n",
              "  'relevance': '0.25160118780341506',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Warren Buffett’s favorite stock-market indicator ‘scares the bejeezus’ out of this investor'},\n",
              " {'abstract': 'President Donald Trump announced guidelines for states to start opening their economies that have been largely shut down by the coronavirus pandemic.\\nMore headlines:President Donald Trump announces guidelines for opening of US economyPresident Donald Trump unveiled recommendations to start the process of states reopening the U.S. economy that has been severely damaged by the coronavirus pandemic.\\nPresident Donald Trump, in announcing the reopening at his daily news conference, said some are more ready than others.\\n– Jesse YomtovMidwest governors in 7 states to partner to reopen regional economyThe governors of Illinois, Michigan, Ohio, Wisconsin, Minnesota, Indiana and Kentucky on Thursday announced that the states plan to coordinate efforts to reopen the Midwest economy.\\n\"Alcohol compromises the body’s immune system and increases the risk of adverse health outcomes,” the WHO\\'s regional office for Europe reported.',\n",
              "  'authors': [],\n",
              "  'item_id': '049e19315810492297603bcd57411c4c',\n",
              "  'keywords': ['york',\n",
              "   'optimism',\n",
              "   'states',\n",
              "   'health',\n",
              "   'trump',\n",
              "   'reopen',\n",
              "   'weeks',\n",
              "   'usa',\n",
              "   'stimulus',\n",
              "   'plan',\n",
              "   'small',\n",
              "   'goes',\n",
              "   'unemployment',\n",
              "   'coronavirus',\n",
              "   'risk',\n",
              "   'updates',\n",
              "   'million',\n",
              "   'live'],\n",
              "  'relevance': '0.2616827921423514',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus live updates: Donald Trump announces plan to reopen economy; small biz stimulus fund goes bust; optimism from New York'},\n",
              " {'abstract': 'Media playback is unsupported on your device Media caption Coronavirus quarantine: \\'I haven\\'t had freedom for a month\\'The World Health Organization says the official name for the disease caused by the new coronavirus is Covid-19.\\n\"We now have a name for the disease and it\\'s Covid-19,\" WHO chief Tedros Adhanom Ghebreyesus told reporters in Geneva.\\nThe word coronavirus refers to the group of viruses it belongs to, rather than the latest strain.\\nThe party secretary for the Hubei Health Commission, and the head of the commission, were among those who lost their jobs.\\nDr Ghebreyesus of the WHO said there was still a realistic chance of containing the disease if enough resources were devoted to the fight.',\n",
              "  'authors': [],\n",
              "  'item_id': 'ede8a198640c440bb399d88cd7bef19a',\n",
              "  'keywords': ['covid19',\n",
              "   'virus',\n",
              "   'group',\n",
              "   'ghebreyesus',\n",
              "   'health',\n",
              "   'number',\n",
              "   'world',\n",
              "   'hubei',\n",
              "   'media',\n",
              "   'named',\n",
              "   'coronavirus',\n",
              "   'disease'],\n",
              "  'relevance': '0.30214752463386885',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus disease named Covid-19'},\n",
              " {'abstract': 'This is because without some form of contacts tracing there’s a risk that hard-won gains to reduce the rate of infections by curtailing people’s movements will be reversed, i.e.\\nAlthough whether contacts tracing apps can be as effective at helping to contain COVID-19 as policymakers and technologists hope remains an open question.\\nAnd just last week the bloc’s lead privacy regulator, the EDPS, told us it’s monitoring developments around COVID-19 contacts tracing apps.\\nHence the fierce push back from certain pro-privacy quarters for contacts tracing to be decentralized — to guard against any state data grabs.\\nYou need both; because if you’re tracing contacts and you can’t get tested what’s that good for?',\n",
              "  'authors': [],\n",
              "  'item_id': '9dcd7b762b9d432780d135ae345a78a6',\n",
              "  'keywords': ['approach',\n",
              "   'covid19',\n",
              "   'centralized',\n",
              "   'health',\n",
              "   'tracing',\n",
              "   'experts',\n",
              "   'system',\n",
              "   'push',\n",
              "   'decentralized',\n",
              "   'eu',\n",
              "   'contacts',\n",
              "   'data',\n",
              "   'theres',\n",
              "   'apps',\n",
              "   'techcrunch',\n",
              "   'privacy'],\n",
              "  'relevance': '0.2797494986641731',\n",
              "  'timestamp': Decimal('1586131200'),\n",
              "  'title': 'EU privacy experts push a decentralized approach to COVID-19 contacts tracing – TechCrunch'},\n",
              " {'abstract': 'The first chart shows that on Jan. 22, The Arora Report called a potential stock market drop due to coronavirus.\\n• The second chart shows the resistance zone.\\nAsk Arora: Nigam Arora answers your questions about investing in stocks, ETFs, bonds, gold and silver, oil and currencies.\\nDisclosure: Subscribers to The Arora Report may have positions in the securities mentioned in this article or may take positions at any time.\\nNigam Arora is an investor, engineer and nuclear physicist by background who has founded two Inc. 500 fastest-growing companies.',\n",
              "  'authors': ['Nigam Arora'],\n",
              "  'item_id': '170ee82697c24b0bbb493b2089631754',\n",
              "  'keywords': ['stock',\n",
              "   'nigam',\n",
              "   'boston',\n",
              "   'stockmarket',\n",
              "   'pay',\n",
              "   'baffling',\n",
              "   'chart',\n",
              "   'attention',\n",
              "   'shows',\n",
              "   'market',\n",
              "   'coronavirus',\n",
              "   'second',\n",
              "   'investors',\n",
              "   'arora',\n",
              "   'report',\n",
              "   'data'],\n",
              "  'relevance': '0.23905831284332354',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Stock-market investors should pay attention to baffling coronavirus data from Boston'},\n",
              " {'abstract': '■ Average earnings rose by 4 cents an hour last month and are up 2.6 percent over the past year.\\nThe TakeawayAmerican employers continue to find reasons to expand their payrolls.\\nApril marked the 91st consecutive month of job gains, far and away the longest streak of increases on record.\\nThe average monthly gain has declined each year since 2014, but that’s normal for an economy that’s been in recovery for such an extended period.',\n",
              "  'authors': ['Natalie Kitroeff'],\n",
              "  'item_id': 'd08c2094bd154274aa7fd79f9edf6e5e',\n",
              "  'keywords': ['39',\n",
              "   'yearthe',\n",
              "   'low',\n",
              "   'job',\n",
              "   'record',\n",
              "   'reasons',\n",
              "   'hits',\n",
              "   'streak',\n",
              "   'thats',\n",
              "   'average',\n",
              "   'rare',\n",
              "   'rate',\n",
              "   'rose',\n",
              "   'unemployment',\n",
              "   'market',\n",
              "   'month',\n",
              "   'takeawayamerican',\n",
              "   'competitive',\n",
              "   'recovery'],\n",
              "  'relevance': '0.25794068459957875',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Unemployment Rate Hits 3.9%, a Rare Low, as Job Market Becomes More Competitive'},\n",
              " {'abstract': 'Lockdowns have led to cleaner air, but will it last?',\n",
              "  'authors': ['Jordan Freiman',\n",
              "   'Justin Carissimo',\n",
              "   'Kate Gibson',\n",
              "   'Pamela Falk',\n",
              "   'Tucker Reals',\n",
              "   'Caroline Linton'],\n",
              "  'item_id': '68b79352d57a4ac6bdddb6bc66d7cfad',\n",
              "  'keywords': ['led',\n",
              "   'washington',\n",
              "   'state',\n",
              "   'kills',\n",
              "   'lockdowns',\n",
              "   'air',\n",
              "   'cleaner',\n",
              "   'coronavirus'],\n",
              "  'relevance': '0.1978738949667574',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus kills 6 in Washington state'},\n",
              " {'abstract': 'Photo: Getty ImagesAs of Friday afternoon, there have been more than 309,696 confirmed cases of the coronavirus in New York, including more than 172,784 in New York City.\\nThe facility, however, will remain intact in case there is a second wave of the coronavirus outbreak in New York City.\\nFriday, April 10New York State has more cases than any country other than the U.S.With at least 161,807 confirmed cases, New York State now has more confirmed coronavirus cases than any country outside the United States.\\nWow (in a good way): In New York state, coronavirus hospitalizations have only gone up by 200 in the past 24 hours, Gov.\\nUSNS Comfort arrives in New York CityThe USNS Comfort, a Navy hospital ship that left Norfolk Saturday, arrived in New York City Monday morning.',\n",
              "  'authors': ['Chas Danner', 'Matt Stieb', 'Benjamin Hart'],\n",
              "  'item_id': 'a1301212fe174dc59defdb2e7900d6f4',\n",
              "  'keywords': ['coronavirus',\n",
              "   'covid19',\n",
              "   'updates',\n",
              "   'blasio',\n",
              "   'latest',\n",
              "   'cases',\n",
              "   'york',\n",
              "   'governor',\n",
              "   'deaths',\n",
              "   'state',\n",
              "   'city',\n",
              "   'cuomo'],\n",
              "  'relevance': '0.12961380866231548',\n",
              "  'timestamp': Decimal('1588291200'),\n",
              "  'title': 'Coronavirus in New York: Latest Updates'},\n",
              " {'abstract': '\"China\\'s response to the outbreak was truly a nationwide response: systematic, comprehensive and coordinated.\\nGlobal action:Great Recession showed nations can’t fight coronavirus crisis aloneEurope has adopted some, but not all, of China’s most restrictive steps.\\n\"It may be a country like China has a more top-down ability to insist on certain behavior changes.\\nChina\\'s National Health Commission said Tuesday it will start including asymptomatic coronavirus carriers in its daily figures.\\nAs of Wednesday, China recorded less than half – about 82,000 – the number of U.S. coronavirus cases.',\n",
              "  'authors': [],\n",
              "  'item_id': '5168644fe23c4fe1983e9ff0772770c5',\n",
              "  'keywords': ['handle',\n",
              "   'hospitals',\n",
              "   'say',\n",
              "   'beat',\n",
              "   'health',\n",
              "   'response',\n",
              "   'couldnt',\n",
              "   'experts',\n",
              "   'chinese',\n",
              "   'chinas',\n",
              "   'wuhan',\n",
              "   'china',\n",
              "   'coronavirus',\n",
              "   'america',\n",
              "   'public',\n",
              "   'usa'],\n",
              "  'relevance': '0.4483042473005062',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"This is what China did to beat coronavirus. Experts say America couldn't handle it\"},\n",
              " {'abstract': 'Rand Paul becomes first senator to test positive for Covid-19 Read moreCoronavirus cases in the US have increased to more than 30,000.\\nThere have been 390 reported deaths in the US, with New York state the country’s hardest hit, with 114 deaths.\\nBy Sunday the state of New York accounted for half the country’s 30,000 cases nationwide.\\nCuomo also announced plans to set up temporary hospitals in three suburbs of New York City.\\nAlso in New York City, at least 38 people have now tested positive at the notorious Rikers Island complex and nearby facilities.',\n",
              "  'authors': ['Kenya Evelyn'],\n",
              "  'item_id': '48d51ec44a8b4f2ca7ea9a9bb749a2d3',\n",
              "  'keywords': ['masks',\n",
              "   'york',\n",
              "   'covid19',\n",
              "   'states',\n",
              "   'city',\n",
              "   'trump',\n",
              "   'state',\n",
              "   'worldwide',\n",
              "   'test',\n",
              "   'battlefront',\n",
              "   'cuomo',\n",
              "   'cases',\n",
              "   'medical'],\n",
              "  'relevance': '0.2256967865273097',\n",
              "  'timestamp': Decimal('1584835200'),\n",
              "  'title': 'New York has 5% of Covid-19 cases worldwide as city becomes battlefront'},\n",
              " {'abstract': 'Published on Apr 7, 2020New York State Governor Andrew Cuomo said his state recorded the highest single-day increase in deaths since the outbreak began, as 731 people died.\\n- Subscribe to our channel: http://aje.io/AJSubscribe- Follow us on Twitter: https://twitter.com/AJEnglish- Find us on Facebook: https://www.facebook.com/aljazeera- Check our website: https://www.aljazeera.com/#AlJazeeraEnglish #NewYork #Coronavirus',\n",
              "  'authors': [],\n",
              "  'item_id': '9d7237ee4d06491880c47cb487b36546',\n",
              "  'keywords': ['website',\n",
              "   'covid19',\n",
              "   'records',\n",
              "   'york',\n",
              "   'subscribe',\n",
              "   'increase',\n",
              "   'highest',\n",
              "   'recorded',\n",
              "   'singleday',\n",
              "   'published',\n",
              "   'state',\n",
              "   'deaths',\n",
              "   'outbreak',\n",
              "   'newyork',\n",
              "   'twitter'],\n",
              "  'relevance': '0.31573518388118765',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'New York records highest single-day increase in COVID-19 deaths'},\n",
              " {'abstract': 'New York City\\'s COVID-19 Death Toll Soars Past 10,000Enlarge this image toggle caption Johannes Eisele/AFP via Getty Images Johannes Eisele/AFP via Getty ImagesNew York City has drastically increased its estimate of the number of people killed by COVID-19 to include probable victims who were not tested.\\nFor weeks, firefighters and paramedics have been recording a massive spike in deaths at home around New York City.\\nNow city officials have recalculated the toll that the virus has taken and reached a staggering number — adding nearly 4,000 to the total.\\nWe are focused on ensuring that every New Yorker who died because of COVID-19 gets counted,\" said the city\\'s health commissioner, Dr. Oxiris Barbot.\\nBut all have agreed that the health care crisis must be under control first, which is likely to take months.',\n",
              "  'authors': ['Quil Lawrence'],\n",
              "  'item_id': 'dfa68ff2e23d4c6d912b22c7e44dccb6',\n",
              "  'keywords': ['soars',\n",
              "   'covid19',\n",
              "   'victims',\n",
              "   'citys',\n",
              "   'tested',\n",
              "   'number',\n",
              "   'health',\n",
              "   'past',\n",
              "   'york',\n",
              "   'virus',\n",
              "   'johannes',\n",
              "   'city',\n",
              "   'death',\n",
              "   '10000',\n",
              "   'toll'],\n",
              "  'relevance': '0.2591720589600871',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': \"New York City's COVID-19 Death Toll Soars Past 10,000\"},\n",
              " {'abstract': 'With this Recommendation, we put in motion a European coordinated approach for the use of such apps and data, without compromising on our EU privacy and data protection rules, and avoiding the fragmentation of the internal market.\\nThe Commission itself has leant on telcos to provide anonymized and aggregated user location data for COVID-19 tracking purposes.\\nThe European Commission Recommendation on Bluetooth COVID-19 proximity tracing apps specifically notes that they should apply decentralisation as a key data minimisation safeguard, in line with #DP3T.\\nWhile a compliance, e-government and health expert subgroup is also now working on guidance for the processing of health data for research purposes in the coronavirus context.\\nThis is good.”The Commission intends the toolbox for moving towards a pan-European approach for COVID-19 mobile applications to be developed by April 15.',\n",
              "  'authors': ['Natasha Lomas'],\n",
              "  'item_id': 'cbcaa60ab1064ddca0887fb76614075d',\n",
              "  'keywords': ['common',\n",
              "   'covid19',\n",
              "   'techcrunch',\n",
              "   'rights',\n",
              "   'states',\n",
              "   'health',\n",
              "   'tracing',\n",
              "   'commission',\n",
              "   'eu',\n",
              "   'crisis',\n",
              "   'apps',\n",
              "   'citizens',\n",
              "   'protect',\n",
              "   'privacy',\n",
              "   'fight',\n",
              "   'public',\n",
              "   'data'],\n",
              "  'relevance': '0.23787068421098098',\n",
              "  'timestamp': Decimal('1586304000'),\n",
              "  'title': 'Call for common EU approach to apps and data to fight COVID-19 and protect citizens’ rights – TechCrunch'},\n",
              " {'abstract': 'The soaring U.S. unemployment rate might not match the peak of 25% seen during the Great Depression of the 1930s, but it could come uncomfortably close in the next few months.\\nIt will be at least another month or two, however, before the official unemployment rate starts to reflect the full devastation in the labor market.\\nThe official unemployment rate, even one that rises to 15% or higher, might actually underestimate how many people are really out of work.\\nStill, the federal bailout “artificially lowers the number of people unemployed and the unemployment rate,” said economist Joel Naroff of Naroff Economic Advisors.\\n“[W]e need a new ‘real unemployment rate measure.”Expect economists to come up ways to guestimate the “real” unemployment rate, but it doesn’t really matter.',\n",
              "  'authors': ['Jeffry Bartash'],\n",
              "  'item_id': '59b4f9b5608a4e54830df5d5dfcd0d1d',\n",
              "  'keywords': ['approach',\n",
              "   'economists',\n",
              "   'depressionera',\n",
              "   'soaring',\n",
              "   'millions',\n",
              "   'work',\n",
              "   'levels',\n",
              "   'rate',\n",
              "   'unemployment',\n",
              "   'lost',\n",
              "   'great',\n",
              "   'high',\n",
              "   'coronavirus',\n",
              "   'million',\n",
              "   'jobs'],\n",
              "  'relevance': '0.2313724992955028',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'The soaring U.S. unemployment rate could approach Great Depression-era levels'},\n",
              " {'abstract': 'The widow-making collapse in oil prices came to a pause in Asia trade on Thursday, but analysts aren\\'t convinced that the worst sell-off in oil market history is over yet.\\n\"We think that this is the inflection phase,\" Goldman Sachs oil analyst Damien Courvalin told CNBC\\'s \"Squawk Box Asia.\"\\n\"The downward pressure on oil prices still remains immense,\" Hari told CNBC\\'s \"Street Signs Asia,\" pointing to the continued uncertainty around the pandemic.\\nTrump also said he would consider halting Saudi Arabian crude oil imports, a move that would help protect the U.S. oil industry and related jobs in an election year.\\nWATCH: Global oil storage may become full by end May: S&P Global Platts',\n",
              "  'authors': ['Dan Murphy'],\n",
              "  'item_id': '9d6c669f8adc4906a4ccadfce6368ca8',\n",
              "  'keywords': ['historic',\n",
              "   'goldman',\n",
              "   'trade',\n",
              "   'ships',\n",
              "   'global',\n",
              "   'inflection',\n",
              "   'oil',\n",
              "   'crude',\n",
              "   'markets',\n",
              "   'supply',\n",
              "   'storage',\n",
              "   'market',\n",
              "   'week',\n",
              "   'prices',\n",
              "   'told',\n",
              "   'courvalin',\n",
              "   'sachs',\n",
              "   'phase'],\n",
              "  'relevance': '0.2475873724538644',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"Oil markets in 'inflection phase' after historic week of trade, Goldman Sachs says\"},\n",
              " {'abstract': \"New York City had an average of just over 27,000 people per square mile, according to the 2010 Census .\\nSo New York's high number of coronavirus cases is also just a reflection of its size.\\nAnother reason why New York has so many confirmed coronavirus cases is because it is looking for them.\\nNew York state had its first case of community spread, meaning its origin was not known, on March 2 in New Rochelle, a suburb of New York City.\\nConcrete jungle where dreams are made ofNew York City is a world-renowned tourist destination and the most visited destination in the US.\",\n",
              "  'authors': ['Eric Levenson'],\n",
              "  'item_id': '515ba303df9b45cd898696361e352967',\n",
              "  'keywords': ['coronavirus',\n",
              "   'epicenter',\n",
              "   'travel',\n",
              "   'cases',\n",
              "   'york',\n",
              "   'american',\n",
              "   'virus',\n",
              "   'stop',\n",
              "   'state',\n",
              "   'city',\n",
              "   'outbreak',\n",
              "   'country',\n",
              "   'testing'],\n",
              "  'relevance': '0.39562551172856597',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'Why New York is the epicenter of the American coronavirus outbreak'},\n",
              " {'abstract': 'Ford said Tuesday it will temporarily shut down vehicle and engine production at its factories in Europe in response to the spread of COVID-19, a disease caused by the coronavirus.\\nThe shutdown will begin Thursday and is expected to continue for a number of weeks, Ford said, without providing a specific timeline.\\nOther automakers are also shuttering factories in Europe, including Volkswagen and Daimler.\\nVolkswagen CEO Herbert Diess announced plans to suspend production at factories in Spain, Portugal and Italy before the end of this week.\\nDaimler Group announced Tuesday it will suspend the majority of its production in Europe, as well as work in selected administrative departments, for an initial period of two weeks.',\n",
              "  'authors': [],\n",
              "  'item_id': 'd4396b9625734a23a4f7410bc4ad8832',\n",
              "  'keywords': ['covid19',\n",
              "   'ford',\n",
              "   'volkswagen',\n",
              "   'europe',\n",
              "   'temporarily',\n",
              "   'production',\n",
              "   'daimler',\n",
              "   'week',\n",
              "   'techcrunch',\n",
              "   'european',\n",
              "   'coronavirus',\n",
              "   'suspend',\n",
              "   'factories',\n",
              "   'vehicle'],\n",
              "  'relevance': '0.29947517431795134',\n",
              "  'timestamp': Decimal('1584403200'),\n",
              "  'title': 'Ford, Daimler to suspend production at European factories due to COVID-19 – TechCrunch'},\n",
              " {'abstract': 'Traffic accidents and crash-related injuries and deaths were reduced by half during the first three weeks of California’s shelter-in-place order, which began March 20.\\nThe report, “Impact of COVID-19 on California Traffic Crashes,” is published on the center’s website.\\nFlattening the crash curveIn parallel with the more than 50 percent reduction in traffic collisions and related injuries and deaths came a 55 percent reduction in traffic on some highways.\\nTo create the report, the authors used observations and reported traffic incidents from CHP in the real-time “California Highway Incident Processing System,” or CHIPS.\\nThe shelter-in-place order offered a new opportunity to explore unintended impacts of the order on state highway traffic conditions.',\n",
              "  'authors': [],\n",
              "  'item_id': 'ead082fad1cd4677ba42cebdc975ca1e',\n",
              "  'keywords': ['covid19',\n",
              "   'reduction',\n",
              "   'report',\n",
              "   'finds',\n",
              "   'california',\n",
              "   'injuries',\n",
              "   'state',\n",
              "   'shelterinplace',\n",
              "   'collisions',\n",
              "   'traffic',\n",
              "   'crashes',\n",
              "   'silver',\n",
              "   'lining',\n",
              "   'order'],\n",
              "  'relevance': '0.2975028490445077',\n",
              "  'timestamp': Decimal('1586995200'),\n",
              "  'title': 'California COVID-19 Traffic Report Finds Silver Lining'},\n",
              " {'abstract': \"The virus, which causes a respiratory illness called COVID-19, has spread to more than 180 countries and territories on six continents, infecting 1.6 million people and killing nearly 100,000.\\nIn the following weeks, the outbreak ballooned across China, spread across the border, and reached all but one of the world's continents.\\nHow does the virus spread?\\nLike other coronaviruses, the new one is transmitted from person to person via droplets when an infected person breathes out, coughs or sneezes.\\nWHO says COVID-19 has killed about 5.9 percent of confirmed cases globally, a figure far above the fatality rate for the seasonal flu.\",\n",
              "  'authors': [],\n",
              "  'item_id': '994a7c26177e4cad8c92d835fb946148',\n",
              "  'keywords': ['covid19',\n",
              "   'virus',\n",
              "   'weeks',\n",
              "   'number',\n",
              "   'person',\n",
              "   'authorities',\n",
              "   'spread',\n",
              "   'causes',\n",
              "   'charts',\n",
              "   'coronavirus',\n",
              "   'china',\n",
              "   'maps',\n",
              "   'cases'],\n",
              "  'relevance': '0.28144397376943403',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'COVID-19: In charts and maps'},\n",
              " {'abstract': '(CNN) Nearly seven million people living in a wide swath of Northern California, including Silicon Valley, are being ordered to shelter in place starting at midnight Monday night as authorities try to stop the novel coronavirus from spreading.\\nThe Bay Area order is the most draconian yet of measures being taken across the country to stem the virus\\' spread as the number of cases continues to rise.\\n\"We must move aggressively and immediately,\" San Jose Mayor Sam Liccardo said at a news conference announcing the order.\\nHealth services, grocery stores, gas stations, banks and food delivery services will remain open.\\nMass transit will stay open but is to be used only for travel to and from essential services.',\n",
              "  'authors': ['Cheri Mossburg', 'Theresa Waldrop'],\n",
              "  'item_id': 'ce0bf3550e76431980f3d6caa8073949',\n",
              "  'keywords': ['place',\n",
              "   'wide',\n",
              "   'stay',\n",
              "   'ordered',\n",
              "   'california',\n",
              "   'open',\n",
              "   'services',\n",
              "   'virus',\n",
              "   'residents',\n",
              "   'shelter',\n",
              "   'waiting',\n",
              "   'measures',\n",
              "   'valley',\n",
              "   'million',\n",
              "   'san',\n",
              "   'order'],\n",
              "  'relevance': '0.20662006828465446',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Almost 7 million California residents ordered to shelter in place'},\n",
              " {'abstract': 'Tokyo — The U.S. Embassy in Tokyo has warned Americans of a \"significant increase\" in the number of coronavirus infections in Japan, and urged them to leave the country now unless they plan to stay indefinitely.\\nPeople wearing face masks wait to cross a road on April 3, 2020 in Tokyo, Japan.\\nGettyThe U.S. State Department\\'s current advisory for Japan stands at \"Level 2,\" which calls on Americans to \"exercise increased caution.\"\\nJapan reported more than 650 new COVID-19 infections over the past three days, with the country\\'s biggest jump happening April 2.\\nUnless control measures are stepped-up, his modelling predicts new cases in Tokyo will peak at 6,000 per day.',\n",
              "  'authors': ['Ramy Inocencio'],\n",
              "  'item_id': '93c81722b863450d871d63b32be030d7',\n",
              "  'keywords': ['significant',\n",
              "   'covid19',\n",
              "   'stay',\n",
              "   'increase',\n",
              "   'leave',\n",
              "   'unless',\n",
              "   'japan',\n",
              "   'surge',\n",
              "   'warns',\n",
              "   'weekend',\n",
              "   'shinzo',\n",
              "   'americans',\n",
              "   'infections',\n",
              "   'amid',\n",
              "   'advisory',\n",
              "   'cases',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.2519435708973662',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'U.S. warns Americans to leave Japan amid \"significant increase\" in COVID-19 cases'},\n",
              " {'abstract': 'The reductions would then be eased to 8 million barrels per day from July 1 through Dec. 31, followed by 6 million barrels in cuts from Jan. 1, 2021 to April 30, 2022.\\nMexico has reportedly not agreed to the plan and wants to lessen its share of production cuts.\\nAnother source told TASS that no agreements on oil production cuts were reached because the issue was not even raised during the meeting.\\nHe said the U.S. will “look for more opportunities to ease the hurt felt by our producers,” but did not mention any voluntary U.S. oil output cuts.\\n“Trump’s willingness to assist with oil cuts seems limited to a free-market-based response, not U.S.-mandated cuts,” Stewart Glickman, energy analyst at CFRA Research, told MarketWatch on Friday.',\n",
              "  'authors': ['Myra P. Saefong', 'Marshall Steeves', 'Ihs Markit'],\n",
              "  'item_id': '4534ce6feafb44d6aea807752b9d740e',\n",
              "  'keywords': ['save',\n",
              "   'cuts',\n",
              "   'global',\n",
              "   'cut',\n",
              "   'oil',\n",
              "   'plan',\n",
              "   'opecs',\n",
              "   'production',\n",
              "   'market',\n",
              "   'opec',\n",
              "   'barrels',\n",
              "   'output',\n",
              "   'heres',\n",
              "   'demand',\n",
              "   'million'],\n",
              "  'relevance': '0.16531170678881443',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Here’s OPEC+’s plan to save the oil market'},\n",
              " {'abstract': 'Why did this happen?\\nPlease make sure your browser supports JavaScript and cookies and that you are not blocking them from loading.\\nFor more information you can review our Terms of Service and Cookie Policy.',\n",
              "  'authors': [],\n",
              "  'item_id': 'd691562237e44afcb226125a9e7bd815',\n",
              "  'keywords': ['sure',\n",
              "   'information',\n",
              "   'terms',\n",
              "   'review',\n",
              "   'robot',\n",
              "   'javascript',\n",
              "   'service',\n",
              "   'happenplease',\n",
              "   'policy',\n",
              "   'loading',\n",
              "   'supports'],\n",
              "  'relevance': '0.5149772078791859',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Are you a robot?'},\n",
              " {'abstract': 'Hundreds were ordered off beaches and cited in one California community over the weekend by police enforcing a shelter-in-place order meant to slow the spread of coronavirus.\\nCORONAVIRUS STAY-AT-HOME ORDER IN CALIFORNIA HAS POLICE GIVING CITATIONS TO PEOPLE WATCHING SUNSET AT BEACHHundreds violated that order this past weekend across San Mateo County.\\n\"We had hundreds of people on the beach and practically anywhere they could find a place to sit along the coast line.\"\\nDeputies gave more than 650 people verbal warnings, issued more than 300 parking tickets and handed out five citations.\\nState officials have asked residents to avoid flocking to beaches and parks due to COVID-19.',\n",
              "  'authors': ['Travis Fedschun',\n",
              "   'Travis Fedschun Is A Reporter For Foxnews.Com. Follow Him On Twitter'],\n",
              "  'item_id': '871976263e224eeaa2db387460ed98c8',\n",
              "  'keywords': ['visitors',\n",
              "   'citations',\n",
              "   'california',\n",
              "   'issue',\n",
              "   'shelterinplace',\n",
              "   'violations',\n",
              "   'beach',\n",
              "   'beaches',\n",
              "   'hundreds',\n",
              "   'orders',\n",
              "   'parks',\n",
              "   'weekend',\n",
              "   'miles',\n",
              "   'pacifica',\n",
              "   'order'],\n",
              "  'relevance': '0.19896043857961218',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'California police order hundreds off beaches, issue citations over shelter-in-place violations'},\n",
              " {'abstract': 'Some states ease restrictions as protestors call for reopening economyHere are the latest updates from around the globe.',\n",
              "  'authors': ['Nbc News'],\n",
              "  'item_id': '0cd624e442944c2a8f9049cc2011a4fc',\n",
              "  'keywords': ['latest',\n",
              "   'protestors',\n",
              "   'reopening',\n",
              "   'states',\n",
              "   'economyhere',\n",
              "   'restrictions',\n",
              "   'economy',\n",
              "   'updates',\n",
              "   'ease',\n",
              "   'globe'],\n",
              "  'relevance': '0.27190678735394025',\n",
              "  'timestamp': Decimal('1587254400'),\n",
              "  'title': 'Some states ease restrictions as protestors call for reopening economy'},\n",
              " {'abstract': 'President Donald Trump speaks during the daily briefing on Covid-19, in the White House on April 22.\\nMandel Ngan/AFP/Getty ImagesPresident Trump defended the decision to reopen states amid the coronavirus pandemic, saying the virus will not \"be coming back\" in as many cases as the US saw earlier this year.\\n\"And if it comes back, though, it won\\'t be coming back in the form that it was.\\nHe continued: \"But in my opinion, from everything I\\'ve seen, it can never be like anything we\\'ve witnessed right now.\\nReporter to Trump: How can you say the virus will not come back at the same level?',\n",
              "  'authors': ['Meg Wagner',\n",
              "   'Fernando Alfonso Iii',\n",
              "   'Veronica Rocha',\n",
              "   \"Cnn'S Anna-Maja Rappard\",\n",
              "   \"Cnn'S Will Brown\",\n",
              "   \"Cnn'S Allison Flexner\",\n",
              "   \"Cnn'S Jamiel Lynch\",\n",
              "   'Cnn S Sharif Paget'],\n",
              "  'item_id': '021d16c88c2b417a93c3fedb6a477fd9',\n",
              "  'keywords': ['saying',\n",
              "   'virus',\n",
              "   'flu',\n",
              "   'embers',\n",
              "   'georgia',\n",
              "   'trump',\n",
              "   'reopen',\n",
              "   'corona',\n",
              "   'weve',\n",
              "   'economy',\n",
              "   'coming',\n",
              "   'soon',\n",
              "   'yearand',\n",
              "   'come'],\n",
              "  'relevance': '0.1958213677523165',\n",
              "  'timestamp': Decimal('1587513600'),\n",
              "  'title': 'Trump says it is \"too soon\" for Georgia to reopen its economy'},\n",
              " {'abstract': 'Watch Channel 2 Action News for LIVE hour-by-hour updates.\\nGet extended coverage on the free WSB Now app on your Roku, Amazon Fire and Apple TV.',\n",
              "  'authors': ['Wsbtv.Com News Staff',\n",
              "   'Cox Media Group National Content Desk',\n",
              "   'Debbie Lord',\n",
              "   'Natalie Dreier',\n",
              "   'Jared Leone'],\n",
              "  'item_id': '6957584e79664fdca72caaf1df7e21ff',\n",
              "  'keywords': ['wsb',\n",
              "   'extended',\n",
              "   'georgia',\n",
              "   'roku',\n",
              "   'free',\n",
              "   'tv',\n",
              "   'coverage',\n",
              "   'watch',\n",
              "   'hourbyhour',\n",
              "   'shelter',\n",
              "   'updates',\n",
              "   '30',\n",
              "   'coronavirus',\n",
              "   'end',\n",
              "   'place',\n",
              "   'live'],\n",
              "  'relevance': '0.19937573187707705',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'CORONAVIRUS IN GEORGIA: Shelter in place to end April 30'},\n",
              " {'abstract': \"State and county totals come from a continual Times survey of California's 58 county health agencies as well as the three run by cities.\\nState officials acknowledge that their tallies lag behind the updates posted by local agencies throughout the day and do not dispute The Times' method.\\nThe tallies here are mostly limited to residents of California, which is the standard method used to count patients by the state’s health authorities.\\nIn an effort to aid scientists and researchers in the fight against COVID-19, The Times has released its database of California coronavirus cases to the public.\\nClosures and restrictions are drawn from an ongoing Times survey of county governments.\",\n",
              "  'authors': ['Los Angeles Times Staff'],\n",
              "  'item_id': 'bb5374f02fed4767befc01017968cffe',\n",
              "  'keywords': ['using',\n",
              "   'times',\n",
              "   'california',\n",
              "   'states',\n",
              "   'health',\n",
              "   'tallies',\n",
              "   'tracking',\n",
              "   'survey',\n",
              "   'outbreak',\n",
              "   'method',\n",
              "   'county',\n",
              "   'coronavirus',\n",
              "   'cases',\n",
              "   'totals',\n",
              "   'data'],\n",
              "  'relevance': '0.269729321056546',\n",
              "  'timestamp': Decimal('1587513600'),\n",
              "  'title': 'California coronavirus cases: Tracking the outbreak'},\n",
              " {'abstract': 'Published on Apr 7, 2020New York State Governor Andrew Cuomo said his state recorded the highest single-day increase in deaths since the outbreak began, as 731 people died.\\n- Subscribe to our channel: http://aje.io/AJSubscribe- Follow us on Twitter: https://twitter.com/AJEnglish- Find us on Facebook: https://www.facebook.com/aljazeera- Check our website: https://www.aljazeera.com/#AlJazeeraEnglish #NewYork #Coronavirus',\n",
              "  'authors': [],\n",
              "  'item_id': '2f32480746684ea9b8704530ad5716f5',\n",
              "  'keywords': ['york',\n",
              "   'covid19',\n",
              "   'subscribe',\n",
              "   'increase',\n",
              "   'deaths',\n",
              "   'twitter',\n",
              "   'state',\n",
              "   'newyork',\n",
              "   'outbreak',\n",
              "   'published',\n",
              "   'website',\n",
              "   'highest',\n",
              "   'singleday',\n",
              "   'records',\n",
              "   'recorded'],\n",
              "  'relevance': '0.31573518388118765',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'New York records highest single-day increase in COVID-19 deaths'},\n",
              " {'abstract': '(Carlos Garcia Rawlins/Reuters)On today’s menu: a day-by-day, month-by-month breakdown of China’s coronavirus coverup and the irreparable damage it has caused around the globe.\\nThis is additional strong evidence of human-to-human transmission.\\nThree weeks after doctors first started noticing the cases, China contacts the World Health Organization.\\nIn the first quarter of 2019 for comparison, 13,267 air passengers traveled from Wuhan, China, to destinations in the United States, or about 4,422 per month.\\nNo clear evidence of human-to-human transmission has been found.”Also on this day, political leaders in Hubei province, which includes Wuhan, began their regional meeting.',\n",
              "  'authors': ['Jim Geraghty',\n",
              "   'Rich Lowry',\n",
              "   'Kyle Smith',\n",
              "   'Conrad Black',\n",
              "   'Victor Davis Hanson',\n",
              "   'Zachary Evans',\n",
              "   'Dan Mclaughlin',\n",
              "   'Kevin D. Williamson',\n",
              "   'Read More',\n",
              "   'The American Conservative'],\n",
              "  'item_id': 'aa3c0714a9f64a0c869e4323ed1a514d',\n",
              "  'keywords': ['devastating',\n",
              "   'virus',\n",
              "   'health',\n",
              "   'humantohuman',\n",
              "   'chinese',\n",
              "   'chinas',\n",
              "   'lies',\n",
              "   'wuhan',\n",
              "   'coverup',\n",
              "   'china',\n",
              "   'coronavirus',\n",
              "   'transmission',\n",
              "   'cases',\n",
              "   'medical'],\n",
              "  'relevance': '0.14098462182115898',\n",
              "  'timestamp': Decimal('1584921600'),\n",
              "  'title': \"China's Coronavirus Coverup -- Devastating Lies\"},\n",
              " {'abstract': \"This morning, New York state reported 10,841 new cases and 630 new deaths, raising its total cases to 113,704.\\nItaly today reported 4,805 more cases and 681 more deaths, raising its respective totals to 124,632 and 15,362.\\nFrance has 90,843 total cases.\\nIn Turkey, one of Europe's new hot spots, health officials reported 3,013 more cases, raising its total to 23,934.\\nSouth Korea has 10,156 total cases.\",\n",
              "  'authors': [],\n",
              "  'item_id': 'abb1d3d1f59847298459ebdb9bf99813',\n",
              "  'keywords': ['masks',\n",
              "   'york',\n",
              "   'covid19',\n",
              "   'nations',\n",
              "   '300000',\n",
              "   'raising',\n",
              "   'reported',\n",
              "   'deaths',\n",
              "   'health',\n",
              "   'total',\n",
              "   'longer',\n",
              "   'tops',\n",
              "   'lockdown',\n",
              "   'eye',\n",
              "   'european',\n",
              "   'face',\n",
              "   'today',\n",
              "   'cases'],\n",
              "  'relevance': '0.2902357156279859',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'US COVID-19 total tops 300,000; European nations eye longer lockdown'},\n",
              " {'abstract': 'Boeing plans to start offering some workers voluntary buyouts on Monday, according to an email sent to employees this week.\\nThe email did not say how many workers would be eligible or how many Boeing hoped would take up the offer.\\nThose approved would leave the company in early June with a week of pay for every year worked, up to 26 weeks.\\nKraft Heinz will continue to offer manufacturing employees a $100 bonus for the next two weeks, a company spokesman said on Wednesday.\\nAfter two employees at a food production plant in Holland, Mich., tested positive for the coronavirus, Kraft Heinz closed the facility on Sunday for a deep cleaning and reopened it on Monday.',\n",
              "  'authors': [],\n",
              "  'item_id': 'e34511719b5b4d368a72548fed5120b0',\n",
              "  'keywords': ['automakers',\n",
              "   'plans',\n",
              "   'reopen',\n",
              "   'airlines',\n",
              "   'billion',\n",
              "   'employees',\n",
              "   'production',\n",
              "   'offering',\n",
              "   'week',\n",
              "   'workers',\n",
              "   'heinz',\n",
              "   'plant',\n",
              "   'consider',\n",
              "   'offer'],\n",
              "  'relevance': '0.3365223142785369',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Airlines and Automakers Consider How to Reopen'},\n",
              " {'abstract': '(CNN) The basics of a \"shelter-in-place\" order during the coronavirus pandemic are fairly clear: Stay at home.\\nThat order, though, has plenty of exemptions for \"essential\" activities.\\nAnd it is far from the shelter-in-place of acute emergencies, like for active shootings or tornadoes.\\nAs cities, states and the federal government take increasingly aggressive moves to stop the spread of the novel coronavirus, the precise details of a shelter-in-place order and its many exceptions for \"essential\" activities may soon become familiar to millions of Americans across the country.\\nThe purpose of such an order is to enforce social distancing, or to keep people away from each other to limit the spread of the virus.',\n",
              "  'authors': ['Eric Levenson', 'Kevin Flower'],\n",
              "  'item_id': '70645cd2919549f99ac1ad450580e319',\n",
              "  'keywords': ['virus',\n",
              "   'stay',\n",
              "   'tornadoesas',\n",
              "   'allowed',\n",
              "   'isnt',\n",
              "   'shelterinplace',\n",
              "   'activities',\n",
              "   'essential',\n",
              "   'stop',\n",
              "   'spread',\n",
              "   'coronavirus',\n",
              "   'order'],\n",
              "  'relevance': '0.4022621495003324',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"What is and isn't allowed during a 'shelter-in-place' order\"},\n",
              " {'abstract': '(CNN) Most people who contract the novel coronavirus experience mild symptoms , according to data from China, where the worst of the epidemic now appears to be over .\\nThe data shows that men and women have roughly the same chance of contracting the virus.\\nWhile 2.8% of the men diagnosed with the disease died, only 1.7% of women did.\\nAccording to the paper, among people aged 49 or younger, only about 0.2% of those who contracted the disease died, compared to 14.8% of those who were 80 and older.\\nExperts have warned that even though younger people may only experience mild symptoms of Covid-19, they still need to take the risk seriously.',\n",
              "  'authors': ['Ivana Kottasová', 'Henrik Pettersson'],\n",
              "  'item_id': '5c9fe19fa2a74ec8ad8b4d7f7e47cd75',\n",
              "  'keywords': ['covid19',\n",
              "   'virus',\n",
              "   'suffer',\n",
              "   'health',\n",
              "   'mild',\n",
              "   'recover',\n",
              "   'shows',\n",
              "   'patients',\n",
              "   'died',\n",
              "   'majority',\n",
              "   'china',\n",
              "   'men',\n",
              "   'disease',\n",
              "   'cases',\n",
              "   'symptoms',\n",
              "   'data'],\n",
              "  'relevance': '0.35139238369341824',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Data from China shows the majority of people with Covid-19 only suffer mild symptoms, then recover'},\n",
              " {'abstract': 'Since California’s first COVID-19 patient was confirmed in January, cases of the disease caused by the novel coronavirus have continued to increase in the state.\\nAs the virus spreads in communities, it’s important to understand the scale of the outbreak in your local region.\\nHere, you can track the number of reported cases and deaths over time in your county.\\nYou can also examine the prevalence of certain risk factors for more serious COVID-19 cases in your county.\\nLearn more about COVID-19 in your county here.',\n",
              "  'authors': ['Emily Zentner'],\n",
              "  'item_id': 'e6536b77020b41f98cea02583fcfb1ca',\n",
              "  'keywords': ['track',\n",
              "   'understand',\n",
              "   'covid19',\n",
              "   'virus',\n",
              "   'california',\n",
              "   'stateas',\n",
              "   'stoltz',\n",
              "   'county',\n",
              "   'zentner',\n",
              "   'coronavirus',\n",
              "   'cases'],\n",
              "  'relevance': '0.2575979802852105',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Track COVID-19 Cases In California By County'},\n",
              " {'abstract': 'The answer is simple: We need diagnostic test kits.\\n“Right now, labs can start testing with the existing CDC test kits,” Messonnier said Friday.\\nMeanwhile, New York, at least, has started to make its own coronavirus test kits in the wake of the CDC production errors.\\n“Lack of testing kits has angered doctors and patients in China,” he added, leading them to lean on things like CT scans and lung X-rays to diagnose patients.\\nAs of February 17, the agency had sent 3,500 testing kits to 34 countries and three regional WHO offices.',\n",
              "  'authors': ['Julia Belluz',\n",
              "   'Brian Resnick',\n",
              "   'German Lopez',\n",
              "   'Dylan Scott',\n",
              "   'Rani Molla',\n",
              "   'Emily Stewart',\n",
              "   'Ian Millhiser',\n",
              "   'Terry Nguyen',\n",
              "   'Feb'],\n",
              "  'item_id': '6ef53e0489364f2d8b36ff0488f44d15',\n",
              "  'keywords': ['cases',\n",
              "   'diagnostic',\n",
              "   'tests',\n",
              "   'covid19',\n",
              "   'explained',\n",
              "   'health',\n",
              "   'testing',\n",
              "   'kits',\n",
              "   'snafu',\n",
              "   'coronavirus',\n",
              "   'disease',\n",
              "   'test',\n",
              "   'symptoms',\n",
              "   'cdc'],\n",
              "  'relevance': '0.22867232885257974',\n",
              "  'timestamp': Decimal('1582848000'),\n",
              "  'title': 'The coronavirus diagnostic testing snafu, explained'},\n",
              " {'abstract': \"Something that's never happened in the oil market is happening today: negative prices on an oil contract.\\nWhile many people may see this and think the overall price of oil is negative, there's nuance.\\nToward the end of a contract's expiration date, the price typically converges with the physical price of oil as the final buyers of these contracts are entities like refineries or airlines that are going to take actual physical delivery of the oil.\\nThose who stay in the position to the final day are typically buying the physical commodity, such as a refiner.\\nWith the coronavirus pandemic leading to unprecedented demand loss, and with storage tanks quickly filling up, there is no demand for this oil contract expiring Tuesday.\",\n",
              "  'authors': ['Pippa Stevens'],\n",
              "  'item_id': '39a75522092f4f9d81c4e07340f765fd',\n",
              "  'keywords': ['price',\n",
              "   'bad',\n",
              "   'physical',\n",
              "   'negative',\n",
              "   'buying',\n",
              "   'isnt',\n",
              "   'delivery',\n",
              "   'oil',\n",
              "   'commodity',\n",
              "   'crash',\n",
              "   'market',\n",
              "   'heres',\n",
              "   'contracts',\n",
              "   'contract'],\n",
              "  'relevance': '0.439661778878113',\n",
              "  'timestamp': Decimal('1587340800'),\n",
              "  'title': \"This oil price crash isn't as bad as it seems — here's why\"},\n",
              " {'abstract': 'California has recorded more than 370 deaths due to Covid-19 and 15,800 confirmed cases, nowhere near as bad as the surge ravaging New York.\\nIn Los Angeles county, where there are more than 6,000 cases, officials have urged residents to avoid the grocery store this week, if possible.\\nIf cases peak in mid-May, that doesn’t mean the lockdown ends.\\nSo how does California decide to start to reopen after we think we’ve hit a peak?\\nI don’t think it’s especially likely, but I don’t rule it out.',\n",
              "  'authors': ['Sam Levin'],\n",
              "  'item_id': 'e1df57a9e9e442e2a95cf659e6736a07',\n",
              "  'keywords': ['cases',\n",
              "   'covid19',\n",
              "   'california',\n",
              "   'going',\n",
              "   'wave',\n",
              "   'think',\n",
              "   'officials',\n",
              "   'reality',\n",
              "   'lockdown',\n",
              "   'week',\n",
              "   'dont',\n",
              "   'whats',\n",
              "   'ahead',\n",
              "   'math',\n",
              "   'peak',\n",
              "   'check'],\n",
              "  'relevance': '0.3022327745225444',\n",
              "  'timestamp': Decimal('1586217600'),\n",
              "  'title': \"'A reality check on the math': What's ahead for California and Covid-19\"},\n",
              " {'abstract': 'US President Donald Trump has said he strongly disagreed with Georgia state\\'s aggressive push to reopen its economy in the midst of the novel coronavirus pandemic, saying it was \"just too soon\" to lift restrictions.\\nGeorgia Governor Brian Kemp, a Republican and Trump ally, is allowing businesses such as beauty salons, tattoo parlours and bowling alleys to reopen as soon as Friday.\\nTrump said Georgia is not adhering to federal guidelines for states to restart their economies.\\nI think it\\'s too soon,\" Trump said on Wednesday.\\nTrump\\'s top adviser on the pandemic, Dr Anthony Fauci, said mitigation strategies were working, setting the stage for some states to reopen.',\n",
              "  'authors': [],\n",
              "  'item_id': '8ffdd02ceb284075bce70608534505fa',\n",
              "  'keywords': ['virus',\n",
              "   'states',\n",
              "   'georgia',\n",
              "   'trump',\n",
              "   'reopen',\n",
              "   'state',\n",
              "   'disagrees',\n",
              "   'push',\n",
              "   'kemp',\n",
              "   'economy',\n",
              "   'soon',\n",
              "   'workers',\n",
              "   'pandemic',\n",
              "   'coronavirus'],\n",
              "  'relevance': '0.29627649872548734',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Coronavirus: Trump disagrees with Georgia push to reopen economy'},\n",
              " {'abstract': 'Dow Jones Industrial Average rises 456 pointsThe Dow surged 456.94 points, or 1.99%, to 23,475.82.\\nThe S&P 500 advanced 2.29% to 2,799.31.\\nOil surgesThe West Texas Intermediate contract for June jumped 19%, erasing an earlier decline and lifting the energy sector in the S&P 500.\\nChipotle jumps on earningsChipotle shares gained more than 12% after the company reported quarterly earnings that beat analyst expectations.\\nSnap shares gained more than 36% on the back of its quarterly numbers.',\n",
              "  'authors': ['Fred Imbert'],\n",
              "  'item_id': '3c464a3d3e8345c89fd796835f4294b7',\n",
              "  'keywords': ['stock',\n",
              "   'gained',\n",
              "   'shares',\n",
              "   'points',\n",
              "   'dow',\n",
              "   'quarterly',\n",
              "   'welloil',\n",
              "   'earnings',\n",
              "   'west',\n",
              "   'happened',\n",
              "   'market',\n",
              "   '500',\n",
              "   'heres',\n",
              "   'sp'],\n",
              "  'relevance': '0.41920020983458567',\n",
              "  'timestamp': Decimal('1587513600'),\n",
              "  'title': \"Here's what happened to the stock market on Wednesday\"},\n",
              " {'abstract': 'In New York, Number Of COVID-19 Patients In ICU Drops For First TimeEnlarge this image toggle caption New York State/New York State New York State/New York StateNew York Governor Andrew Cuomo said that for the first time since the beginning of the coronavirus crisis the number of people in intensive care units has gone down.\\nSpeaking to reporters Friday, Cuomo said that across the state there were 17 fewer ICU patients than the day before.\\nBut he said he\\'s cautiously optimistic that the infection rate is slowing, and urged people to continue staying at home.\\nMayor Bill de Blasio spoke at the Billie Jean King tennis center in Queens, which he said would get its first patients Friday.\\nCuomo called on Congress to pass another stimulus bill, calling the previous legislation \"unfair\" to New York, which, he said, is where the need is.',\n",
              "  'authors': ['Rose Friedman'],\n",
              "  'item_id': '91baa61f8fd0483994f438a0417040dd',\n",
              "  'keywords': ['york',\n",
              "   'covid19',\n",
              "   'scale',\n",
              "   'number',\n",
              "   'state',\n",
              "   'blasio',\n",
              "   'testing',\n",
              "   'statenew',\n",
              "   'cuomo',\n",
              "   'drops',\n",
              "   'patients',\n",
              "   'urged',\n",
              "   'rate',\n",
              "   'icu'],\n",
              "  'relevance': '0.25869286797248997',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'In New York, Number Of COVID-19 Patients In ICU Drops For First Time'},\n",
              " {'abstract': 'AdvertisementPrime Minister Shinzo Abe has declared a state of emergency in Tokyo and six other hard-hit Japanese prefectures to fortify the fight against the coronavirus outbreak.\\nA look at what Japan’s state of emergency entails:Q.\\nSome Tokyo residents drew criticism for rushing to escape from Tokyo to the countryside.\\nDoes a state of emergency cause a Tokyo lockdown?\\nA monthlong state of emergency in the Tokyo area could cause consumer spending to fall nearly 2.5 trillion yen ($23 billion), according to Nomura Research Institute.',\n",
              "  'authors': ['Associated Press'],\n",
              "  'item_id': '69614121752f4216bccc73275d3f38f6',\n",
              "  'keywords': ['japans',\n",
              "   'covid19',\n",
              "   'abe',\n",
              "   'yen',\n",
              "   'state',\n",
              "   'japan',\n",
              "   'emergency',\n",
              "   'trillion',\n",
              "   'residents',\n",
              "   'lockdown',\n",
              "   'request',\n",
              "   'whats',\n",
              "   'tokyo'],\n",
              "  'relevance': '0.23888528273457105',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Japan’s COVID-19 State of Emergency Is No Lockdown. What’s In It?'},\n",
              " {'abstract': \"Published on Apr 1, 2020ABC News' David Wright reports on how the disease has turned one of the biggest tourist destinations into a ghost town, and New Yorkers' continued struggle against COVID-19.\\nABC News Live Prime, Weekdays at 7EST & 9ESTWATCH the ABC News Live Stream Here: https://www.youtube.com/watch?v=w_Ma8...\\nSUBSCRIBE to ABC NEWS: https://bit.ly/2vZb6yPWatch More on http://abcnews.go.com/LIKE ABC News on FACEBOOK https://www.facebook.com/abcnewsFOLLOW ABC News on TWITTER: https://twitter.com/abc\",\n",
              "  'authors': [],\n",
              "  'item_id': 'bf843eb2236c4fec810bcb93cbb8646a',\n",
              "  'keywords': ['empties',\n",
              "   'york',\n",
              "   'covid19',\n",
              "   'tourist',\n",
              "   'twitter',\n",
              "   'weekdays',\n",
              "   'struggle',\n",
              "   'wright',\n",
              "   'town',\n",
              "   'turned',\n",
              "   'yorkers',\n",
              "   'abc',\n",
              "   'live'],\n",
              "  'relevance': '0.32169574752318014',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'COVID-19 empties out New York'},\n",
              " {'abstract': 'The nation has not experienced this magnitude of layoffs and economic contraction since the Great Depression, many experts say, and recovery is unlikely to be swift.\\nLast week, 6.6 million Americans applied for unemployment benefits, the Labor Department said Thursday, a staggering number that would have been unthinkable a few months ago.\\nIt’s just shy of the record set the week of March 28, when 6.9 million people filed for aid.\\nADADGovernment relief has been slow to reach people losing their jobs as states have been overwhelmed with claims.\\nA study by Health Management Associates warned that more than 12 million Americans could lose employer-based health insurance for themselves and family members as layoffs mount.',\n",
              "  'authors': ['Heather Long',\n",
              "   'Economics Correspondent',\n",
              "   'Andrew Van Dam',\n",
              "   'Reporter Focusing On Economic Data'],\n",
              "  'item_id': '3370d03d62044c62a9e9b7b68560a30d',\n",
              "  'keywords': ['states',\n",
              "   'shortlived',\n",
              "   'americans',\n",
              "   'unemployment',\n",
              "   'jobs',\n",
              "   'economy',\n",
              "   'workers',\n",
              "   'week',\n",
              "   'depression',\n",
              "   'challenge',\n",
              "   'america',\n",
              "   'million',\n",
              "   'aid',\n",
              "   'businesses'],\n",
              "  'relevance': '0.34592483437901134',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'America is in a depression. The challenge now is to make it short-lived.'},\n",
              " {'abstract': 'A long line of unemployed people outside the State Labor Bureau building in New York City on Nov. 24, 1933.',\n",
              "  'authors': [],\n",
              "  'item_id': '0f6375fbe8b847c5b9874717bc618ca1',\n",
              "  'keywords': ['york',\n",
              "   'job',\n",
              "   'labor',\n",
              "   'city',\n",
              "   'long',\n",
              "   'estimates',\n",
              "   'state',\n",
              "   'unemployed',\n",
              "   'line',\n",
              "   'total',\n",
              "   'rate',\n",
              "   'hit',\n",
              "   'unemployment',\n",
              "   'losses',\n",
              "   'outside',\n",
              "   'file',\n",
              "   'nov',\n",
              "   'fed',\n",
              "   'coronavirus',\n",
              "   'million'],\n",
              "  'relevance': '0.3207706922589581',\n",
              "  'timestamp': Decimal('1585526400'),\n",
              "  'title': 'Coronavirus job losses could total 47 million, unemployment rate may hit 32 percent, Fed estimates'},\n",
              " {'abstract': \"While Mnuchin made clear this was just one of a number of possible scenarios and Congress seems poised to pass a massive stimulus bill as soon as today, it's worth thinking about the political impact of a 20% unemployment rate.\\n(Nota bene: Obviously political considerations aren't the first thing any of us would think about if unemployment even approximated these levels.\\nLet's dig into the history of extremely high unemployment rates -- and what they meant politically.\\nThe only time the unemployment rate has been over 20% in modern American history in a four-year period from 1932 to 1935 -- right in the teeth of the Great Depression.\\nDuring that time, the unemployment rate peaked at 24.9% in 1933.\",\n",
              "  'authors': ['Analysis Chris Cillizza', 'Cnn Editor-At-Large'],\n",
              "  'item_id': 'dfc2820649344612b60ba5470d6d96ab',\n",
              "  'keywords': ['20',\n",
              "   'mnuchin',\n",
              "   'actually',\n",
              "   'rate',\n",
              "   'impact',\n",
              "   'unemployment',\n",
              "   'politics',\n",
              "   'high',\n",
              "   'political',\n",
              "   'congress',\n",
              "   'massive',\n",
              "   'history',\n",
              "   'mean'],\n",
              "  'relevance': '0.3930719351236418',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'What a 20% unemployment rate would actually mean for our politics'},\n",
              " {'abstract': 'It’s hard to deny, although some do, that the stock market, pre-coronavirus, was pushing the limits of what it means to be in a bubble.\\nOf course, bubbles come and go, but as Hofstra University’s Jean-Paul Rodrigue suggests, this one had a particularly fierce tailwind.\\n“Although manias and bubbles have taken place many times before in history...” he once wrote, “central banks appear to make matters worse by providing too much credit and being unable or unwilling to stop the process with things are getting out of control.”',\n",
              "  'authors': ['Shawn Langlois'],\n",
              "  'item_id': 'f009998df02047da83d6b26ff1e6b054',\n",
              "  'keywords': ['stock',\n",
              "   'times',\n",
              "   'wrote',\n",
              "   'normal',\n",
              "   'unwilling',\n",
              "   'worse',\n",
              "   'return',\n",
              "   'market',\n",
              "   'things',\n",
              "   'taken',\n",
              "   'universitys',\n",
              "   'bubbles',\n",
              "   'mean',\n",
              "   'unable',\n",
              "   'tailwindalthough',\n",
              "   'disaster'],\n",
              "  'relevance': '0.3245635112130983',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Why a ‘return to normal’ could mean disaster for the stock market'},\n",
              " {'abstract': 'The UK government reportedly believes the coronavirus outbreak may have started in a Chinese laboratory.\\nSome scientists, however, believe an accidental leak is a plausible alternative theory — and the Mail on Sunday said UK officials were not ruling it out.\\nA UK parliamentary committee last week accused the Chinese government of spreading disinformation about the origins of the virus.\\n\"Perhaps it is no coincidence that there is that laboratory in Wuhan,\" one UK government official told the Mail on Sunday.\\nLast week it was reported that UK officials were furious with the Chinese state.',\n",
              "  'authors': ['Adam Payne'],\n",
              "  'item_id': '53750e62ce69492f8d9adea62de31878',\n",
              "  'keywords': ['laboratory',\n",
              "   'leaked',\n",
              "   'johnsons',\n",
              "   'virus',\n",
              "   'lab',\n",
              "   'chinese',\n",
              "   'officials',\n",
              "   'possibility',\n",
              "   'boris',\n",
              "   'accidentally',\n",
              "   'considered',\n",
              "   'wuhan',\n",
              "   'scientists',\n",
              "   'outbreak',\n",
              "   'coronavirus',\n",
              "   'believe',\n",
              "   'uk'],\n",
              "  'relevance': '0.34877798302110324',\n",
              "  'timestamp': Decimal('1586131200'),\n",
              "  'title': \"Boris Johnson's government has considered the possibility that the coronavirus may have accidentally leaked from a Chinese lab\"},\n",
              " {'abstract': 'Nick Oxford | ReutersStocks rose for a second day even after data showed another 4.4 million Americans filed for jobless claims last week amid coronavirus shutdowns.\\nOccidental Petroleum shot up more than 8%, Devon Energy soared more than 11%, and Exxon Mobil rose more than 5%.\\n— Pound9:31 am: Stocks open slightly higherStocks rose a tad at the open as investors digested data showing jobless claims totaled 4.4 million last week.\\nLast week, new jobless claims totaled 4.427 million, the Labor Department reported Thursday.\\nAnother 4.3 million workers are expected to have filed state unemployment claims last week, according to Dow Jones.',\n",
              "  'authors': ['Yun Li Maggie Fitzgerald Jesse Pound',\n",
              "   'Yun Li',\n",
              "   'Maggie Fitzgerald',\n",
              "   'Jesse Pound'],\n",
              "  'item_id': 'c783b65d785b4bfab9cdc7cf3cb82a87',\n",
              "  'keywords': ['stock',\n",
              "   'live',\n",
              "   'exxon',\n",
              "   'filed',\n",
              "   'dow',\n",
              "   'claims',\n",
              "   'number',\n",
              "   'oil',\n",
              "   'sales',\n",
              "   'rose',\n",
              "   'market',\n",
              "   'jobless',\n",
              "   'jumps',\n",
              "   'week',\n",
              "   'coronavirus',\n",
              "   'million',\n",
              "   'updates',\n",
              "   '300',\n",
              "   'stocks',\n",
              "   'pops'],\n",
              "  'relevance': '0.18023834798697072',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Stock market live updates: Dow up 300, oil jumps above $17, Exxon pops 5%'},\n",
              " {'abstract': 'Over the past few days, I’ve been reading the major plans for what comes after social distancing.\\nThe app could be developed for a purely public health nonprofit entity such as the Association of State and Territorial Health Officials (ASTHO)—an organization that represents state health officials—which would host the data.\\nThat is, in my view, far less likely than the construction of a huge digital surveillance state.\\nThey either envision life under a surveillance and testing state of dystopian (but perhaps necessary!)\\nRead my colleagues at Recode and the Verge for more on that (and here’s some smart analysis from Casey Newton).',\n",
              "  'authors': ['Ezra Klein',\n",
              "   'German Lopez',\n",
              "   'Dylan Scott',\n",
              "   'Rani Molla',\n",
              "   'Emily Stewart',\n",
              "   'Ian Millhiser',\n",
              "   'Terry Nguyen',\n",
              "   'Apr'],\n",
              "  'item_id': '337ced7073aa4c6db91c43d9162ddf32',\n",
              "  'keywords': ['social',\n",
              "   'imagine',\n",
              "   'plans',\n",
              "   'scary',\n",
              "   'reopen',\n",
              "   'health',\n",
              "   'state',\n",
              "   'testing',\n",
              "   'surveillance',\n",
              "   'data',\n",
              "   'plan',\n",
              "   'economy',\n",
              "   'ive',\n",
              "   'read',\n",
              "   'tests',\n",
              "   'theyre'],\n",
              "  'relevance': '0.1569558178122778',\n",
              "  'timestamp': Decimal('1586476800'),\n",
              "  'title': 'I’ve read the plans to reopen the economy. They’re scary.'},\n",
              " {'abstract': \"COVID-19 map of California: Coronavirus cases by county Share Shares Copy Link CopyHide Transcript Show TranscriptCOUNTY ALSO CRACKING DOWN ON PEOPLE NOT FOLLOWING THE RULES.\\nIT'S PARTICULARLY TROUBLESOME AT PLEASURE POINT WHICH IS A POPULAR SURFING SPOT.\\nTHE COUNTY ALSO TAKING AWAY PARKING SPOTS TO DISCOURAGE VISITS TO THAT AREA.\\n'IF PEOPLE CONTINUE TO IGNORE THE STAY AT HOME ORDER OR SOCIAL DISTANCE PROTOCAL, THE COUNTY COULD CLOSE ITS BEACHES.\\nIN SANTA CRUZ COUNTY, PHIL GOMEZ, KSBW ACTION NEWS 8 NEW AT SIX..\",\n",
              "  'authors': [],\n",
              "  'item_id': '863b61cb5ba94deb8f1102ce7ed575c4',\n",
              "  'keywords': ['social',\n",
              "   'pleasure',\n",
              "   'phil',\n",
              "   'covid19',\n",
              "   'stay',\n",
              "   'spots',\n",
              "   'california',\n",
              "   'wont',\n",
              "   'parking',\n",
              "   'sheriff',\n",
              "   'county',\n",
              "   'point',\n",
              "   'coronavirus',\n",
              "   'map',\n",
              "   'cases'],\n",
              "  'relevance': '0.3112919342308422',\n",
              "  'timestamp': Decimal('1586390400'),\n",
              "  'title': 'COVID-19 map of California: Coronavirus cases by county'},\n",
              " {'abstract': 'California recommends COVID-19 testing for asymptomatic people as coronavirus death toll soars An autopsy confirms that two California residents have possibly died from the coronavirus about three weeks earlier from the first coronavirus death in the U.S.\\nThis comes as the Golden State wants to ramp up testing for COVID-19.\\nJamie Yuccas reports.',\n",
              "  'authors': [],\n",
              "  'item_id': '8ae924a122744fca86ab565164c3ce77',\n",
              "  'keywords': ['covid19',\n",
              "   'california',\n",
              "   'weeks',\n",
              "   'state',\n",
              "   'testing',\n",
              "   'toll',\n",
              "   'wants',\n",
              "   'yuccas',\n",
              "   'soars',\n",
              "   'coronavirus',\n",
              "   'asymptomatic',\n",
              "   'death',\n",
              "   'recommends'],\n",
              "  'relevance': '0.31439908065311933',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'California recommends COVID-19 testing for asymptomatic people as coronavirus death toll soars'},\n",
              " {'abstract': \"Jared Polis extended the state's stay-at-home order, which now remains in effect until April 26.\\nJohn Bel Edwards extended the state's stay-at-home order through April 30.\\nGretchen Whitmer extended the state's stay-at-home order through April 30.\\nTim Walz extended the state's stay-at-home order through May 3.\\nTony Evers has extended his state's stay-at-home order to expire May 26, according to a statement from the governor's office.\",\n",
              "  'authors': ['Alaa Elassar'],\n",
              "  'item_id': 'c5928ffa837447da98c111590124e6df',\n",
              "  'keywords': ['reopening',\n",
              "   'states',\n",
              "   'stayathome',\n",
              "   '50',\n",
              "   'state',\n",
              "   'reopen',\n",
              "   'economy',\n",
              "   'issued',\n",
              "   'stand',\n",
              "   'governor',\n",
              "   'order',\n",
              "   'businesses'],\n",
              "  'relevance': '0.3391071344959313',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'This is where all 50 states stand on reopening'},\n",
              " {'abstract': \"Tracking The Pandemic: How Quickly Is The Coronavirus Spreading State By State?\\nTo see how quickly your state's case count is growing, click here.\\nLoading...Click here to see a global map of confirmed cases and deaths.\\nIn response to mounting cases, state and federal authorities have emphasized a social distancing strategy, widely seen as the best available means to slow the spread of the virus.\\nSince March 20, New York state, Connecticut and New Jersey have accounted for around 50% of all U.S. cases.\",\n",
              "  'authors': ['Daniel Wood', 'Elena Renken'],\n",
              "  'item_id': '29491bc877cc45f5906f1d145b4c55da',\n",
              "  'keywords': ['york',\n",
              "   'states',\n",
              "   'spreading',\n",
              "   'state',\n",
              "   'tracking',\n",
              "   'testing',\n",
              "   'rate',\n",
              "   'case',\n",
              "   'pandemic',\n",
              "   'coronavirus',\n",
              "   'confirmed',\n",
              "   'cases',\n",
              "   'quickly',\n",
              "   'data'],\n",
              "  'relevance': '0.20911756424142322',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Tracking The Pandemic: How Quickly Is The Coronavirus Spreading State By State?'},\n",
              " {'abstract': 'GULSTAN: SO WHAT IS COVID-19 MODELING ALL ABOUT, AND HOW DO THE RESEARCHERS COME TO THOSE NUMBERS?\\nBASED ON THIS RESEARCH, AT ITS PEAK, THE U.S. COULD SEE MORE THAN 2200 DEATHS IN ONE DAY.\\nAT ITS PEAK, CALIFORNIA IS PROJECTED TO SEE 100 DEATHS IN ONE DAY, WITH A TOTAL NUMBER OF DEATHS PROJECTED TO BE MORE THAN 4300.\\nThe data can be used to predict when the peak amount of cases and deaths could occur.The curve model shows which state has issued a stay-at-home order.\\nWhy do some states peak later?Dr.',\n",
              "  'authors': ['Brandi Cummings', 'Https', 'Www.Facebook.Com'],\n",
              "  'item_id': 'efcdb5793c0447dcb2cc90b0eea80ec4',\n",
              "  'keywords': ['covid19',\n",
              "   'predict',\n",
              "   'california',\n",
              "   'states',\n",
              "   'number',\n",
              "   'deaths',\n",
              "   'state',\n",
              "   'projected',\n",
              "   'measures',\n",
              "   'mokdad',\n",
              "   'really',\n",
              "   'researchers',\n",
              "   'cases',\n",
              "   'peak',\n",
              "   'data'],\n",
              "  'relevance': '0.17355574606072013',\n",
              "  'timestamp': Decimal('1585612800'),\n",
              "  'title': 'Researchers predict when California will see peak of COVID-19 cases'},\n",
              " {'abstract': 'SAN FRANCISCO, Calif. (KRON) — The projected number of deaths in California due to COVID-19 has been lowered in a new model by The University of Washington.\\nThe model displays coronavirus cases and deaths anticipated in the United States, and also breaks down numbers by each state.\\nAt the time, the website projected there would be 6,109 deaths in California alone by Aug. 4.\\nThe new number is “likely the result of modeling the effects of continued social distancing.\\nBy Aug. 4, 81,114 deaths were projected in the United States in our original report a week ago.',\n",
              "  'authors': ['Tristi Rodriguez',\n",
              "   'Cnn Newssource',\n",
              "   'Jack Watson',\n",
              "   'Nexstar Media Wire'],\n",
              "  'item_id': '9c1be0d1867144a49755694ca9cac1d4',\n",
              "  'keywords': ['social',\n",
              "   'covid19',\n",
              "   'increase',\n",
              "   'deaths',\n",
              "   'california',\n",
              "   'number',\n",
              "   'distancing',\n",
              "   'projected',\n",
              "   'university',\n",
              "   'united',\n",
              "   'states',\n",
              "   'decrease',\n",
              "   'washingtonthe',\n",
              "   'website'],\n",
              "  'relevance': '0.2631817599663063',\n",
              "  'timestamp': Decimal('1585958400'),\n",
              "  'title': 'Projected number of California COVID-19 deaths decrease, US increase'},\n",
              " {'abstract': 'At a nursing facility in Kirkland, Wash, approximately 27 of the 108 residents and 25 of the 180 staff have some symptoms, health officials said during a teleconference with the Centers for Disease Control and Prevention.\\nadvertisementThe deceased, a man in his 50s with underlying health conditions, was not a resident of the facility, and officials have not yet found a link between his case and the outbreak in the nursing facility.\\nSo far, two people from the nursing facility outbreak have tested positive for Covid-19.\\nKathy Lofy, state health officer for the Washington State Department of Health, said that if Washington starts to see more spread, the state might consider social distancing measures, such as canceling large events.\\n“While there is some spread in some communities, there is not national spread of Covid-19,” said Nancy Messonier, director of the National Center for Immunization and Respiratory Diseases.',\n",
              "  'authors': ['Eric Boodman',\n",
              "   'Helen Branswell',\n",
              "   'Adam Feuerstein',\n",
              "   'Matthew Herper',\n",
              "   'Ed Silverman',\n",
              "   'Nicholas Florko',\n",
              "   'Lev Facher'],\n",
              "  'item_id': 'e62bb9e8d7fe4934ab1a6adad520ea63',\n",
              "  'keywords': ['washington',\n",
              "   'covid19',\n",
              "   'reported',\n",
              "   'health',\n",
              "   'concerns',\n",
              "   'state',\n",
              "   'underlying',\n",
              "   'raises',\n",
              "   'outbreak',\n",
              "   'facility',\n",
              "   'spread',\n",
              "   'nursing',\n",
              "   'cases'],\n",
              "  'relevance': '0.17865023555116108',\n",
              "  'timestamp': Decimal('1582934400'),\n",
              "  'title': 'First Covid-19 outbreak in a U.S. nursing home raises concerns'},\n",
              " {'abstract': 'That\\'s put Abbott in a difficult position of having to protect the Texas economy while heeding the warnings of epidemiologists.\\nMr. Abbott alone is accountable for destroying the Texas economy,\" wrote Huffines.\\nMeanwhile, Texas Democrats are highlighting a recent investigative piece by the Houston Chronicle stating that Texas is near the bottom among the 50 states in per capita testing.\\n\"As difficult as it may be, Texas must wait for the right time to reopen,\" said Manny Garcia, executive director of the Texas Democratic Party.\\nA push to expand testingIn interviews with CNN this week, Texas medical and public health officials warned that Texas is nowhere near the level of testing needed to draw firm conclusions about the extent of the outbreak.',\n",
              "  'authors': ['Maeve Reston', 'Caroline Kenny', 'Donald Judd', 'Julia Jones'],\n",
              "  'item_id': 'b1cea4f06ff6455ba0fd0d7712442705',\n",
              "  'keywords': ['houston',\n",
              "   'states',\n",
              "   'health',\n",
              "   'reopen',\n",
              "   'state',\n",
              "   'testing',\n",
              "   'tries',\n",
              "   'economy',\n",
              "   'week',\n",
              "   'coronavirus',\n",
              "   'texas',\n",
              "   'abbott'],\n",
              "  'relevance': '0.13724295214974563',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Behind on testing, Texas tries to be first to reopen'},\n",
              " {'abstract': 'New York Has Lost Another 783 Lives To Coronavirus, Cuomo SaysEnlarge this image toggle caption John Minchillo/AP John Minchillo/APNew York is flattening the curve, but the state still lost 783 lives over the last 24 hours, Gov.\\nIn total, as of early Saturday, the coronavirus has taken more than 8,600 lives in New York, Cuomo said.\\nReopening is both a public health question and an economic question, Cuomo said.\\n\"You can\\'t ask the people of this state, or this country, to choose between lives lost and dollars gained,\" he said.\\nCuomo said New York was recruiting attorneys state-wide to do pro-bono legal assistance on housing issues and Small Business Administration loan applications.',\n",
              "  'authors': ['Matthew S. Schwartz'],\n",
              "  'item_id': '6734961a386d4296a593f7216a77ff7b',\n",
              "  'keywords': ['coronavirus',\n",
              "   'lost',\n",
              "   'lives',\n",
              "   '783',\n",
              "   'weeks',\n",
              "   'york',\n",
              "   'stay',\n",
              "   'state',\n",
              "   'end',\n",
              "   'ask',\n",
              "   'question',\n",
              "   'cuomo'],\n",
              "  'relevance': '0.23715691996221452',\n",
              "  'timestamp': Decimal('1588377600'),\n",
              "  'title': 'New York Has Lost Another 783 Lives To Coronavirus, Cuomo Says'},\n",
              " {'abstract': 'ROME — Europe is seeing further signs of hope in the coronavirus outbreak as Italy’s daily death toll was at its lowest in more than two weeks and health officials noted with caution Sunday that the infection curve was finally descending.\\nIn Spain, new deaths dropped for the third straight day,But the optimism was tempered by Britain’s jump in coronavirus deaths that outpaced the daily toll in Italy.\\n“The curve, which had been plateauing for days, is starting to descend,″ national health official Silvio Brusaferro told reporters, referring to graphs indicating daily numbers of confirmed cases.\\nWith 621 more deaths reported on Sunday, Britain has 4,934 virus deaths overall among 47,806 cases.\\nSpain’s confirmed new virus deaths dropped for the third straight day, to 674 — the first time daily deaths have fallen below 800 in the past week.',\n",
              "  'authors': [],\n",
              "  'item_id': '0788d1d2aa5048988b7be1c6a1b59bba',\n",
              "  'keywords': ['covid19',\n",
              "   'virus',\n",
              "   'national',\n",
              "   'deaths',\n",
              "   'health',\n",
              "   'hope',\n",
              "   'europe',\n",
              "   'sees',\n",
              "   'signs',\n",
              "   'daily',\n",
              "   'email',\n",
              "   'day',\n",
              "   'falls',\n",
              "   'coronavirus',\n",
              "   'italys',\n",
              "   'confirmed',\n",
              "   'curve'],\n",
              "  'relevance': '0.2781386439008309',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': \"Europe Sees More Signs of Hope as Italy's COVID-19 Curve Falls\"},\n",
              " {'abstract': 'Kent Sepkowitz is a CNN medical analyst and a physician and infection control expert at Memorial Sloan Kettering Cancer Center in New York City.\\nFurthermore, in New York City, the Covid-19 death rate is about 6% higher than most countries.\\nNo other city or county has a fraction of the cases of New York City.\\nEven so, New York City is off the charts.\\nMore concerning is the elevated mortality rate in New York City.',\n",
              "  'authors': ['Opinion Kent Sepkowitz'],\n",
              "  'item_id': '2e704f2576ea431686d26de238f998ed',\n",
              "  'keywords': ['york',\n",
              "   'covid19',\n",
              "   'population',\n",
              "   'city',\n",
              "   'rates',\n",
              "   'hard',\n",
              "   'mortality',\n",
              "   'hit',\n",
              "   'rate',\n",
              "   'coronavirus',\n",
              "   'likely',\n",
              "   'death',\n",
              "   'cases'],\n",
              "  'relevance': '0.36510205578123117',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': 'Why New York has been hit so hard by coronavirus'},\n",
              " {'abstract': 'Two pet cats in New York state have tested positive for COVID-19, the disease caused by the new coronavirus that has claimed more than 180,000 human lives globally, according to the U.S. Department of Agriculture (USDA).\\nThe USDA said these are the first pets in the U.S. to test positive for COVID-19.\\nNo individuals in the household were confirmed to be ill with COVID-19,\" USDA said in its press release.\\nThe testing used to confirm the cats were positive for coronavirus is different than the procedure used on humans.\\nThe CDC and USDA have told pet owners to \"wash your hands before you touch your pet, wash your hands after you touch your pet.',\n",
              "  'authors': ['Peter Martinez'],\n",
              "  'item_id': 'c1791861b1614f9f8861b5b49a2a8f65',\n",
              "  'keywords': ['york',\n",
              "   'covid19',\n",
              "   'pet',\n",
              "   'cats',\n",
              "   'virus',\n",
              "   'zoo',\n",
              "   'positive',\n",
              "   'study',\n",
              "   'animals',\n",
              "   'pets',\n",
              "   'usda',\n",
              "   'coronavirus',\n",
              "   'test'],\n",
              "  'relevance': '0.23358305878897948',\n",
              "  'timestamp': Decimal('1587600000'),\n",
              "  'title': '2 pet cats test positive for COVID-19 in New York, USDA says'},\n",
              " {'abstract': 'According to American Progress, many of the stay-at-home orders say that noncompliance will be enforced through civil or criminal penalties, including fines, orders to suspend business operations, or imprisonment.\\nWhich states don’t have stay-at-home orders?\\nCurrently, there are eight states that do not have stay-at-home orders: Utah, Wyoming, North Dakota, South Dakota, Nebraska, Iowa, Oklahoma, and Arkansas.\\nWhen will the stay-at-home orders end?\\nAcross the country, the stay-at-home orders have had serious economic consequences, and around 22 million Americans have filed for unemployment benefits over the past four weeks.',\n",
              "  'authors': ['Erica Schwiegershausen'],\n",
              "  'item_id': '2f29148aa3564f529173b8f57b9a6cac',\n",
              "  'keywords': ['stay',\n",
              "   'states',\n",
              "   'stayathome',\n",
              "   'announced',\n",
              "   'shelterinplace',\n",
              "   'orders',\n",
              "   'essential',\n",
              "   'residents',\n",
              "   'including',\n",
              "   'mean',\n",
              "   'order',\n",
              "   'businesses'],\n",
              "  'relevance': '0.22610027057870327',\n",
              "  'timestamp': Decimal('1587513600'),\n",
              "  'title': 'Shelter-in-Place and Stay-at-Home Orders: What They Mean'},\n",
              " {'abstract': 'All are examples of dramatic action, often called “shelter in place,” in response to the coronavirus pandemic.\\nIt’s worth noting, though, that Cuomo specifically declined to use the words “shelter in place,” saying it caused more panic than was necessary.\\nWhile it sounds official, “‘shelter in place’ is not a legal term,” Mariner says.\\nA shelter-in-place order doesn’t mean that everybody has to be strictly homebound.\\nDoes shelter in place prohibit long-distance travel, like driving, or prevent people from leaving the state?',\n",
              "  'authors': ['Steve Calechman', 'Mar'],\n",
              "  'item_id': '7c3db34fcce0407a953b81830c25a4f7',\n",
              "  'keywords': ['social',\n",
              "   'stay',\n",
              "   'public',\n",
              "   'help',\n",
              "   'state',\n",
              "   'shelterinplace',\n",
              "   'essential',\n",
              "   'does',\n",
              "   'mariner',\n",
              "   'shelter',\n",
              "   'really',\n",
              "   'mean',\n",
              "   'place',\n",
              "   'order'],\n",
              "  'relevance': '0.34974642573502285',\n",
              "  'timestamp': Decimal('1584662400'),\n",
              "  'title': 'What does “shelter in place” really mean?'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    }
  ]
}