{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Context_Timeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F4DK0hxF1xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install google-api-python-client\n",
        "\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tweepy\n",
        "import json\n",
        "import json\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import urllib.parse\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk-terF_GEM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBRmxyONGaPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timeline_data = {}\n",
        "news_api_title = []\n",
        "news_api_decription = []\n",
        "news_api_date = []\n",
        "news_api_url = []\n",
        "\n",
        "def decode_title(dct):\n",
        "    if \"title\" in dct:\n",
        "        news_api_title.append(p.clean(dct[\"title\"]))\n",
        "    else:\n",
        "        return [\"\"]\n",
        "def decode_description(dct):\n",
        "    if \"description\" in dct:\n",
        "        news_api_decription.append(p.clean(dct[\"description\"]))\n",
        "    else:\n",
        "        return [\"\"]\n",
        "def decode_date(dct):\n",
        "    if \"publishedAt\" in dct:\n",
        "        news_api_date.append(dct[\"publishedAt\"])\n",
        "    else:\n",
        "        return [\"\"]\n",
        "def decode_url(dct):\n",
        "    if \"url\" in dct:\n",
        "        news_api_url.append(dct[\"url\"])\n",
        "    else:\n",
        "        return [\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3tJ1edZlHd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTimelineData():\n",
        "    for i in range(len(news_api_title)):\n",
        "      if news_api_date[i][:10] not in timeline_data:\n",
        "        timeline_data[news_api_date[i][:10]] = []\n",
        "        temp=[]\n",
        "        temp.append(news_api_title[i])\n",
        "        temp.append(news_api_url[i])\n",
        "        timeline_data[news_api_date[i][:10]].append(temp)\n",
        "      else:\n",
        "        temp=[]\n",
        "        temp.append(news_api_title[i])\n",
        "        temp.append(news_api_url[i])\n",
        "        timeline_data[news_api_date[i][:10]].append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CRLhd75GM4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNewsAPI(query):\n",
        "  keywords = urllib.parse.quote(query)\n",
        "  url = \"https://newsapi.org/v2/everything?q=\"+keywords+\"&from=&apiKey=2ed8bf16847a458d9af4ed36ce4f979b\"\n",
        "  response = requests.get(url)\n",
        "  newsApi_json = json.dumps(response.json(),sort_keys=True)\n",
        "  # print(newsApi_json)\n",
        "  # Decode title\n",
        "  parsed = json.loads(newsApi_json,object_hook=decode_title)\n",
        "  # Decode description\n",
        "  parsed = json.loads(newsApi_json,object_hook=decode_description)\n",
        "  # Decode date\n",
        "  parsed = json.loads(newsApi_json,object_hook=decode_date)\n",
        "  # Decode url\n",
        "  parsed = json.loads(newsApi_json,object_hook=decode_url)\n",
        "  getTimelineData()\n",
        "getNewsAPI(\"kobe\")\n",
        "print(timeline_data)\n",
        "print(len(timeline_data))\n",
        "print(len(news_api_title))\n",
        "print(len(news_api_url))\n",
        "# print(len(news_api_title))\n",
        "# print(len(news_api_url))\n",
        "news_api_title = []\n",
        "news_api_decription = []\n",
        "news_api_date = []\n",
        "news_api_url = []\n",
        "timeline_data = {}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A13zpmvfLsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getGNewsAPI(query):\n",
        "  initialize_corpus()\n",
        "  keywords = urllib.parse.quote(query)\n",
        "  url = \"https://gnews.io/api/v3/search?q=\"+keywords+\"&token=a6419076f909fc74c42016c6bebf0755\"\n",
        "  response = requests.get(url)\n",
        "  gNewsApi_json = json.dumps(response.json(), indent = 4,sort_keys=True)\n",
        "  print(gNewsApi_json)\n",
        "  # Decode title\n",
        "  parsed = json.loads(gNewsApi_json,object_hook=decode_title)\n",
        "  # Decode description\n",
        "  parsed = json.loads(gNewsApi_json,object_hook=decode_description)\n",
        "  # Decode date\n",
        "  parsed = json.loads(gNewsApi_json,object_hook=decode_date)\n",
        "  # Decode url\n",
        "  parsed = json.loads(gNewsApi_json,object_hook=decode_url)\n",
        "  getTimelineData()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mlsIGXworHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildGraph():\n",
        "  timeline_data = {}\n",
        "  keyword = \"Kobe\"\n",
        "  getNewsAPI(keyword)\n",
        "  getGNewsAPI(keyword)\n",
        "  print(timeline_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8zCviLNakZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def getArticle(url):\n",
        "    result = requests.get(url)\n",
        "    c = result.content\n",
        "    soup = BeautifulSoup(c)\n",
        "\n",
        "    article_text = ''\n",
        "    article = soup.findAll('p')\n",
        "    for element in article:\n",
        "        article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    return article_text"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}