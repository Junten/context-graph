{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-Context.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junten/context-graph/blob/master/src/Deep_Context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prIBp4ceu1Fu",
        "colab_type": "text"
      },
      "source": [
        "#News Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbGyyeovhqHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install google-api-python-client\n",
        "\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tweepy\n",
        "import json\n",
        "import json\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import urllib.parse\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJES2GcFmww4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x2xNKInO7aa",
        "colab_type": "text"
      },
      "source": [
        "__Get News Data from News API__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzYzJtS0g-a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_api_data = []\n",
        "def decode_title(dct):\n",
        "    if \"title\" in dct:\n",
        "        news_api_data.append(p.clean(dct[\"title\"]))\n",
        "    else:\n",
        "        return [\"\"]\n",
        "def decode_description(dct):\n",
        "    if \"description\" in dct:\n",
        "        news_api_data.append(p.clean(dct[\"description\"]))\n",
        "    else:\n",
        "        return [\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiucM-sKh1T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://newsapi.org/docs/endpoints/everything\n",
        "def getNewsAPI(query):\n",
        "  keywords = urllib.parse.quote(query)\n",
        "  url = \"https://newsapi.org/v2/everything?q=\"+keywords+\"&from=2020-01-01&apiKey=2ed8bf16847a458d9af4ed36ce4f979b\"\n",
        "  response = requests.get(url)\n",
        "  newsApi_json = json.dumps(response.json(), sort_keys=True)\n",
        "  # Decode title\n",
        "  parsed = json.loads(newsApi_json,object_hook=decode_title)\n",
        "  # Decode description\n",
        "  parsed = json.loads(newsApi_json,object_hook=decode_description)\n",
        "  return news_api_data\n",
        "\n",
        "# print(getNewsAPI(\"kobe\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mopwrZ5ZlEN3",
        "colab_type": "text"
      },
      "source": [
        "__Get News Data from Google News__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7cq5ng6Mqpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getGNewsAPI(query):\n",
        "  keywords = urllib.parse.quote(query)\n",
        "  url = \"https://gnews.io/api/v3/search?q=\"+keywords+\"&token=a6419076f909fc74c42016c6bebf0755\"\n",
        "  response = requests.get(url)\n",
        "  gNewsApi_json = json.dumps(response.json(), sort_keys=True)\n",
        "  # Decode title\n",
        "  parsed = json.loads(gNewsApi_json,object_hook=decode_title)\n",
        "  # Decode description\n",
        "  parsed = json.loads(gNewsApi_json,object_hook=decode_description)\n",
        "  return news_api_data\n",
        "# print(getGNewsAPI(\"kobe\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dusgoVpEiHgV",
        "colab_type": "text"
      },
      "source": [
        "__Get Twitter Data__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTw1uONRiLVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_twitter_context(topicName):\n",
        "    auth = tweepy.OAuthHandler('jSFt0cV8xU9fDkjRWpJefifWo', 'pTLQxVryjlPoQIO6RyrlBol3SZnyhWtufNldLylnpx4A07Efw4')\n",
        "    auth.set_access_token('1045504930428071937-9RciJ5ESjvoszHaMUUeajMkgcueNZS', 'ERSUHYQRYcjAxizyInpB3mwRfLyICyTvZSE3i7kEx1V4F')\n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    # The search term you want to find\n",
        "    query = topicName\n",
        "    language = \"en\"\n",
        "    # Calling the user_timeline function with our parameters\n",
        "    results = api.search(q=query, lang=language)\n",
        "    corpus = []\n",
        "    # foreach through all tweets pulled\n",
        "    for tweet in results:\n",
        "        # print(tweet.user.screen_name, \"Tweeted:\", tweet.text)\n",
        "        corpus.append(p.clean(tweet.text))\n",
        "    return corpus\n",
        "\n",
        "# print(get_twitter_context(\"kobe\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS5cviRVkAZo",
        "colab_type": "text"
      },
      "source": [
        "__Get top N topic__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCcnR6uakV38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
        "                   vec.vocabulary_.items() if word not in stop_words]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                       reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IkxKLUkXxs",
        "colab_type": "text"
      },
      "source": [
        "__Google Search API__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV9kdyUzk1FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# my_api_key = \"AIzaSyAULWtrSkRR-FLRSMfz5ycwFlrYHhCw1Vw\"\n",
        "# my_cse_id = \"014947934928168541572:hgmnooclf3g\"\n",
        "# google_result_list = []\n",
        "def decode_valueString(dct):\n",
        "        if \"title\" in dct:\n",
        "              google_result_list.append(p.clean(dct[\"title\"]))       \n",
        "        else:\n",
        "            return [\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PB9SXJctBQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def google_search(search_term, api_key, cse_id, **kwargs):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
        "    app_json = json.dumps(res, sort_keys=True)\n",
        "    parsed = json.loads(app_json,object_hook=decode_valueString)\n",
        "    return google_result_list\n",
        "# print(google_search(\"kobe\",my_api_key,my_cse_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BlAVlQLoESY",
        "colab_type": "text"
      },
      "source": [
        "**Data Clearning and Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwhlRyyeoCeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def cleaning(raw_news):\n",
        "    import nltk\n",
        "    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmentize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMe7sFxftoES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keyword = \"kobe\"\n",
        "corpus_twitter = get_twitter_context(keyword)\n",
        "newApi = getNewsAPI(keyword)\n",
        "gNews = getGNewsAPI(keyword)\n",
        "corpus = corpus_twitter + newApi + gNews"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYArPgrbtpqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH8qlxD6unlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfObj = pd.DataFrame(corpus) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v2HCJapu2EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfObj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRBOK3mIxgEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfObj.columns = ['news_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9jwVL82xy6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfObj.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZQ5is78x_cZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86-A5jepyOYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3m6DrVOtPK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# t1 = time.time()\n",
        "# Add the processed data to the original data. \n",
        "# Perhaps using apply function would be more elegant and concise than using for loop\n",
        "dfObj[\"news_text\"] = dfObj[\"news_text\"].apply(cleaning)\n",
        "\n",
        "# t2 = time.time()\n",
        "# print(\"\\nTime to clean, tokenize and stem train data: \\n\", len(dfObj), \"news:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9jw7PJrubI0",
        "colab_type": "text"
      },
      "source": [
        "__Generate Memory Graph for Visualization__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68LTP31uwuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_api_data = []\n",
        "keyword = \"coronavirus\"\n",
        "my_api_key = \"AIzaSyAULWtrSkRR-FLRSMfz5ycwFlrYHhCw1Vw\"\n",
        "my_cse_id = \"014947934928168541572:hgmnooclf3g\"\n",
        "G=nx.Graph()\n",
        "G.add_node(keyword)\n",
        " \n",
        "corpus_twitter = get_twitter_context(keyword)\n",
        "newApi = getNewsAPI(keyword)\n",
        "gNews = getGNewsAPI(keyword)\n",
        "corpus = corpus_twitter + newApi + gNews\n",
        "\n",
        "top5_keyword_twitter = get_top_n_words(corpus,n=10)\n",
        "for item in top5_keyword_twitter:\n",
        "   edge = (keyword, item[0])\n",
        "   G.add_edge(*edge)\n",
        "   google_result_list = []\n",
        "   google_keyword = item[0]\n",
        "   google_result = google_search(google_keyword,my_api_key,my_cse_id)\n",
        "   top5_keyword_google = get_top_n_words(google_result,n=10)\n",
        "   for result in top5_keyword_google:\n",
        "       edge = (item[0], result[0])\n",
        "       G.add_edge(*edge)   \n",
        " \n",
        "nx.draw(G,with_labels=True)\n",
        "plt.savefig(\"plot.png\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW8545J4lWhg",
        "colab_type": "text"
      },
      "source": [
        "__LDA Topic Modeling__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwcXYraFfuVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "def print_topics(model, count_vectorizer, n_top_words):\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "number_topics = 10\n",
        "number_words = 10\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "# Fit and transform the processed titles\n",
        "count_data = count_vectorizer.fit_transform(corpus)\n",
        "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
        "lda.fit(count_data)\n",
        "# Print the topics found by the LDA model\n",
        "print(\"Topics found via LDA:\")\n",
        "print_topics(lda, count_vectorizer, number_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW1_8hJEJoXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pyLDAvis\n",
        "from pyLDAvis import sklearn as sklearn_lda\n",
        "import pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stZEqeragrlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
        "pyLDAvis.display(LDAvis_prepared)\n",
        "# pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RThgJI9Ul3XW",
        "colab_type": "text"
      },
      "source": [
        "#Neo4j Graph Database Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_oz6K8yl67-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install -U ipython\n",
        "pip install py2neo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJ_UQpul8s-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from py2neo import Graph, Node, Relationship"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcXVNY09l-aG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#graph = Graph(\"bolt://ec2-100-27-23-215.compute-1.amazonaws.com:7687\")\n",
        "graph = Graph(\"bolt://ec2-100-27-23-215.compute-1.amazonaws.com:7687\", user = \"kevin\", password = \"sjsucmpe295\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZaH0rUbtGiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph.delete_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKK7D59omawn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_api_data = []\n",
        "keyword = \"iowacaucus\"\n",
        "my_api_key = \"AIzaSyAULWtrSkRR-FLRSMfz5ycwFlrYHhCw1Vw\"\n",
        "my_cse_id = \"014947934928168541572:hgmnooclf3g\"\n",
        "\n",
        "topic = Node(\"Keyword\", name=keyword)\n",
        "graph.create(topic)\n",
        "\n",
        "corpus_twitter = get_twitter_context(keyword)\n",
        "newApi = getNewsAPI(keyword)\n",
        "gNews = getGNewsAPI(keyword)\n",
        "corpus = corpus_twitter + newApi + gNews\n",
        "\n",
        "top5_keyword_twitter = get_top_n_words(corpus,n=10)\n",
        "for item in top5_keyword_twitter:\n",
        "   n = Node(\"Twitter\", name=item[0])\n",
        "   r = Relationship(topic, \"LINKS_TO\", n)\n",
        "   graph.create(n | r)\n",
        "   google_result_list = []\n",
        "   google_keyword = item[0]\n",
        "   google_result = google_search(google_keyword,my_api_key,my_cse_id)\n",
        "   top5_keyword_google = get_top_n_words(google_result,n=10)\n",
        "   for result in top5_keyword_google:\n",
        "       res = Node(\"Google\", name=result[0])\n",
        "       rel = Relationship(n, \"LINKS_TO\", res)\n",
        "       graph.create(res)\n",
        "       graph.create(rel)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}