{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/3d/289963bbf51f8d00cdf7483cdc2baee25ba877e8b4eb72157c47211e3b57/Wikipedia-API-0.5.4.tar.gz\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from wikipedia-api) (2.21.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in ./Library/Python/3.7/lib/python/site-packages (from requests->wikipedia-api) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->wikipedia-api) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->wikipedia-api) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->wikipedia-api) (2.8)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Running setup.py bdist_wheel for wikipedia-api ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/laura_liu/Library/Caches/pip/wheels/bf/40/42/ba1d497f3712281b659dd65b566fc868035c859239571a725a\n",
      "Successfully built wikipedia-api\n",
      "\u001b[31mawscli 1.16.152 has requirement botocore==1.12.142, but you'll have botocore 1.12.143 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.5.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi  # pip install wikipedia-api\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def wiki_scrape(topic):\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en',\n",
    "            extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(topic)\n",
    "    if not page_name.exists():\n",
    "        print('page does not exist')\n",
    "        exit()\n",
    "    page_links = [topic] + list(page_name.links.keys())\n",
    "    working_links = [link for link in page_links\n",
    "                     if wiki_api.page(link).exists()]\n",
    "    blacklist = ('Template', 'Help:', 'Category:', 'Portal:', 'Wikipedia:', 'Talk:')\n",
    "    sources = [link for link in working_links\n",
    "               if len(wiki_api.page(link).text) > 20\n",
    "               and not link.startswith(blacklist)]\n",
    "    wiki_data = {'topic':topic, 'page':sources}\n",
    "    wiki_data['text'] = [wiki_api.page(page).text for page in sources]\n",
    "    wiki_data['link'] = [wiki_api.page(page).fullurl for page in sources]\n",
    "    wiki_data['categories'] = [[y[9:] for y in\n",
    "                               list(wiki_api.page(page).categories.keys())]\n",
    "                               for page in sources]\n",
    "    print ('Wikipedia pages scraped:', len(sources))\n",
    "    wiki_scrape_df = pd.DataFrame(wiki_data)\n",
    "    return wiki_scrape_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = wiki_scrape('Financial crisis of 2007â€“08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "coref = spacy.load('en_coref_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install MODEL_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "\n",
    "def entity_pairs(text, coref=True):\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = list()\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "        dep = [token.dep_ for token in sent]\n",
    "        if (dep.count('obj')+dep.count('dobj'))==1 \\\n",
    "                and (dep.count('subj')+dep.count('nsubj'))==1:\n",
    "            for token in sent:\n",
    "                if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
    "                    subject = [w for w in token.head.lefts if w.dep_\n",
    "                               in ('subj', 'nsubj')]  # identify subject nodes\n",
    "                    if subject:\n",
    "                        subject = subject[0]\n",
    "                        # identify relationship by root dependency\n",
    "                        relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
    "                        if relation:\n",
    "                            relation = relation[0]\n",
    "                            # add adposition or particle to relationship\n",
    "                            if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
    "                                relation = ' '.join((str(relation),\n",
    "                                        str(relation.nbor(1))))\n",
    "                        else:\n",
    "                            relation = 'unknown'\n",
    "                        subject, subject_type = refine_ent(subject, sent)\n",
    "                        token, object_type = refine_ent(token, sent)\n",
    "                        ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                str(subject_type), str(object_type)])\n",
    "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(x) == '' for x in sublist)]\n",
    "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
    "                         'relation', 'object', 'subject_type',\n",
    "                         'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(filtered_ent_pairs)))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def refine_ent(ent, sent):\n",
    "    unwanted_tokens = (\n",
    "        'PRON',  # pronouns\n",
    "        'PART',  # particle\n",
    "        'DET',  # determiner\n",
    "        'SCONJ',  # subordinating conjunction\n",
    "        'PUNCT',  # punctuation\n",
    "        'SYM',  # symbol\n",
    "        'X',  # other\n",
    "        )\n",
    "    ent_type = ent.ent_type_  # get entity type\n",
    "    if ent_type == '':\n",
    "        ent_type = 'NOUN_CHUNK'\n",
    "        ent = ' '.join(str(t.text) for t in\n",
    "                nlp(str(ent)) if t.pos_\n",
    "                not in unwanted_tokens and t.is_stop == False)\n",
    "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "        t = ''\n",
    "        for i in range(len(sent) - ent.i):\n",
    "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                t += ' ' + str(ent.nbor(i))\n",
    "            else:\n",
    "                ent = t.strip()\n",
    "                break\n",
    "    return ent, ent_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = entity_pairs(wiki_data.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_kg(pairs):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    node_deg = nx.degree(k_graph)\n",
    "    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n",
    "    plt.figure(num=None, figsize=(120, 90), dpi=80)\n",
    "    nx.draw_networkx(\n",
    "        k_graph,\n",
    "        node_size=[int(deg[1]) * 500 for deg in node_deg],\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white',\n",
    "        )\n",
    "    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n",
    "                  pairs['relation'].tolist()))\n",
    "    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n",
    "                                 font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_kg(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_graph(pairs, node):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    edges = nx.dfs_successors(k_graph, node)\n",
    "    nodes = []\n",
    "    for k, v in edges.items():\n",
    "        nodes.extend([k])\n",
    "        nodes.extend(v)\n",
    "    subgraph = k_graph.subgraph(nodes)\n",
    "    layout = (nx.random_layout(k_graph))\n",
    "    nx.draw_networkx(\n",
    "        subgraph,\n",
    "        node_size=1000,\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white'\n",
    "        )\n",
    "    labels = dict(zip((list(zip(pairs.subject, pairs.object))),\n",
    "                    pairs['relation'].tolist()))\n",
    "    edges= tuple(subgraph.out_edges(data=False))\n",
    "    sublabels ={k: labels[k] for k in edges}\n",
    "    print(k_graph.out_edges(data=False))\n",
    "    nx.draw_networkx_edge_labels(subgraph, pos=layout, edge_labels=sublabels,\n",
    "                                font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
